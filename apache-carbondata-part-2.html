<!doctype html>
<html lang="en" prefix="og: http://ogp.me/ns#">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  Apache CarbonData (part 2) | APOTHEM
</title>
  <link rel="canonical" href="https://apothem.blog/apache-carbondata-part-2.html">

    <link rel="icon" type="image/x-icon" href="https://apothem.blog/favicon.ico">

  <link rel="stylesheet" href="https://apothem.blog/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://apothem.blog/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://apothem.blog/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://apothem.blog/theme/css/theme.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="https://apothem.blog/feeds/all.atom.xml">
  <link rel="alternate" type="application/atom+xml" title="Categories Atom Feed"
        href="https://apothem.blog/feeds/{slug}.atom.xml">  
  <meta name="description" content="In the previous article we have seen many exciting features that CarbonData offers, but we haven't explored them all; in this article we will try out the streaming capabilities and we will delve a bit deeper into the data layout, looking at concept like compaction and partitioning, and the way …">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
    <div class="col-sm-4">
      <a href="https://apothem.blog/">
        <img class="img-fluid rounded" src=https://apothem.blog/images/profile.svg width=180 alt="APOTHEM">
      </a>
    </div>
  <div class="col-sm-8">
    <h1 class="title"><a href="https://apothem.blog/">APOTHEM</a></h1>
      <p class="text-muted">Apache Project(s) of the month</p>
      <ul class="list-inline">
          <li class="list-inline-item"><a href="https://projects.apache.org" target="_blank">projects.apache.org</a></li>
              <li class="list-inline-item text-muted">|</li>
            <li class="list-inline-item"><a href="https://apothem.blog/pages/about.html">About</a></li>
            <li class="list-inline-item"><a href="https://apothem.blog/pages/faq.html">FAQ</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fa fa-github" href="https://github.com/nvitucci/apothem" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fa fa-twitter" href="https://twitter.com/nvitucci" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fa fa-rss" href="feeds/all.atom.xml" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h2>  Apache CarbonData (part 2)
</h2>
      <hr>
  <article class="article">
    <header>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2019-10-31T00:00:00+00:00">
          <i class="fa fa-clock-o"></i>
          Thu 31 October 2019
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="https://apothem.blog/category/projects.html">projects</a>
        </li>
          <li class="list-inline-item">
            <i class="fa fa-files-o"></i>
              <a href="https://apothem.blog/tag/big-data.html">#Big Data</a>,               <a href="https://apothem.blog/tag/data-storage-format.html">#data storage format</a>          </li>
      </ul>
    </header>
    <div class="content">
      <p>In the <a href="https://apothem.blog/apache-carbondata.html">previous article</a> we have seen many exciting features that CarbonData offers, but we haven't explored them all; in this article we will try out the streaming capabilities and we will delve a bit deeper into the data layout, looking at concept like compaction and partitioning, and the way the different files are managed.</p>
<h2 id="streaming">Streaming</h2>
<p>Another feature that sets CarbonData apart from other Hadoop data formats is the support for streaming data. Rather than writing many small files or waiting until a larger file can be written, CarbonData uses a slightly different format where single rows can be added as they arrive; such files can later be converted to the standard columnar format (via compaction) in order to support all the features we have already discussed. Every task, including the creation of the sink table and the management of the streaming job, can be performed using SQL commands. Let's see how.</p>
<p>First of all, we need to create a streaming source. An easy way to do this is to use <a href="https://en.wikipedia.org/wiki/Netcat">netcat</a>, a Linux utility that, among many other things, can be used to send data through a socket, thus simulating a process streaming data over the network. We need to open a new terminal (or to use another utility like <a href="https://en.wikipedia.org/wiki/GNU_Screen">screen</a> or <a href="https://en.wikipedia.org/wiki/Tmux">tmux</a>) in order to keep <em>netcat</em> running while we use the Spark shell; on the new terminal we need to run the following command:</p>
<div class="highlight"><pre><span></span>$ nc -lk <span class="m">9099</span>
</pre></div>


<p>which will seem to be "hanging up", while it will actually be waiting for our input. Let's leave it for now and switch to our running Spark shell with a CarbonData session active.</p>
<p>We need to create a source table that CarbonData will use to interpret the format of the incoming data. For instance, if we want to stream data in CSV format where the first field is an integer and the second is a string, we can create the table like the following:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">s&quot;&quot;&quot;</span>
<span class="s">    CREATE TABLE source(col1 INT, col2 STRING)</span>
<span class="s">    STORED AS carbondata</span>
<span class="s">    TBLPROPERTIES(</span>
<span class="s">        &#39;streaming&#39; = &#39;source&#39;,</span>
<span class="s">        &#39;format&#39; = &#39;socket&#39;,</span>
<span class="s">        &#39;host&#39; = &#39;localhost’,</span>
<span class="s">        &#39;port&#39; = &#39;9099&#39;,</span>
<span class="s">        &#39;record_format&#39; = &#39;csv&#39;,</span>
<span class="s">        &#39;delimiter&#39; = &#39;,&#39;)</span>
<span class="s">   &quot;&quot;&quot;</span><span class="o">)</span>
</pre></div>


<p>The table properties should be self-explanatory; they are basically Spark Structured Streaming <a href="https://spark.apache.org/docs/2.3.4/structured-streaming-programming-guide.html#creating-streaming-dataframes-and-streaming-datasets">DataStreamReader</a>'s options. Once the source table is created, we need to create a sink table as well:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">s&quot;&quot;&quot;</span>
<span class="s">    CREATE TABLE sink(col1 INT, col2 STRING)</span>
<span class="s">    STORED AS carbondata</span>
<span class="s">    TBLPROPERTIES(&#39;streaming&#39; = &#39;true&#39;)</span>
<span class="s">  &quot;&quot;&quot;</span><span class="o">)</span>
</pre></div>


<p>In this case our sink table mimics the source table exactly, but this is not mandatory. Now we are ready to create the actual streaming job:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">s&quot;&quot;&quot;</span>
<span class="s">    CREATE STREAM job1 ON TABLE sink</span>
<span class="s">    STMPROPERTIES(</span>
<span class="s">        &#39;trigger&#39; = &#39;ProcessingTime&#39;,</span>
<span class="s">        &#39;interval&#39; = &#39;1 seconds&#39;)</span>
<span class="s">    AS</span>
<span class="s">        SELECT *</span>
<span class="s">        FROM source</span>
<span class="s">  &quot;&quot;&quot;</span><span class="o">)</span>
</pre></div>


<p>which is translated as a Spark <a href="https://spark.apache.org/docs/2.3.4/structured-streaming-programming-guide.html#starting-streaming-queries">Streaming Query</a>. That's all the setup we need.</p>
<p>If we run a <code>SELECT</code> query on the sink table, we will not see any results; this is normal because we haven't streamed any data yet. Let's switch to the terminal where <em>netcat</em> is running and add a few lines like the following:</p>
<div class="highlight"><pre><span></span>1,value1
2,value2
10,value10
</pre></div>


<p>then switch back to the Spark shell and run a <code>SELECT</code> query on the sink table again:</p>
<div class="highlight"><pre><span></span><span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM sink&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>Now we should see the rows we have just inserted!</p>
<div class="highlight"><pre><span></span>+----+-------+
|col1|   col2|
+----+-------+
|   1| value1|
|   2| value2|
|  10|value10|
+----+-------+
</pre></div>


<p>While the streaming job is running, we can see some information on the job itself by running the following command:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SHOW STREAMS&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>To terminate the streaming job we need to run a <code>DROP</code> command using the stream name:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;DROP STREAM job1&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>After closing the stream, we could decide to <em>compact</em> the sink table and convert its files to the standard CarbonData column-based format. Let's check the table's <em>segments</em> first:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SHOW SEGMENTS FOR TABLE sink&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>which should output something like the following:</p>
<div class="highlight"><pre><span></span>+-----------------+---------+--------------------+-------------+---------+-----------+---------+----------+
|SegmentSequenceId|   Status|     Load Start Time|Load End Time|Merged To|File Format|Data Size|Index Size|
+-----------------+---------+--------------------+-------------+---------+-----------+---------+----------+
|                0|Streaming|2019-10-31 22:46:...|           NA|       NA|     ROW_V1|   634.0B|    174.0B|
+-----------------+---------+--------------------+-------------+---------+-----------+---------+----------+
</pre></div>


<p>Let's now compact the table:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;ALTER TABLE sink COMPACT &#39;CLOSE_STREAMING&#39;&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>and check the segments again:</p>
<div class="highlight"><pre><span></span>+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
|SegmentSequenceId|   Status|     Load Start Time|       Load End Time|Merged To|File Format|Data Size|Index Size|
+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
|                1|  Success|2019-10-31 23:04:...|2019-10-31 23:04:...|       NA|COLUMNAR_V3|   1,22KB|    744.0B|
|                0|Compacted|2019-10-31 22:46:...|2019-10-31 23:04:...|        1|     ROW_V1|   634.0B|    174.0B|
+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
</pre></div>


<p>The format of the data has actually changed, and it cannot be changed back to <code>ROW_V1</code>. We'll see compaction into more detail in the next section.</p>
<h2 id="compaction">Compaction</h2>
<p>Whenever we load data into CarbonData, a new folder called <em>segment</em> is created; this makes dealing with transactions easier and helps maintaining consistency. In the long run, anyway, the growing number of segments degrades the query performance; <em>compaction</em> can then be used to merge multiple segments into one.</p>
<p>CarbonData supports two main types of compaction besides the <code>CLOSE_STREAMING</code> that we've seen in the previous section, namely <em>minor</em> (based on the number of new segments) and <em>major</em> (based on the size of the new segments). We'll see how they work using another table from the TPCH benchmark.</p>
<p>Let's create the <code>order</code> table:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">s&quot;&quot;&quot;</span>
<span class="s">    CREATE TABLE IF NOT EXISTS orders(</span>
<span class="s">        o_orderdate DATE,</span>
<span class="s">        o_orderpriority STRING,</span>
<span class="s">        o_orderstatus STRING,</span>
<span class="s">        o_orderkey INT,</span>
<span class="s">        o_custkey STRING,</span>
<span class="s">        o_totalprice DOUBLE,</span>
<span class="s">        o_clerk STRING,</span>
<span class="s">        o_shippriority INT,</span>
<span class="s">        o_comment STRING)</span>
<span class="s">    STORED AS carbondata</span>
<span class="s">    TBLPROPERTIES(</span>
<span class="s">        &#39;SORT_COLUMNS&#39; = &#39;o_orderdate&#39;)</span>
<span class="s">  &quot;&quot;&quot;</span><span class="o">)</span>
</pre></div>


<p>and the load some data:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">s&quot;&quot;&quot;</span>
<span class="s">    LOAD DATA INPATH &#39;/tmp/tpch-dbgen/orders.tbl&#39;</span>
<span class="s">    INTO TABLE orders</span>
<span class="s">    OPTIONS(</span>
<span class="s">        &#39;DELIMITER&#39; = &#39;|&#39;,</span>
<span class="s">        &#39;FILEHEADER&#39; = &#39;o_orderkey,o_custkey,o_orderstatus,o_totalprice,o_orderdate,o_orderpriority,o_clerk,o_shippriority,o_comment&#39;)</span>
<span class="s">  &quot;&quot;&quot;</span><span class="o">)</span>
</pre></div>


<p>We can now look at the segment information for this table:</p>
<div class="highlight"><pre><span></span>&gt; carbon.sql(&quot;SHOW SEGMENTS FOR TABLE orders&quot;).show
</pre></div>


<p>which returns:</p>
<div class="highlight"><pre><span></span>+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
|SegmentSequenceId| Status|     Load Start Time|       Load End Time|Merged To|File Format|Data Size|Index Size|
+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
|                0|Success|2019-11-02 12:41:...|2019-11-02 12:41:...|       NA|COLUMNAR_V3|  51,20MB|    1,59KB|
+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
</pre></div>


<p>and, if we look at the content of the CarbonData store directory containing the <code>order</code> table, we will see something like this:</p>
<div class="highlight"><pre><span></span>$ tree /var/carbondata/data/store/default/orders

/var/carbondata/data/store/default/orders
├── Fact
│   └── Part0
│       └── Segment_0
│           ├── 0_1572694894322.carbonindexmerge
│           └── part-0-0_batchno0-0-0-1572694887333.carbondata
├── LockFiles
│   ├── Segment_0.lock
│   └── tablestatus.lock
└── Metadata
    ├── schema
    ├── segments
    │   └── 0_1572694887333.segment
    └── tablestatus
</pre></div>


<p>Let's now load some more data running the same <code>LOAD</code> command as before for three more times (there will be duplicate rows, but it does not matter), then check again the segments:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SHOW SEGMENTS FOR TABLE orders&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>which look like this:</p>
<div class="highlight"><pre><span></span><span class="err">+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+</span>
<span class="err">|SegmentSequenceId| Status|     Load Start Time|       Load End Time|Merged To|File Format|Data Size|Index Size|</span>
<span class="err">+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+</span>
<span class="err">|                3|Success|2019-11-02 12:50:...|2019-11-02 12:50:...|       NA|COLUMNAR_V3|  51,20MB|    1,59KB|</span>
<span class="err">|                2|Success|2019-11-02 12:41:...|2019-11-02 12:41:...|       NA|COLUMNAR_V3|  51,20MB|    1,59KB|</span>
<span class="err">|                1|Success|2019-11-02 12:41:...|2019-11-02 12:41:...|       NA|COLUMNAR_V3|  51,21MB|    1,59KB|</span>
<span class="err">|                0|Success|2019-11-02 12:41:...|2019-11-02 12:41:...|       NA|COLUMNAR_V3|  51,20MB|    1,59KB|</span>
<span class="err">+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+</span>
</pre></div>


<p>and the directory:</p>
<div class="highlight"><pre><span></span>$ tree /var/carbondata/data/store/default/orders

/var/carbondata/data/store/default/orders
├── Fact
│   └── Part0
│       ├── Segment_0
│       │   ├── 0_1572694894322.carbonindexmerge
│       │   └── part-0-0_batchno0-0-0-1572694887333.carbondata
│       ├── Segment_1
│       │   ├── 1_1572694906502.carbonindexmerge
│       │   └── part-0-0_batchno0-0-1-1572694899093.carbondata
│       ├── Segment_2
│       │   ├── 2_1572694914891.carbonindexmerge
│       │   └── part-0-0_batchno0-0-2-1572694907902.carbondata
│       └── Segment_3
│           ├── 3_1572695453562.carbonindexmerge
│           └── part-0-0_batchno0-0-3-1572695446456.carbondata
├── LockFiles
│   ├── compaction.lock
│   ├── Segment_0.lock
│   ├── Segment_1.lock
│   ├── Segment_2.lock
│   ├── Segment_3.lock
│   ├── tablestatus.lock
│   └── update.lock
└── Metadata
    ├── schema
    ├── segments
    │   ├── 0_1572694887333.segment
    │   ├── 1_1572694899093.segment
    │   ├── 2_1572694907902.segment
    │   └── 3_1572695446456.segment
    └── tablestatus
</pre></div>


<p>Now we can run a minor compaction:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;ALTER TABLE orders COMPACT &#39;MINOR&#39;&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>and see what changed in the segments:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SHOW SEGMENTS FOR TABLE orders&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>which look like this:</p>
<div class="highlight"><pre><span></span>+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
|SegmentSequenceId|   Status|     Load Start Time|       Load End Time|Merged To|File Format|Data Size|Index Size|
+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
|                3|Compacted|2019-11-02 12:50:...|2019-11-02 12:50:...|      0.1|COLUMNAR_V3|  51,20MB|    1,59KB|
|                2|Compacted|2019-11-02 12:41:...|2019-11-02 12:41:...|      0.1|COLUMNAR_V3|  51,20MB|    1,59KB|
|                1|Compacted|2019-11-02 12:41:...|2019-11-02 12:41:...|      0.1|COLUMNAR_V3|  51,21MB|    1,59KB|
|              0.1|  Success|2019-11-02 12:51:...|2019-11-02 12:51:...|       NA|COLUMNAR_V3| 146,33MB|    2,81KB|
|                0|Compacted|2019-11-02 12:41:...|2019-11-02 12:41:...|      0.1|COLUMNAR_V3|  51,20MB|    1,59KB|
+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
</pre></div>


<p>and in the directory:</p>
<div class="highlight"><pre><span></span>$ tree /var/carbondata/data/store/default/orders

/var/carbondata/data/store/default/orders
├── Fact
│   └── Part0
│       ├── Segment_0
│       │   ├── 0_1572694894322.carbonindexmerge
│       │   └── part-0-0_batchno0-0-0-1572694887333.carbondata
│       ├── Segment_0.1
│       │   ├── <span class="m">0</span>.1_1572695514980.carbonindexmerge
│       │   └── part-0-0_batchno0-0-0.1-1572695501043.carbondata
│       ├── Segment_1
│       │   ├── 1_1572694906502.carbonindexmerge
│       │   └── part-0-0_batchno0-0-1-1572694899093.carbondata
│       ├── Segment_2
│       │   ├── 2_1572694914891.carbonindexmerge
│       │   └── part-0-0_batchno0-0-2-1572694907902.carbondata
│       └── Segment_3
│           ├── 3_1572695453562.carbonindexmerge
│           └── part-0-0_batchno0-0-3-1572695446456.carbondata
├── LockFiles
│   ├── compaction.lock
│   ├── Segment_0.lock
│   ├── Segment_1.lock
│   ├── Segment_2.lock
│   ├── Segment_3.lock
│   ├── tablestatus.lock
│   └── update.lock
└── Metadata
    ├── schema
    ├── segments
    │   ├── <span class="m">0</span>.1_1572695501043.segment
    │   ├── 0_1572694887333.segment
    │   ├── 1_1572694899093.segment
    │   ├── 2_1572694907902.segment
    │   └── 3_1572695446456.segment
    └── tablestatus
</pre></div>


<p>A new segment with ID <code>0.1</code> has been created, while the other segments are marked as compacted. We had to create 4 segments because the default setting for the parameter <code>carbon.compaction.level.threshold</code>, which controls the way minor compactions are performed, is <code>(4,3)</code>; it means that 4 segments will be compacted in a "level 1" new segment, and 3 "level 1" segments (when present) will be compacted to a single "level 2" segment. More information on the other configuration parameters can be found on the <a href="https://carbondata.apache.org/configuration-parameters.html">website</a>.</p>
<p>After compaction, the segments which have been compacted are still present; if we want to delete them and free up the space, we can use the following SQL command:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;CLEAN FILES FOR TABLE orders&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>which will give the following segments:</p>
<div class="highlight"><pre><span></span>+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
|SegmentSequenceId| Status|     Load Start Time|       Load End Time|Merged To|File Format|Data Size|Index Size|
+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
|              0.1|Success|2019-11-02 12:51:...|2019-11-02 12:51:...|       NA|COLUMNAR_V3| 146,33MB|    2,81KB|
+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
</pre></div>


<p>and the following directory structure:</p>
<div class="highlight"><pre><span></span>$ tree /var/carbondata/data/store/default/orders

/var/carbondata/data/store/default/orders
├── Fact
│   └── Part0
│       └── Segment_0.1
│           ├── <span class="m">0</span>.1_1572695514980.carbonindexmerge
│           └── part-0-0_batchno0-0-0.1-1572695501043.carbondata
├── LockFiles
│   ├── clean_files.lock
│   ├── compaction.lock
│   ├── Segment_0.lock
│   ├── Segment_1.lock
│   ├── Segment_2.lock
│   ├── Segment_3.lock
│   ├── tablestatus.lock
│   └── update.lock
└── Metadata
    ├── schema
    ├── segments
    │   └── <span class="m">0</span>.1_1572695501043.segment
    ├── tablestatus
    └── tablestatus.history
</pre></div>


<p>The major compaction works in a similar way and it is performed by running the following command:</p>
<div class="highlight"><pre><span></span><span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;ALTER TABLE orders COMPACT &#39;MAJOR&#39;&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>By default (parameter <code>carbon.major.compaction.size</code>) the compaction will only take place on the segments whose sum of the sizes is below 1024 MB. You can try this out as well by loading data once more and running the SQL command.</p>
<h3 id="update-and-delete-operations">Update and Delete operations</h3>
<p>There is another type of compaction, called <em>horizontal compaction</em>, that takes place on delta files created by Update and Delete operations. Whenever an Update is performed, two files are created:</p>
<ul>
<li>an <em>Insert Delta</em> file which stores newly added rows in the CarbonData columnar format;</li>
<li>a <em>Delete Delta</em> file which only stores the IDs of the rows that are deleted in a Bitmap file format.</li>
</ul>
<p>For instance, if we start from this directory structure (clean table, one load):</p>
<div class="highlight"><pre><span></span>├── Fact
│   └── Part0
│       └── Segment_0
│           ├── 0_1572700087590.carbonindexmerge
│           └── part-0-0_batchno0-0-0-1572700080525.carbondata
├── LockFiles
│   ├── Segment_0.lock
│   └── tablestatus.lock
└── Metadata
    ├── schema
    ├── segments
    │   └── 0_1572700080525.segment
    └── tablestatus
</pre></div>


<p>and run this update:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">s&quot;&quot;&quot;</span>
<span class="s">    UPDATE orders</span>
<span class="s">    SET (o_orderdate) = (&#39;2016-06-06&#39;)</span>
<span class="s">    WHERE o_orderdate = &#39;1996-06-06&#39;</span>
<span class="s">&quot;&quot;&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>the directory will be updated like so:</p>
<div class="highlight"><pre><span></span>├── Fact
│   └── Part0
│       └── Segment_0
│           ├── 0_1572700087590.carbonindexmerge
│           ├── 1_batchno0-0-0-1572700111168.carbonindex
│           ├── part-0-0_batchno0-0-0-1572700080525.carbondata
│           ├── part-0-0_batchno0-0-0-1572700111168.deletedelta
│           └── part-0-1_batchno0-0-0-1572700111168.carbondata
├── LockFiles
│   ├── compaction.lock
│   ├── meta.lock
│   ├── Segment_0.lock
│   ├── Segment_.lock
│   ├── tablestatus.lock
│   ├── tableupdatestatus.lock
│   └── update.lock
└── Metadata
    ├── schema
    ├── segments
    │   ├── 0_1572700080525.segment
    │   └── 0_1572700111168.segment
    ├── tablestatus
└── tableupdatestatus-1572700111168
</pre></div>


<p>basically creating:</p>
<ul>
<li>a <code>.carbonindex</code> file;</li>
<li>a <code>.deletedelta</code> file;</li>
<li>a new <code>.carbondata</code> file;</li>
<li>a new <code>.segment</code> file;</li>
<li>a <code>tableupdatestatus-</code> file.</li>
</ul>
<p>A Delete operation, instead, creates the <em>Delete Delta</em> file only. Let's run another update:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">s&quot;&quot;&quot;</span>
<span class="s">    UPDATE orders</span>
<span class="s">    SET (o_orderdate) = (&#39;2016-06-05&#39;)</span>
<span class="s">    WHERE o_orderdate = &#39;1996-06-05&#39;</span>
<span class="s">&quot;&quot;&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>If we run the <code>tree</code> command we'll see that there will be one or more files for each of the 4 types in the list. Now, if we run the <code>CLEAN</code> command again:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;CLEAN FILES FOR TABLE orders&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>we will see that there is again only one file per type (except for the <code>.segment</code> files). This behaviour is controlled by the following parameters:</p>
<ul>
<li><code>carbon.horizontal.compaction.enable</code>: enables this kind of compaction (default is <code>true</code>);</li>
<li><code>carbon.horizontal.update.compaction.threshold</code>: number of <em>Update delta</em> files above which the compaction will take place (default is 1);</li>
<li><code>carbon.horizontal.delete.compaction.threshold</code>: number of <em>Delete delta</em> files above which the compaction will take place (default is 1).</li>
</ul>
<h2 id="partitioning">Partitioning</h2>
<p>Partitioning is a data organization strategy used to increase performance and isolation. CarbonData offers two kinds of partitions, a "standard" one (similar to <a href="https://spark.apache.org/docs/2.3.4/sql-programming-guide.html#partition-discovery">Spark</a> and Hive partitions) and a "CarbonData" one (that supports several partitioning schemes such as ranges, hashes, and lists, but no Update/Delete); since the latter is still experimental, we will only focus on the former.</p>
<p>A partitioned table can be created with the usual Spark/Hive <code>PARTITIONED BY</code> syntax:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">s&quot;&quot;&quot;</span>
<span class="s">    CREATE TABLE IF NOT EXISTS orders_part(</span>
<span class="s">        o_orderdate DATE,</span>
<span class="s">        o_orderpriority STRING,</span>
<span class="s">        o_orderkey INT,</span>
<span class="s">        o_custkey STRING,</span>
<span class="s">        o_totalprice DOUBLE,</span>
<span class="s">        o_clerk STRING,</span>
<span class="s">        o_shippriority INT,</span>
<span class="s">        o_comment STRING)</span>
<span class="s">    PARTITIONED BY(o_orderstatus STRING)</span>
<span class="s">    STORED AS carbondata</span>
<span class="s">    TBLPROPERTIES(&#39;SORT_COLUMNS&#39; = &#39;o_orderdate&#39;)</span>
<span class="s">  &quot;&quot;&quot;</span><span class="o">)</span>
</pre></div>


<p>paying attention to exclude the partitioning column from both the list of columns and from the <code>SORT_COLUMNS</code> table property (if present).</p>
<p>The data can be loaded in the usual way when using <em>dynamic partitioning</em>, i.e. without specifying the partition to write to:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">s&quot;&quot;&quot;</span>
<span class="s">    LOAD DATA INPATH &#39;/tmp/tpch-dbgen/orders.tbl&#39;</span>
<span class="s">    INTO TABLE orders_part</span>
<span class="s">    OPTIONS(</span>
<span class="s">        &#39;DELIMITER&#39; = &#39;|&#39;,</span>
<span class="s">        &#39;FILEHEADER&#39; = &#39;o_orderkey,o_custkey,o_orderstatus,o_totalprice,o_orderdate,o_orderpriority,o_clerk,o_shippriority,o_comment&#39;)</span>
<span class="s">  &quot;&quot;&quot;</span><span class="o">)</span>
</pre></div>


<p>and the partitioned column can be used to quickly filter data:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT COUNT(*) FROM orders_part WHERE o_orderstatus = &#39;F&#39;&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>The partitions can be shown using the following command:</p>
<div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">carbon</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SHOW PARTITIONS orders_part&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>and we can see their layout using the <code>tree</code> command on the new directory:</p>
<div class="highlight"><pre><span></span>$ tree /var/carbondata/data/store/default/orders_part

/var/carbondata/data/store/default/orders_part
├── LockFiles
│   ├── Segment_0.lock
│   └── tablestatus.lock
├── Metadata
│   ├── schema
│   ├── segments
│   │   └── 0_1572701993390.segment
│   └── tablestatus
├── <span class="nv">o_orderstatus</span><span class="o">=</span>F
│   ├── 0_1572702007421.carbonindexmerge
│   ├── part-0-100100000100001_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100001100001_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100002100001_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100003100001_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100004100001_batchno0-0-0-1572701993390.carbondata
│   └── part-0-100100005100001_batchno0-0-0-1572701993390.carbondata
├── <span class="nv">o_orderstatus</span><span class="o">=</span>O
│   ├── 0_1572702007422.carbonindexmerge
│   ├── part-0-100100000100002_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100001100002_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100002100002_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100003100002_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100004100002_batchno0-0-0-1572701993390.carbondata
│   └── part-0-100100005100002_batchno0-0-0-1572701993390.carbondata
├── <span class="nv">o_orderstatus</span><span class="o">=</span>P
│   ├── 0_1572702007422.carbonindexmerge
│   ├── part-0-100100000100003_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100001100003_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100002100003_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100003100003_batchno0-0-0-1572701993390.carbondata
│   ├── part-0-100100004100003_batchno0-0-0-1572701993390.carbondata
│   └── part-0-100100005100003_batchno0-0-0-1572701993390.carbondata
└── _SUCCESS
</pre></div>


<p>where we can see that, instead of the usual <code>Fact/Part0/Segment_0</code> structure, we have one directory per partition. With this structure, Update Delta and Delete Delta files are created within each folder. Whether partitions can make queries more efficient as opposed to MDKs and dictionaries will likely depend on the use case.</p>
<h2 id="conclusions">Conclusions</h2>
<p>As we have been digging deeper into the internals of CarbonData, we have discovered that there are not only many interesting features to cover different use cases, but also many tools to increase the performance. I would suggest to take a look at the list of <a href="https://carbondata.apache.org/configuration-parameters.html">configuration parameters</a> as well as at the list of <a href="https://carbondata.apache.org/usecases.html">use cases</a>, since they can provide some guidance in the (daunting) task getting the better out of CarbonData. Have fun!</p>
    </div>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <ul class="col-sm-6 list-inline">
    <li class="list-inline-item"><a href="https://apothem.blog/archives.html">Archives</a></li>
    <li class="list-inline-item"><a href="https://apothem.blog/categories.html">Categories</a></li>
      <li class="list-inline-item"><a href="https://apothem.blog/tags.html">Tags</a></li>
  </ul>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p>
</div>    </div>
  </footer>
</body>

</html>