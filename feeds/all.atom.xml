<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>APOTHEM</title><link href="https://apothem.blog/" rel="alternate"></link><link href="https://apothem.blog/feeds/all.atom.xml" rel="self"></link><id>https://apothem.blog/</id><updated>2020-03-01T00:00:00+00:00</updated><subtitle>Apache Project(s) of the month</subtitle><entry><title>Apache Hivemall (part 2)</title><link href="https://apothem.blog/apache-hivemall-part-2.html" rel="alternate"></link><published>2020-03-01T00:00:00+00:00</published><updated>2020-03-01T00:00:00+00:00</updated><author><name>Nicola Vitucci</name></author><id>tag:apothem.blog,2020-03-01:/apache-hivemall-part-2.html</id><summary type="html">&lt;h3&gt;Regression&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Regression_analysis"&gt;Regression analysis&lt;/a&gt; (or, in short, &lt;em&gt;regression&lt;/em&gt;) is the task of estimating a model that can predict the value of a (dependent) variable given the values of other (independent) variables usually called &lt;em&gt;features&lt;/em&gt;. For example, if we know the size and the number of bathrooms of a house in a â€¦&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Regression&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Regression_analysis"&gt;Regression analysis&lt;/a&gt; (or, in short, &lt;em&gt;regression&lt;/em&gt;) is the task of estimating a model that can predict the value of a (dependent) variable given the values of other (independent) variables usually called &lt;em&gt;features&lt;/em&gt;. For example, if we know the size and the number of bathrooms of a house in a neighbourhood, we might be able to predict its price if we have these data for a few other houses in the same neighbourhood. There are many algorithms to create regression models; as we will see, Hivemall offers a very convenient "general" regressor (along with other ones) that can be easily customized and used for prediction.&lt;/p&gt;
&lt;h4&gt;Data preprocessing&lt;/h4&gt;
&lt;p&gt;Let's download some data. We will use an extract of the &lt;a href="https://www.cs.cmu.edu/~ark/10K/"&gt;"10-K Corpus"&lt;/a&gt;, a collection of 10-K filings where &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/E2006.train.bz2 and https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/E2006.test.bz2&lt;/li&gt;
&lt;li&gt;bunzip2 both&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Launch Spark shell&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TEMPORARY FUNCTION extract_feature as &amp;#39;hivemall.ftvec.ExtractFeatureUDF&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TEMPORARY FUNCTION extract_weight as &amp;#39;hivemall.ftvec.ExtractWeightUDF&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TEMPORARY FUNCTION add_bias as &amp;#39;hivemall.ftvec.AddBiasUDF&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;:::&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;e2006Train&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;delimiter&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;E2006.train&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;e2006TrainTable&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;e2006Train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;monotonically_increasing_id&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;rowid&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e2006Train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)).&lt;/span&gt;&lt;span class="n"&gt;cast&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Float&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;target&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;array_except&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e2006Train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;e2006Train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="kt"&gt;*&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lit&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;))).&lt;/span&gt;&lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;features&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;// cannot use array_remove with null&lt;/span&gt;
        &lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;libsvm&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;???&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;e2006Test&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;delimiter&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;E2006.test&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;e2006TestTable&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;e2006Test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;monotonically_increasing_id&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;rowid&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e2006Test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)).&lt;/span&gt;&lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;target&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;array_except&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e2006Test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;e2006Test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="k"&gt;_&lt;/span&gt;&lt;span class="kt"&gt;*&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lit&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;))).&lt;/span&gt;&lt;span class="n"&gt;as&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;features&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;e2006TrainTable&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createOrReplaceTempView&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;e2006_train_table&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;e2006TestTable&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createOrReplaceTempView&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;e2006_test_table&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;:::&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;trained&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT feature, avg(weight) as weight FROM (SELECT train_regressor(features, target, &amp;#39;-loss squared -opt AdaGrad -reg No -iters 20&amp;#39;) AS (feature, weight) FROM e2006_train_table) t GROUP BY feature&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;:::&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT t.rowid, sum(m.weight * t.value) as predicted FROM e2006_test_table_exploded t LEFT OUTER JOIN regr_model m USING (feature) GROUP BY t.rowid&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createOrReplaceTempView&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;pred&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;:::&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;eval&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT rmse(predicted, actual) as RMSE, mse(predicted, actual) as MSE, mae(predicted, actual) as MAE, r2(predicted, actual) as R2 FROM (SELECT t.target actual, p.predicted predicted FROM e2006_test_table t JOIN pred p USING (rowid))&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Big Data"></category><category term="Machine Learning"></category></entry><entry><title>Apache Hivemall</title><link href="https://apothem.blog/apache-hivemall.html" rel="alternate"></link><published>2020-02-02T00:00:00+00:00</published><updated>2020-02-02T00:00:00+00:00</updated><author><name>Nicola Vitucci</name></author><id>tag:apothem.blog,2020-02-02:/apache-hivemall.html</id><summary type="html">&lt;p&gt;With this article we will move a little bit out of the data engineering space and delve into another subject I love: we will explore the world of distributed machine learning with &lt;a href="https://hivemall.incubator.apache.org/"&gt;Apache Hivemall&lt;/a&gt;. From the project's homepage:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Hivemall is a scalable machine learning library that runs on Apache Hive â€¦&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;With this article we will move a little bit out of the data engineering space and delve into another subject I love: we will explore the world of distributed machine learning with &lt;a href="https://hivemall.incubator.apache.org/"&gt;Apache Hivemall&lt;/a&gt;. From the project's homepage:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Hivemall is a scalable machine learning library that runs on Apache Hive, Spark and Pig.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In short, Hivemall is a collection of Hive UDFs (User-Defined Functions) to create, train, and evaluate machine learning models using any of the supported engines. Since Hive provides a SQL-like query language, Hivemall is basically &lt;strong&gt;machine learning in SQL&lt;/strong&gt;; it is then well suited to users who are already fluent in SQL or who need an additional abstraction layer on top of machine learning libraries.&lt;/p&gt;
&lt;p&gt;We will explore some of the many capabilities that Hivemall provides by tackling one of the most common machine learning tasks: classification. Let's start by setting up the environment.&lt;/p&gt;
&lt;h3&gt;Setting up Hivemall and Spark&lt;/h3&gt;
&lt;p&gt;Like for other Java-based Apache projects, we will need to install git, Java 8, and Maven 3.5+ (check &lt;a href="https://apothem.blog/apache-atlas.html"&gt;the article on Apache Atlas&lt;/a&gt; if you need a detailed walkthrough). Once we have installed everything, we need to build Hivemall:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone https://github.com/apache/incubator-hivemall.git
$ &lt;span class="nb"&gt;cd&lt;/span&gt; incubator-hivemall
$ bin/build.sh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since Hive is a bit cumbersome to install and configure, and we will need to do a bit of data preprocessing as well, we will run Hive queries from Spark instead. We therefore need to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;download Spark 2.4.4 from &lt;a href="https://spark.apache.org/downloads.html"&gt;this page&lt;/a&gt; (choosing the 2.4.4 release pre-built for Hadoop 2.7);&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;extract the content of the .tgz file into a directory, possibly alongside the directory where we cloned the Hivemall repository:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tar zxvf spark-2.4.4-bin-hadoop2.7.tgz
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;launch the Spark shell with the Hivemall JAR file (release &lt;code&gt;0.6.2-SNAPSHOT&lt;/code&gt; as of now) built before:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ spark-2.4.4-bin-hadoop2.7/bin/spark-shell &lt;span class="se"&gt;\&lt;/span&gt;
  --jars incubator-hivemall/target/hivemall-all-0.6.2-incubating-SNAPSHOT.jar
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once the Spark shell is running, we need to load all the Hivemall functions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;:&lt;/span&gt;&lt;span class="kt"&gt;load&lt;/span&gt; &lt;span class="kt"&gt;incubator-hivemall/resources/ddl/define-all.spark&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We will likely see some errors because some Hivemall functions already exist in Spark 2.4.4, but there is no need to worry because the Spark functions will not be overridden. We are now ready to start using Hivemall!&lt;/p&gt;
&lt;h3&gt;Classification&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Classification_(machine_learning)"&gt;Classification&lt;/a&gt; is the task of assigning a category to an &lt;em&gt;observation&lt;/em&gt; based on the value of its &lt;em&gt;features&lt;/em&gt;. For example, we may have some emails that we want to classify as "spam" or "non-spam" based on the presence of one or more keywords; this is called &lt;em&gt;binary classification&lt;/em&gt; because we only have two categories, or one category that an instance belongs to or not. Another example, which is a sort of "Hello world" for machine learning and classification, is the categorization of a specific variant of the &lt;a href="https://en.wikipedia.org/wiki/Iris_(plant)"&gt;iris&lt;/a&gt; plant depending on features such as the length and width of its sepals and petals; this is called &lt;em&gt;multiclass classification&lt;/em&gt; because there are more than two different classes. We will use the &lt;a href="https://archive.ics.uci.edu/ml/datasets/Iris"&gt;Iris Data Set&lt;/a&gt; in this section.&lt;/p&gt;
&lt;p&gt;The algorithm we will use is called &lt;a href="https://xgboost.readthedocs.io/en/latest/"&gt;XGBoost&lt;/a&gt;. There are several reasons behind this choice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I like its conceptual simplicity and its &lt;em&gt;interpretability&lt;/em&gt;, which is an important feature of modern machine learning algorithms;&lt;/li&gt;
&lt;li&gt;it is designed for efficiency and has already proved itself;&lt;/li&gt;
&lt;li&gt;it does not require a lot of preprocessing (for instance, it does not require feature scaling);&lt;/li&gt;
&lt;li&gt;it is now supported in Hivemall starting with version 0.6.0 (the supported XGBoost version being 0.90).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are interested in the theoretical and technical details, do take a look at the excellent documentation &lt;a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html"&gt;here&lt;/a&gt;. For our purposes, we need to keep in mind just one thing: simplifying a lot, XGBoost will basically give us a group of decision trees.&lt;/p&gt;
&lt;h4&gt;Data preprocessing&lt;/h4&gt;
&lt;p&gt;First of all, we need to download our dataset from &lt;a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"&gt;here&lt;/a&gt; and put it alongside the previous directories. The data will look like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
...
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's go back to our Spark shell and run the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;inferSchema&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;iris.data&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;toDF&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;sepal_length&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;sepal_width&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;petal_length&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;petal_width&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;class&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This command will read the dataset file interpreting it as a CSV file, automatically creating a schema and assigning the provided names to the columns. We can check both the schema and the first 5 rows of the dataset:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;printSchema&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we can create a temporary view of our Spark Dataframe so that we can use it like a Hive table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createOrReplaceTempView&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;iris&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and then we can run a SQL query as a check:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT * FROM iris&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- This is commented out. --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+------------+-----------+------------+-----------+-----------+
|sepal_length|sepal_width|petal_length|petal_width|      class|
+------------+-----------+------------+-----------+-----------+
|         5.1|        3.5|         1.4|        0.2|Iris-setosa|
|         4.9|        3.0|         1.4|        0.2|Iris-setosa|
|         4.7|        3.2|         1.3|        0.2|Iris-setosa|
|         4.6|        3.1|         1.5|        0.2|Iris-setosa|
|         5.0|        3.6|         1.4|        0.2|Iris-setosa|
+------------+-----------+------------+-----------+-----------+
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We have our raw data table, but in order to run any Hivemall algorithm we need to tweak it a little. More specifically, we need to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;assign a unique ID to each row;&lt;/li&gt;
&lt;li&gt;create a &lt;em&gt;feature vector&lt;/em&gt; such as &lt;code&gt;[1:5.1, 2:3.5, 3:1.4, 4:0.2]&lt;/code&gt;, that is a string array where each element is of the form &lt;code&gt;feature:value&lt;/code&gt;, from the feature columns;&lt;/li&gt;
&lt;li&gt;transform the iris "class" from a string to an integer value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These steps are part of a very important phase of a machine learning pipeline called &lt;em&gt;feature engineering&lt;/em&gt;. We can create a new table with all these transformations with the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    CREATE TABLE iris_processed AS&lt;/span&gt;
&lt;span class="s"&gt;    SELECT monotonically_increasing_id() rowid,&lt;/span&gt;
&lt;span class="s"&gt;    indexed_features(sepal_length, sepal_width, petal_length, petal_width) features,&lt;/span&gt;
&lt;span class="s"&gt;    quantify(true, class) label&lt;/span&gt;
&lt;span class="s"&gt;    FROM iris&lt;/span&gt;
&lt;span class="s"&gt;    ORDER BY class&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;While &lt;code&gt;monotonically_increasing_id&lt;/code&gt; is a Spark function, &lt;code&gt;indexed_features&lt;/code&gt; and &lt;code&gt;quantify&lt;/code&gt; are both Hivemall UDFs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;indexed_features&lt;/code&gt; will create an array from the values passed as parameters, prepending each value with its position in the array (starting from 1, not 0, since 0 is reserved for the &lt;a href="https://hivemall.incubator.apache.org/userguide/tips/addbias.html"&gt;bias feature&lt;/a&gt;);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;quantify&lt;/code&gt; can be used to convert a categorical variable into a numerical variable (the &lt;code&gt;ORDER BY&lt;/code&gt; is used to make sure that it is applied on one reducer only, thus consistently associating the same number to the same non-numerical variable).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are &lt;a href="https://hivemall.incubator.apache.org/userguide/tips/rowid.html"&gt;other ways&lt;/a&gt; to create a unique ID for each row; Hivemall provides a function called &lt;code&gt;rowid&lt;/code&gt;, but it does not work in Spark (it would throw a &lt;code&gt;java.lang.IllegalStateException: MapredContext is not set&lt;/code&gt; exception).&lt;/p&gt;
&lt;p&gt;Let's see how the new table looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT * FROM iris_processed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- This is commented out. --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+-----+----------------------------+-----+
|rowid|features                    |label|
+-----+----------------------------+-----+
|0    |[1:5.1, 2:3.5, 3:1.4, 4:0.2]|0    |
|1    |[1:4.9, 2:3.0, 3:1.4, 4:0.2]|0    |
|2    |[1:4.7, 2:3.2, 3:1.3, 4:0.2]|0    |
|3    |[1:4.6, 2:3.1, 3:1.5, 4:0.2]|0    |
|4    |[1:5.0, 2:3.6, 3:1.4, 4:0.2]|0    |
+-----+----------------------------+-----+
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The table is ready to be used for training.&lt;/p&gt;
&lt;h4&gt;Training&lt;/h4&gt;
&lt;p&gt;We need to add one more function first (which should be included in the &lt;code&gt;define-all.spark&lt;/code&gt; file now, but are not at the moment):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TEMPORARY FUNCTION train_xgboost AS &amp;#39;hivemall.xgboost.XGBoostTrainUDTF&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also need to set the &lt;code&gt;spark.sql.shuffle.partitions&lt;/code&gt; (or &lt;code&gt;mapred.reduce.tasks&lt;/code&gt; if we use Hive directly, not through Spark) configuration parameter to a "low" number, for instance to 10 (instead of the default 200).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SET spark.sql.shuffle.partitions=10&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we don't do this and the dataset is small (as in our case, with only 150 rows), we will have some empty partitions which will cause an error like the following during the training phase:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;...
Caused by: ml.dmlc.xgboost4j.java.XGBoostError: [01:14:14] /home/travis/build/myui/build-xgboost-jvm/xgboost/src/learner.cc:723: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
...
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's check that there are no empty partitions by running this command, which should give no output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT * FROM iris_processed CLUSTER BY rand(0)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;foreachPartition&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;part&lt;/span&gt; &lt;span class="k"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;part&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;Zero-length partition&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Another way to check is to run the following command, which should return the same number of partitions as the value of the &lt;code&gt;spark.sql.shuffle.partitions&lt;/code&gt; parameter:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT DISTINCT spark_partition_id() FROM (SELECT * FROM iris_processed CLUSTER BY rand(0))&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(Try to set &lt;code&gt;spark.sql.shuffle.partitions&lt;/code&gt; to 50 or 100 and run the previous commands to see a different behaviour.)&lt;/p&gt;
&lt;p&gt;Finally we can create the model:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    CREATE TABLE iris_model_softmax AS&lt;/span&gt;
&lt;span class="s"&gt;    SELECT train_xgboost(features, label, &amp;#39;-objective multi:softmax -num_class 3&amp;#39;) AS (model_id, model)&lt;/span&gt;
&lt;span class="s"&gt;    FROM (SELECT features, label FROM iris_processed CLUSTER BY rand(0))&lt;/span&gt;
&lt;span class="s"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When performing a multiclass classification task, we have to explicitly specify the &lt;code&gt;-num_class&lt;/code&gt; parameter; we have 3 classes in our dataset, therefore we set it to 3. The other parameter is the objective function: &lt;code&gt;multi:softmax&lt;/code&gt; will use the &lt;code&gt;softmax&lt;/code&gt; objective and return only the predicted class, while &lt;code&gt;multi:softprob&lt;/code&gt; will use the &lt;code&gt;softprob&lt;/code&gt; objective and return the probability associated to each class (see &lt;a href="https://xgboost.readthedocs.io/en/release_0.90/parameter.html#learning-task-parameters"&gt;the XGBoost docs&lt;/a&gt; for more information, and for the other parameters). A model using the &lt;code&gt;softprob&lt;/code&gt; objective can be created as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;scala&amp;gt; spark.sql(&amp;quot;&amp;quot;&amp;quot;
    CREATE TABLE iris_model_softprob AS
    SELECT train_xgboost(features, label, &amp;#39;-objective multi:softprob -num_class 3&amp;#39;) AS (model_id, model)
    FROM (SELECT features, label FROM iris_processed CLUSTER BY rand(0))
    &amp;quot;&amp;quot;&amp;quot;).show
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Another important consideration is the use of &lt;code&gt;CLUSTER BY rand(0)&lt;/code&gt;, which is used to shuffle the dataset in order to prevent any bias. Please note that, for simplicity, we didn't split the dataset into a training and a test set; in real-world applications this should always be done.&lt;/p&gt;
&lt;p&gt;Now that the model is trained, we can read it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;modelArray&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT * FROM iris_model_softprob&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;take&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;toString&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and then visualize it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.hadoop.io.Text&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hivemall.xgboost.utils.XGBoostUtils.deserializeBooster&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;booster&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;deserializeBooster&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nc"&gt;Text&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;modelArray&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;booster&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getModelDump&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;Array&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;](),&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;json&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;mkString&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;\n&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The parameters of the &lt;code&gt;getModelDump&lt;/code&gt; method are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;an array of feature names (potentially empty, to map the default &lt;code&gt;f0...fn&lt;/code&gt; names to user-defined names);&lt;/li&gt;
&lt;li&gt;a boolean to include/exclude the split statistics;&lt;/li&gt;
&lt;li&gt;the output format (at the moment only &lt;code&gt;"text"&lt;/code&gt; or &lt;code&gt;"json"&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also get the importance of each feature:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;booster&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getFeatureScore&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;Array&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;]()))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- Empty --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{f1=18, f2=2, f3=17}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and the feature importances for several measures (one of &lt;code&gt;"weight"&lt;/code&gt;, &lt;code&gt;"gain"&lt;/code&gt;, &lt;code&gt;"cover"&lt;/code&gt;, &lt;code&gt;"total_gain"&lt;/code&gt;, &lt;code&gt;"total_cover"&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;booster&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getScore&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nc"&gt;Array&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;](),&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gain&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- Empty --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{f1=1.4596930238833332, f2=0.0668432191, f3=1.5014036119411764}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can customize the feature names in both methods:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;featureNames&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="nc"&gt;Array&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;bias&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;sepal_length&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;sepal_width&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;petal_length&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;petal_width&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;booster&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getFeatureScore&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;featureNames&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;booster&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getScore&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;featureNames&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;gain&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;!-- Empty --&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{sepal_width=2, sepal_length=18, petal_length=17}
{sepal_width=0.0668432191, sepal_length=1.4596930238833332, petal_length=1.5014036119411764}
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Prediction&lt;/h4&gt;
&lt;p&gt;Now that the model is ready, we can use it to classify some data. It is important to note that the way Hivemall makes XGBoost parallel is somewhat different from the parallel implementation of the official library; in fact, instead of using the &lt;a href="https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html"&gt;AllReduce paradigm&lt;/a&gt; where all the parallel workers have to communicate, Hivemall uses a &lt;a href="https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier"&gt;voting classifier&lt;/a&gt; on top of many &lt;em&gt;independent&lt;/em&gt; XGBoost learners. In this way every worker can train its own local model based on its partition of data (possibly overlapping with other partitions if the &lt;code&gt;amplify&lt;/code&gt; or &lt;code&gt;rand_amplify&lt;/code&gt; functions are used), and the actual prediction happens via a majority vote (if the objective function is &lt;code&gt;softmax&lt;/code&gt;, since its output is a class label) or by averaging (if the objective function is &lt;code&gt;softprob&lt;/code&gt;, since its output is a list of probabilities). We will see both examples.&lt;/p&gt;
&lt;p&gt;Like we did for training, we need first of all to include a few more functions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TEMPORARY FUNCTION xgboost_predict_one AS &amp;#39;hivemall.xgboost.XGBoostPredictOneUDTF&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TEMPORARY FUNCTION xgboost_predict_triple AS &amp;#39;hivemall.xgboost.XGBoostPredictTripleUDTF&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TEMPORARY FUNCTION majority_vote AS &amp;#39;hivemall.tools.aggr.MajorityVoteUDAF&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and to set another configuration option:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SET spark.sql.crossJoin.enabled=true&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is necessary because Hivemall uses a left join without condition (basically a cross join) between the model and the data to make predictions, and cross joins are disabled by default for performance reasons.&lt;/p&gt;
&lt;p&gt;Let's now create a table containing our predictions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    CREATE TABLE iris_pred_softmax AS&lt;/span&gt;
&lt;span class="s"&gt;    SELECT rowid, majority_vote(CAST(predicted AS INT)) AS label&lt;/span&gt;
&lt;span class="s"&gt;    FROM (&lt;/span&gt;
&lt;span class="s"&gt;        SELECT xgboost_predict_one(rowid, features, model_id, model) AS (rowid, predicted)&lt;/span&gt;
&lt;span class="s"&gt;        FROM iris_model_softmax l LEFT JOIN iris_processed r) t&lt;/span&gt;
&lt;span class="s"&gt;    GROUP BY rowid&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We use the &lt;code&gt;xgboost_predict_one&lt;/code&gt; function to predict the class of each row of the &lt;code&gt;iris_processed&lt;/code&gt; table with the model stored in the &lt;code&gt;model&lt;/code&gt; column of the &lt;code&gt;iris_model_softmax&lt;/code&gt; table; the &lt;code&gt;rowid&lt;/code&gt; is stored as well to make the model evaluation easier later on. In this case we use the &lt;code&gt;majority_vote&lt;/code&gt; function to "vote" for the most frequent prediction, which will become the actual prediction. We can check the predicted values from the new table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT * FROM iris_pred_softmax ORDER BY rowid&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once again, keep in mind that for this example we didn't split the dataset into a training set and a test set. If we want to get the probability of each class assignment, instead, we can use the &lt;code&gt;xgboost_predict_triple&lt;/code&gt; function as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    SELECT rowid, argmax(collect_list(avg_proba)) label, collect_list(avg_proba) proba&lt;/span&gt;
&lt;span class="s"&gt;    FROM (&lt;/span&gt;
&lt;span class="s"&gt;        CREATE TABLE iris_pred_softprob AS&lt;/span&gt;
&lt;span class="s"&gt;        SELECT rowid, label, avg(proba) avg_proba&lt;/span&gt;
&lt;span class="s"&gt;        FROM (&lt;/span&gt;
&lt;span class="s"&gt;            SELECT xgboost_predict_triple(rowid, features, model_id, model) AS (rowid, label, proba)&lt;/span&gt;
&lt;span class="s"&gt;            FROM iris_model_softprob l LEFT JOIN iris_processed r)&lt;/span&gt;
&lt;span class="s"&gt;        GROUP BY rowid, label ORDER BY rowid, label)&lt;/span&gt;
&lt;span class="s"&gt;    GROUP BY rowid ORDER BY rowid&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The prediction works in the same way as before, with three important differences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the prediction function now returns three values instead of two, including the probability as well;&lt;/li&gt;
&lt;li&gt;we do not "vote" for the most frequent class, but rather we average the probabilities of each class and select the class with the maximum average probability;&lt;/li&gt;
&lt;li&gt;the model we run the prediction against has to be created with the &lt;code&gt;multi:softprob&lt;/code&gt; objective, otherwise the &lt;code&gt;xgboost_predict_triple&lt;/code&gt; will return the wrong results.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also use two simpler functions, &lt;code&gt;xgboost_predict&lt;/code&gt; and &lt;code&gt;xgboost_batch_predict&lt;/code&gt;, to obtain the same result (look at the &lt;a href="https://hivemall.incubator.apache.org/userguide/binaryclass/news20b_xgboost.html"&gt;examples&lt;/a&gt; for more details):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TEMPORARY FUNCTION xgboost_predict AS &amp;#39;hivemall.xgboost.XGBoostOnlinePredictUDTF&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TEMPORARY FUNCTION xgboost_batch_predict AS &amp;#39;hivemall.xgboost.XGBoostBatchPredictUDTF&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    CREATE TABLE iris_pred_softprob AS&lt;/span&gt;
&lt;span class="s"&gt;    SELECT rowid, argmax(array_avg(predicted)), array_avg(predicted) as prob&lt;/span&gt;
&lt;span class="s"&gt;    FROM (&lt;/span&gt;
&lt;span class="s"&gt;        SELECT xgboost_predict(rowid, features, model_id, model) AS (rowid, predicted)&lt;/span&gt;
&lt;span class="s"&gt;        FROM iris_model_softprob l LEFT JOIN iris_processed r)&lt;/span&gt;
&lt;span class="s"&gt;    GROUP BY rowid ORDER BY rowid&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    CREATE TABLE iris_pred_softprob AS&lt;/span&gt;
&lt;span class="s"&gt;    SELECT rowid, argmax(array_avg(predicted)), array_avg(predicted) as prob&lt;/span&gt;
&lt;span class="s"&gt;    FROM (&lt;/span&gt;
&lt;span class="s"&gt;        SELECT xgboost_batch_predict(rowid, features, model_id, model) AS (rowid, predicted)&lt;/span&gt;
&lt;span class="s"&gt;        FROM iris_model_softprob l LEFT JOIN iris_processed r)&lt;/span&gt;
&lt;span class="s"&gt;    GROUP BY rowid ORDER BY rowid&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;array_avg&lt;/code&gt; function is used to calculate the average of an array column. Again, we can check the predicted values:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT * FROM iris_pred_softprob ORDER BY rowid&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Evaluation&lt;/h4&gt;
&lt;p&gt;It is time now to evaluate our models. Since we performed our predictions on the same dataset that we used for training, we would expect the models to perform really good. Let's start with the &lt;code&gt;softmax&lt;/code&gt; model:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scala&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    SELECT sum(if(actual=predicted, 1.0, 0.0))/count(1)&lt;/span&gt;
&lt;span class="s"&gt;    FROM (&lt;/span&gt;
&lt;span class="s"&gt;        SELECT source.label actual, pred.label predicted&lt;/span&gt;
&lt;span class="s"&gt;        FROM iris_processed source JOIN iris_pred_softmax pred USING (rowid))&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here we are simply counting all the instances where the actual value corresponds to the predicted value (using the &lt;code&gt;rowid&lt;/code&gt; column to join the training dataset and the predicted dataset), then we divide that number by the number of all the instances to get a ratio. As expected, the ratio is really high (even though not exactly 1.0); we can also run exactly the same query by just swapping &lt;code&gt;iris_pred_softmax&lt;/code&gt; with &lt;code&gt;iris_pred_softprob&lt;/code&gt; to evaluate the second model. If we do not find very close values, the reason can be parallelization (since each partition calculates its own model); in order to get very close results we would need to reduce the &lt;code&gt;spark.sql.shuffle.partitions&lt;/code&gt; value to one, effectively making the whole computation happen in only one partition by creating a single XGBoost model, which will contain the full dataset (obviously not recommended for large datasets).&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;We have seen how Hivemall makes the XGBoost algorithm (and the classification task in general) very easy to implement and to understand. Of course there is much more to it, for instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;XGBoost can be configured with many different parameters, so it's worth reading its &lt;a href="https://hivemall.incubator.apache.org/userguide/binaryclass/news20b_xgboost.html"&gt;Hivemall usage guide&lt;/a&gt; and its &lt;a href="https://xgboost.readthedocs.io/en/release_0.90/parameter.html"&gt;configuration docs&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;XGBoost can be used also for &lt;a href="https://hivemall.incubator.apache.org/userguide/binaryclass/news20b_xgboost.html"&gt;binary classification&lt;/a&gt; and for &lt;a href="https://hivemall.incubator.apache.org/userguide/regression/e2006_xgboost.html"&gt;regression&lt;/a&gt; tasks;&lt;/li&gt;
&lt;li&gt;Hivemall features other algorithms for classification, including a &lt;a href="https://hivemall.incubator.apache.org/userguide/binaryclass/a9a_generic.html"&gt;general and highly customizable classifier&lt;/a&gt;, &lt;a href="https://hivemall.incubator.apache.org/userguide/binaryclass/a9a_lr.html"&gt;logistic regression&lt;/a&gt;, and &lt;a href="https://hivemall.incubator.apache.org/userguide/binaryclass/a9a_minibatch.html"&gt;mini-batch gradient descent&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are many other machine learning tasks to cover, such as &lt;a href="https://en.wikipedia.org/wiki/Regression_analysis"&gt;regression&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Cluster_analysis"&gt;clustering&lt;/a&gt;, and we have just barely touched on feature engineering. So, stay tuned for another article on this very interesting project!&lt;/p&gt;</content><category term="Big Data"></category><category term="Machine Learning"></category></entry><entry><title>Apache CarbonData (part 2)</title><link href="https://apothem.blog/apache-carbondata-part-2.html" rel="alternate"></link><published>2019-10-31T00:00:00+00:00</published><updated>2019-10-31T00:00:00+00:00</updated><author><name>Nicola Vitucci</name></author><id>tag:apothem.blog,2019-10-31:/apache-carbondata-part-2.html</id><summary type="html">&lt;p&gt;In the &lt;a href="https://apothem.blog/apache-carbondata.html"&gt;previous article&lt;/a&gt; we have seen many exciting features that CarbonData offers, but we haven't explored them all; in this article we will try out the streaming capabilities and we will delve a bit deeper into the data layout, looking at concept like compaction and partitioning, and the way â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the &lt;a href="https://apothem.blog/apache-carbondata.html"&gt;previous article&lt;/a&gt; we have seen many exciting features that CarbonData offers, but we haven't explored them all; in this article we will try out the streaming capabilities and we will delve a bit deeper into the data layout, looking at concept like compaction and partitioning, and the way the different files are managed.&lt;/p&gt;
&lt;h2&gt;Streaming&lt;/h2&gt;
&lt;p&gt;Another feature that sets CarbonData apart from other Hadoop data formats is the support for streaming data. Rather than writing many small files or waiting until a larger file can be written, CarbonData uses a slightly different format where single rows can be added as they arrive; such files can later be converted to the standard columnar format (via compaction) in order to support all the features we have already discussed. Every task, including the creation of the sink table and the management of the streaming job, can be performed using SQL commands. Let's see how.&lt;/p&gt;
&lt;p&gt;First of all, we need to create a streaming source. An easy way to do this is to use &lt;a href="https://en.wikipedia.org/wiki/Netcat"&gt;netcat&lt;/a&gt;, a Linux utility that, among many other things, can be used to send data through a socket, thus simulating a process streaming data over the network. We need to open a new terminal (or to use another utility like &lt;a href="https://en.wikipedia.org/wiki/GNU_Screen"&gt;screen&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Tmux"&gt;tmux&lt;/a&gt;) in order to keep &lt;em&gt;netcat&lt;/em&gt; running while we use the Spark shell; on the new terminal we need to run the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ nc -lk &lt;span class="m"&gt;9099&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which will seem to be "hanging up", while it will actually be waiting for our input. Let's leave it for now and switch to our running Spark shell with a CarbonData session active.&lt;/p&gt;
&lt;p&gt;We need to create a source table that CarbonData will use to interpret the format of the incoming data. For instance, if we want to stream data in CSV format where the first field is an integer and the second is a string, we can create the table like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    CREATE TABLE source(col1 INT, col2 STRING)&lt;/span&gt;
&lt;span class="s"&gt;    STORED AS carbondata&lt;/span&gt;
&lt;span class="s"&gt;    TBLPROPERTIES(&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;streaming&amp;#39; = &amp;#39;source&amp;#39;,&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;format&amp;#39; = &amp;#39;socket&amp;#39;,&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;host&amp;#39; = &amp;#39;localhostâ€™,&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;port&amp;#39; = &amp;#39;9099&amp;#39;,&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;record_format&amp;#39; = &amp;#39;csv&amp;#39;,&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;delimiter&amp;#39; = &amp;#39;,&amp;#39;)&lt;/span&gt;
&lt;span class="s"&gt;   &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The table properties should be self-explanatory; they are basically Spark Structured Streaming &lt;a href="https://spark.apache.org/docs/2.3.4/structured-streaming-programming-guide.html#creating-streaming-dataframes-and-streaming-datasets"&gt;DataStreamReader&lt;/a&gt;'s options. Once the source table is created, we need to create a sink table as well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    CREATE TABLE sink(col1 INT, col2 STRING)&lt;/span&gt;
&lt;span class="s"&gt;    STORED AS carbondata&lt;/span&gt;
&lt;span class="s"&gt;    TBLPROPERTIES(&amp;#39;streaming&amp;#39; = &amp;#39;true&amp;#39;)&lt;/span&gt;
&lt;span class="s"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this case our sink table mimics the source table exactly, but this is not mandatory. Now we are ready to create the actual streaming job:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    CREATE STREAM job1 ON TABLE sink&lt;/span&gt;
&lt;span class="s"&gt;    STMPROPERTIES(&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;trigger&amp;#39; = &amp;#39;ProcessingTime&amp;#39;,&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;interval&amp;#39; = &amp;#39;1 seconds&amp;#39;)&lt;/span&gt;
&lt;span class="s"&gt;    AS&lt;/span&gt;
&lt;span class="s"&gt;        SELECT *&lt;/span&gt;
&lt;span class="s"&gt;        FROM source&lt;/span&gt;
&lt;span class="s"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which is translated as a Spark &lt;a href="https://spark.apache.org/docs/2.3.4/structured-streaming-programming-guide.html#starting-streaming-queries"&gt;Streaming Query&lt;/a&gt;. That's all the setup we need.&lt;/p&gt;
&lt;p&gt;If we run a &lt;code&gt;SELECT&lt;/code&gt; query on the sink table, we will not see any results; this is normal because we haven't streamed any data yet. Let's switch to the terminal where &lt;em&gt;netcat&lt;/em&gt; is running and add a few lines like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1,value1
2,value2
10,value10
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;then switch back to the Spark shell and run a &lt;code&gt;SELECT&lt;/code&gt; query on the sink table again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT * FROM sink&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we should see the rows we have just inserted!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+----+-------+
|col1|   col2|
+----+-------+
|   1| value1|
|   2| value2|
|  10|value10|
+----+-------+
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;While the streaming job is running, we can see some information on the job itself by running the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SHOW STREAMS&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To terminate the streaming job we need to run a &lt;code&gt;DROP&lt;/code&gt; command using the stream name:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;DROP STREAM job1&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After closing the stream, we could decide to &lt;em&gt;compact&lt;/em&gt; the sink table and convert its files to the standard CarbonData column-based format. Let's check the table's &lt;em&gt;segments&lt;/em&gt; first:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SHOW SEGMENTS FOR TABLE sink&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which should output something like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+-----------------+---------+--------------------+-------------+---------+-----------+---------+----------+
|SegmentSequenceId|   Status|     Load Start Time|Load End Time|Merged To|File Format|Data Size|Index Size|
+-----------------+---------+--------------------+-------------+---------+-----------+---------+----------+
|                0|Streaming|2019-10-31 22:46:...|           NA|       NA|     ROW_V1|   634.0B|    174.0B|
+-----------------+---------+--------------------+-------------+---------+-----------+---------+----------+
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's now compact the table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ALTER TABLE sink COMPACT &amp;#39;CLOSE_STREAMING&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and check the segments again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
|SegmentSequenceId|   Status|     Load Start Time|       Load End Time|Merged To|File Format|Data Size|Index Size|
+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
|                1|  Success|2019-10-31 23:04:...|2019-10-31 23:04:...|       NA|COLUMNAR_V3|   1,22KB|    744.0B|
|                0|Compacted|2019-10-31 22:46:...|2019-10-31 23:04:...|        1|     ROW_V1|   634.0B|    174.0B|
+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The format of the data has actually changed, and it cannot be changed back to &lt;code&gt;ROW_V1&lt;/code&gt;. We'll see compaction into more detail in the next section.&lt;/p&gt;
&lt;h2&gt;Compaction&lt;/h2&gt;
&lt;p&gt;Whenever we load data into CarbonData, a new folder called &lt;em&gt;segment&lt;/em&gt; is created; this makes dealing with transactions easier and helps maintaining consistency. In the long run, anyway, the growing number of segments degrades the query performance; &lt;em&gt;compaction&lt;/em&gt; can then be used to merge multiple segments into one.&lt;/p&gt;
&lt;p&gt;CarbonData supports two main types of compaction besides the &lt;code&gt;CLOSE_STREAMING&lt;/code&gt; that we've seen in the previous section, namely &lt;em&gt;minor&lt;/em&gt; (based on the number of new segments) and &lt;em&gt;major&lt;/em&gt; (based on the size of the new segments). We'll see how they work using another table from the TPCH benchmark.&lt;/p&gt;
&lt;p&gt;Let's create the &lt;code&gt;order&lt;/code&gt; table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    CREATE TABLE IF NOT EXISTS orders(&lt;/span&gt;
&lt;span class="s"&gt;        o_orderdate DATE,&lt;/span&gt;
&lt;span class="s"&gt;        o_orderpriority STRING,&lt;/span&gt;
&lt;span class="s"&gt;        o_orderstatus STRING,&lt;/span&gt;
&lt;span class="s"&gt;        o_orderkey INT,&lt;/span&gt;
&lt;span class="s"&gt;        o_custkey STRING,&lt;/span&gt;
&lt;span class="s"&gt;        o_totalprice DOUBLE,&lt;/span&gt;
&lt;span class="s"&gt;        o_clerk STRING,&lt;/span&gt;
&lt;span class="s"&gt;        o_shippriority INT,&lt;/span&gt;
&lt;span class="s"&gt;        o_comment STRING)&lt;/span&gt;
&lt;span class="s"&gt;    STORED AS carbondata&lt;/span&gt;
&lt;span class="s"&gt;    TBLPROPERTIES(&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;SORT_COLUMNS&amp;#39; = &amp;#39;o_orderdate&amp;#39;)&lt;/span&gt;
&lt;span class="s"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and the load some data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    LOAD DATA INPATH &amp;#39;/tmp/tpch-dbgen/orders.tbl&amp;#39;&lt;/span&gt;
&lt;span class="s"&gt;    INTO TABLE orders&lt;/span&gt;
&lt;span class="s"&gt;    OPTIONS(&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;DELIMITER&amp;#39; = &amp;#39;|&amp;#39;,&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;FILEHEADER&amp;#39; = &amp;#39;o_orderkey,o_custkey,o_orderstatus,o_totalprice,o_orderdate,o_orderpriority,o_clerk,o_shippriority,o_comment&amp;#39;)&lt;/span&gt;
&lt;span class="s"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can now look at the segment information for this table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; carbon.sql(&amp;quot;SHOW SEGMENTS FOR TABLE orders&amp;quot;).show
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which returns:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
|SegmentSequenceId| Status|     Load Start Time|       Load End Time|Merged To|File Format|Data Size|Index Size|
+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
|                0|Success|2019-11-02 12:41:...|2019-11-02 12:41:...|       NA|COLUMNAR_V3|  51,20MB|    1,59KB|
+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and, if we look at the content of the CarbonData store directory containing the &lt;code&gt;order&lt;/code&gt; table, we will see something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tree /var/carbondata/data/store/default/orders

/var/carbondata/data/store/default/orders
â”œâ”€â”€ Fact
â”‚Â Â  â””â”€â”€ Part0
â”‚Â Â      â””â”€â”€ Segment_0
â”‚Â Â          â”œâ”€â”€ 0_1572694894322.carbonindexmerge
â”‚Â Â          â””â”€â”€ part-0-0_batchno0-0-0-1572694887333.carbondata
â”œâ”€â”€ LockFiles
â”‚Â Â  â”œâ”€â”€ Segment_0.lock
â”‚Â Â  â””â”€â”€ tablestatus.lock
â””â”€â”€ Metadata
    â”œâ”€â”€ schema
    â”œâ”€â”€ segments
    â”‚Â Â  â””â”€â”€ 0_1572694887333.segment
    â””â”€â”€ tablestatus
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's now load some more data running the same &lt;code&gt;LOAD&lt;/code&gt; command as before for three more times (there will be duplicate rows, but it does not matter), then check again the segments:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SHOW SEGMENTS FOR TABLE orders&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which look like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
|SegmentSequenceId| Status|     Load Start Time|       Load End Time|Merged To|File Format|Data Size|Index Size|
+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
|                3|Success|2019-11-02 12:50:...|2019-11-02 12:50:...|       NA|COLUMNAR_V3|  51,20MB|    1,59KB|
|                2|Success|2019-11-02 12:41:...|2019-11-02 12:41:...|       NA|COLUMNAR_V3|  51,20MB|    1,59KB|
|                1|Success|2019-11-02 12:41:...|2019-11-02 12:41:...|       NA|COLUMNAR_V3|  51,21MB|    1,59KB|
|                0|Success|2019-11-02 12:41:...|2019-11-02 12:41:...|       NA|COLUMNAR_V3|  51,20MB|    1,59KB|
+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and the directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tree /var/carbondata/data/store/default/orders

/var/carbondata/data/store/default/orders
â”œâ”€â”€ Fact
â”‚Â Â  â””â”€â”€ Part0
â”‚Â Â      â”œâ”€â”€ Segment_0
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ 0_1572694894322.carbonindexmerge
â”‚Â Â      â”‚Â Â  â””â”€â”€ part-0-0_batchno0-0-0-1572694887333.carbondata
â”‚Â Â      â”œâ”€â”€ Segment_1
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ 1_1572694906502.carbonindexmerge
â”‚Â Â      â”‚Â Â  â””â”€â”€ part-0-0_batchno0-0-1-1572694899093.carbondata
â”‚Â Â      â”œâ”€â”€ Segment_2
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ 2_1572694914891.carbonindexmerge
â”‚Â Â      â”‚Â Â  â””â”€â”€ part-0-0_batchno0-0-2-1572694907902.carbondata
â”‚Â Â      â””â”€â”€ Segment_3
â”‚Â Â          â”œâ”€â”€ 3_1572695453562.carbonindexmerge
â”‚Â Â          â””â”€â”€ part-0-0_batchno0-0-3-1572695446456.carbondata
â”œâ”€â”€ LockFiles
â”‚Â Â  â”œâ”€â”€ compaction.lock
â”‚Â Â  â”œâ”€â”€ Segment_0.lock
â”‚Â Â  â”œâ”€â”€ Segment_1.lock
â”‚Â Â  â”œâ”€â”€ Segment_2.lock
â”‚Â Â  â”œâ”€â”€ Segment_3.lock
â”‚Â Â  â”œâ”€â”€ tablestatus.lock
â”‚Â Â  â””â”€â”€ update.lock
â””â”€â”€ Metadata
    â”œâ”€â”€ schema
    â”œâ”€â”€ segments
    â”‚Â Â  â”œâ”€â”€ 0_1572694887333.segment
    â”‚Â Â  â”œâ”€â”€ 1_1572694899093.segment
    â”‚Â Â  â”œâ”€â”€ 2_1572694907902.segment
    â”‚Â Â  â””â”€â”€ 3_1572695446456.segment
    â””â”€â”€ tablestatus
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we can run a minor compaction:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ALTER TABLE orders COMPACT &amp;#39;MINOR&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and see what changed in the segments:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SHOW SEGMENTS FOR TABLE orders&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which look like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
|SegmentSequenceId|   Status|     Load Start Time|       Load End Time|Merged To|File Format|Data Size|Index Size|
+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
|                3|Compacted|2019-11-02 12:50:...|2019-11-02 12:50:...|      0.1|COLUMNAR_V3|  51,20MB|    1,59KB|
|                2|Compacted|2019-11-02 12:41:...|2019-11-02 12:41:...|      0.1|COLUMNAR_V3|  51,20MB|    1,59KB|
|                1|Compacted|2019-11-02 12:41:...|2019-11-02 12:41:...|      0.1|COLUMNAR_V3|  51,21MB|    1,59KB|
|              0.1|  Success|2019-11-02 12:51:...|2019-11-02 12:51:...|       NA|COLUMNAR_V3| 146,33MB|    2,81KB|
|                0|Compacted|2019-11-02 12:41:...|2019-11-02 12:41:...|      0.1|COLUMNAR_V3|  51,20MB|    1,59KB|
+-----------------+---------+--------------------+--------------------+---------+-----------+---------+----------+
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and in the directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tree /var/carbondata/data/store/default/orders

/var/carbondata/data/store/default/orders
â”œâ”€â”€ Fact
â”‚Â Â  â””â”€â”€ Part0
â”‚Â Â      â”œâ”€â”€ Segment_0
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ 0_1572694894322.carbonindexmerge
â”‚Â Â      â”‚Â Â  â””â”€â”€ part-0-0_batchno0-0-0-1572694887333.carbondata
â”‚Â Â      â”œâ”€â”€ Segment_0.1
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ &lt;span class="m"&gt;0&lt;/span&gt;.1_1572695514980.carbonindexmerge
â”‚Â Â      â”‚Â Â  â””â”€â”€ part-0-0_batchno0-0-0.1-1572695501043.carbondata
â”‚Â Â      â”œâ”€â”€ Segment_1
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ 1_1572694906502.carbonindexmerge
â”‚Â Â      â”‚Â Â  â””â”€â”€ part-0-0_batchno0-0-1-1572694899093.carbondata
â”‚Â Â      â”œâ”€â”€ Segment_2
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ 2_1572694914891.carbonindexmerge
â”‚Â Â      â”‚Â Â  â””â”€â”€ part-0-0_batchno0-0-2-1572694907902.carbondata
â”‚Â Â      â””â”€â”€ Segment_3
â”‚Â Â          â”œâ”€â”€ 3_1572695453562.carbonindexmerge
â”‚Â Â          â””â”€â”€ part-0-0_batchno0-0-3-1572695446456.carbondata
â”œâ”€â”€ LockFiles
â”‚Â Â  â”œâ”€â”€ compaction.lock
â”‚Â Â  â”œâ”€â”€ Segment_0.lock
â”‚Â Â  â”œâ”€â”€ Segment_1.lock
â”‚Â Â  â”œâ”€â”€ Segment_2.lock
â”‚Â Â  â”œâ”€â”€ Segment_3.lock
â”‚Â Â  â”œâ”€â”€ tablestatus.lock
â”‚Â Â  â””â”€â”€ update.lock
â””â”€â”€ Metadata
    â”œâ”€â”€ schema
    â”œâ”€â”€ segments
    â”‚Â Â  â”œâ”€â”€ &lt;span class="m"&gt;0&lt;/span&gt;.1_1572695501043.segment
    â”‚Â Â  â”œâ”€â”€ 0_1572694887333.segment
    â”‚Â Â  â”œâ”€â”€ 1_1572694899093.segment
    â”‚Â Â  â”œâ”€â”€ 2_1572694907902.segment
    â”‚Â Â  â””â”€â”€ 3_1572695446456.segment
    â””â”€â”€ tablestatus
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A new segment with ID &lt;code&gt;0.1&lt;/code&gt; has been created, while the other segments are marked as compacted. We had to create 4 segments because the default setting for the parameter &lt;code&gt;carbon.compaction.level.threshold&lt;/code&gt;, which controls the way minor compactions are performed, is &lt;code&gt;(4,3)&lt;/code&gt;; it means that 4 segments will be compacted in a "level 1" new segment, and 3 "level 1" segments (when present) will be compacted to a single "level 2" segment. More information on the other configuration parameters can be found on the &lt;a href="https://carbondata.apache.org/configuration-parameters.html"&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After compaction, the segments which have been compacted are still present; if we want to delete them and free up the space, we can use the following SQL command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CLEAN FILES FOR TABLE orders&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which will give the following segments:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
|SegmentSequenceId| Status|     Load Start Time|       Load End Time|Merged To|File Format|Data Size|Index Size|
+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
|              0.1|Success|2019-11-02 12:51:...|2019-11-02 12:51:...|       NA|COLUMNAR_V3| 146,33MB|    2,81KB|
+-----------------+-------+--------------------+--------------------+---------+-----------+---------+----------+
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and the following directory structure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tree /var/carbondata/data/store/default/orders

/var/carbondata/data/store/default/orders
â”œâ”€â”€ Fact
â”‚Â Â  â””â”€â”€ Part0
â”‚Â Â      â””â”€â”€ Segment_0.1
â”‚Â Â          â”œâ”€â”€ &lt;span class="m"&gt;0&lt;/span&gt;.1_1572695514980.carbonindexmerge
â”‚Â Â          â””â”€â”€ part-0-0_batchno0-0-0.1-1572695501043.carbondata
â”œâ”€â”€ LockFiles
â”‚Â Â  â”œâ”€â”€ clean_files.lock
â”‚Â Â  â”œâ”€â”€ compaction.lock
â”‚Â Â  â”œâ”€â”€ Segment_0.lock
â”‚Â Â  â”œâ”€â”€ Segment_1.lock
â”‚Â Â  â”œâ”€â”€ Segment_2.lock
â”‚Â Â  â”œâ”€â”€ Segment_3.lock
â”‚Â Â  â”œâ”€â”€ tablestatus.lock
â”‚Â Â  â””â”€â”€ update.lock
â””â”€â”€ Metadata
    â”œâ”€â”€ schema
    â”œâ”€â”€ segments
    â”‚Â Â  â””â”€â”€ &lt;span class="m"&gt;0&lt;/span&gt;.1_1572695501043.segment
    â”œâ”€â”€ tablestatus
    â””â”€â”€ tablestatus.history
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The major compaction works in a similar way and it is performed by running the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ALTER TABLE orders COMPACT &amp;#39;MAJOR&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;By default (parameter &lt;code&gt;carbon.major.compaction.size&lt;/code&gt;) the compaction will only take place on the segments whose sum of the sizes is below 1024 MB. You can try this out as well by loading data once more and running the SQL command.&lt;/p&gt;
&lt;h3&gt;Update and Delete operations&lt;/h3&gt;
&lt;p&gt;There is another type of compaction, called &lt;em&gt;horizontal compaction&lt;/em&gt;, that takes place on delta files created by Update and Delete operations. Whenever an Update is performed, two files are created:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;an &lt;em&gt;Insert Delta&lt;/em&gt; file which stores newly added rows in the CarbonData columnar format;&lt;/li&gt;
&lt;li&gt;a &lt;em&gt;Delete Delta&lt;/em&gt; file which only stores the IDs of the rows that are deleted in a Bitmap file format.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, if we start from this directory structure (clean table, one load):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;â”œâ”€â”€ Fact
â”‚Â Â  â””â”€â”€ Part0
â”‚Â Â      â””â”€â”€ Segment_0
â”‚Â Â          â”œâ”€â”€ 0_1572700087590.carbonindexmerge
â”‚Â Â          â””â”€â”€ part-0-0_batchno0-0-0-1572700080525.carbondata
â”œâ”€â”€ LockFiles
â”‚Â Â  â”œâ”€â”€ Segment_0.lock
â”‚Â Â  â””â”€â”€ tablestatus.lock
â””â”€â”€ Metadata
    â”œâ”€â”€ schema
    â”œâ”€â”€ segments
    â”‚Â Â  â””â”€â”€ 0_1572700080525.segment
    â””â”€â”€ tablestatus
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and run this update:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    UPDATE orders&lt;/span&gt;
&lt;span class="s"&gt;    SET (o_orderdate) = (&amp;#39;2016-06-06&amp;#39;)&lt;/span&gt;
&lt;span class="s"&gt;    WHERE o_orderdate = &amp;#39;1996-06-06&amp;#39;&lt;/span&gt;
&lt;span class="s"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;the directory will be updated like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;â”œâ”€â”€ Fact
â”‚Â Â  â””â”€â”€ Part0
â”‚Â Â      â””â”€â”€ Segment_0
â”‚Â Â          â”œâ”€â”€ 0_1572700087590.carbonindexmerge
â”‚Â Â          â”œâ”€â”€ 1_batchno0-0-0-1572700111168.carbonindex
â”‚Â Â          â”œâ”€â”€ part-0-0_batchno0-0-0-1572700080525.carbondata
â”‚Â Â          â”œâ”€â”€ part-0-0_batchno0-0-0-1572700111168.deletedelta
â”‚Â Â          â””â”€â”€ part-0-1_batchno0-0-0-1572700111168.carbondata
â”œâ”€â”€ LockFiles
â”‚Â Â  â”œâ”€â”€ compaction.lock
â”‚Â Â  â”œâ”€â”€ meta.lock
â”‚Â Â  â”œâ”€â”€ Segment_0.lock
â”‚Â Â  â”œâ”€â”€ Segment_.lock
â”‚Â Â  â”œâ”€â”€ tablestatus.lock
â”‚Â Â  â”œâ”€â”€ tableupdatestatus.lock
â”‚Â Â  â””â”€â”€ update.lock
â””â”€â”€ Metadata
    â”œâ”€â”€ schema
    â”œâ”€â”€ segments
    â”‚Â Â  â”œâ”€â”€ 0_1572700080525.segment
    â”‚Â Â  â””â”€â”€ 0_1572700111168.segment
    â”œâ”€â”€ tablestatus
â””â”€â”€ tableupdatestatus-1572700111168
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;basically creating:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;code&gt;.carbonindex&lt;/code&gt; file;&lt;/li&gt;
&lt;li&gt;a &lt;code&gt;.deletedelta&lt;/code&gt; file;&lt;/li&gt;
&lt;li&gt;a new &lt;code&gt;.carbondata&lt;/code&gt; file;&lt;/li&gt;
&lt;li&gt;a new &lt;code&gt;.segment&lt;/code&gt; file;&lt;/li&gt;
&lt;li&gt;a &lt;code&gt;tableupdatestatus-&lt;/code&gt; file.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A Delete operation, instead, creates the &lt;em&gt;Delete Delta&lt;/em&gt; file only. Let's run another update:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    UPDATE orders&lt;/span&gt;
&lt;span class="s"&gt;    SET (o_orderdate) = (&amp;#39;2016-06-05&amp;#39;)&lt;/span&gt;
&lt;span class="s"&gt;    WHERE o_orderdate = &amp;#39;1996-06-05&amp;#39;&lt;/span&gt;
&lt;span class="s"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we run the &lt;code&gt;tree&lt;/code&gt; command we'll see that there will be one or more files for each of the 4 types in the list. Now, if we run the &lt;code&gt;CLEAN&lt;/code&gt; command again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CLEAN FILES FOR TABLE orders&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;we will see that there is again only one file per type (except for the &lt;code&gt;.segment&lt;/code&gt; files). This behaviour is controlled by the following parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;carbon.horizontal.compaction.enable&lt;/code&gt;: enables this kind of compaction (default is &lt;code&gt;true&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;carbon.horizontal.update.compaction.threshold&lt;/code&gt;: number of &lt;em&gt;Update delta&lt;/em&gt; files above which the compaction will take place (default is 1);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;carbon.horizontal.delete.compaction.threshold&lt;/code&gt;: number of &lt;em&gt;Delete delta&lt;/em&gt; files above which the compaction will take place (default is 1).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Partitioning&lt;/h2&gt;
&lt;p&gt;Partitioning is a data organization strategy used to increase performance and isolation. CarbonData offers two kinds of partitions, a "standard" one (similar to &lt;a href="https://spark.apache.org/docs/2.3.4/sql-programming-guide.html#partition-discovery"&gt;Spark&lt;/a&gt; and Hive partitions) and a "CarbonData" one (that supports several partitioning schemes such as ranges, hashes, and lists, but no Update/Delete); since the latter is still experimental, we will only focus on the former.&lt;/p&gt;
&lt;p&gt;A partitioned table can be created with the usual Spark/Hive &lt;code&gt;PARTITIONED BY&lt;/code&gt; syntax:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    CREATE TABLE IF NOT EXISTS orders_part(&lt;/span&gt;
&lt;span class="s"&gt;        o_orderdate DATE,&lt;/span&gt;
&lt;span class="s"&gt;        o_orderpriority STRING,&lt;/span&gt;
&lt;span class="s"&gt;        o_orderkey INT,&lt;/span&gt;
&lt;span class="s"&gt;        o_custkey STRING,&lt;/span&gt;
&lt;span class="s"&gt;        o_totalprice DOUBLE,&lt;/span&gt;
&lt;span class="s"&gt;        o_clerk STRING,&lt;/span&gt;
&lt;span class="s"&gt;        o_shippriority INT,&lt;/span&gt;
&lt;span class="s"&gt;        o_comment STRING)&lt;/span&gt;
&lt;span class="s"&gt;    PARTITIONED BY(o_orderstatus STRING)&lt;/span&gt;
&lt;span class="s"&gt;    STORED AS carbondata&lt;/span&gt;
&lt;span class="s"&gt;    TBLPROPERTIES(&amp;#39;SORT_COLUMNS&amp;#39; = &amp;#39;o_orderdate&amp;#39;)&lt;/span&gt;
&lt;span class="s"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;paying attention to exclude the partitioning column from both the list of columns and from the &lt;code&gt;SORT_COLUMNS&lt;/code&gt; table property (if present).&lt;/p&gt;
&lt;p&gt;The data can be loaded in the usual way when using &lt;em&gt;dynamic partitioning&lt;/em&gt;, i.e. without specifying the partition to write to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;s&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s"&gt;    LOAD DATA INPATH &amp;#39;/tmp/tpch-dbgen/orders.tbl&amp;#39;&lt;/span&gt;
&lt;span class="s"&gt;    INTO TABLE orders_part&lt;/span&gt;
&lt;span class="s"&gt;    OPTIONS(&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;DELIMITER&amp;#39; = &amp;#39;|&amp;#39;,&lt;/span&gt;
&lt;span class="s"&gt;        &amp;#39;FILEHEADER&amp;#39; = &amp;#39;o_orderkey,o_custkey,o_orderstatus,o_totalprice,o_orderdate,o_orderpriority,o_clerk,o_shippriority,o_comment&amp;#39;)&lt;/span&gt;
&lt;span class="s"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and the partitioned column can be used to quickly filter data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT COUNT(*) FROM orders_part WHERE o_orderstatus = &amp;#39;F&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The partitions can be shown using the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SHOW PARTITIONS orders_part&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and we can see their layout using the &lt;code&gt;tree&lt;/code&gt; command on the new directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tree /var/carbondata/data/store/default/orders_part

/var/carbondata/data/store/default/orders_part
â”œâ”€â”€ LockFiles
â”‚Â Â  â”œâ”€â”€ Segment_0.lock
â”‚Â Â  â””â”€â”€ tablestatus.lock
â”œâ”€â”€ Metadata
â”‚Â Â  â”œâ”€â”€ schema
â”‚Â Â  â”œâ”€â”€ segments
â”‚Â Â  â”‚Â Â  â””â”€â”€ 0_1572701993390.segment
â”‚Â Â  â””â”€â”€ tablestatus
â”œâ”€â”€ &lt;span class="nv"&gt;o_orderstatus&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;F
â”‚Â Â  â”œâ”€â”€ 0_1572702007421.carbonindexmerge
â”‚Â Â  â”œâ”€â”€ part-0-100100000100001_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100001100001_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100002100001_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100003100001_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100004100001_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â””â”€â”€ part-0-100100005100001_batchno0-0-0-1572701993390.carbondata
â”œâ”€â”€ &lt;span class="nv"&gt;o_orderstatus&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;O
â”‚Â Â  â”œâ”€â”€ 0_1572702007422.carbonindexmerge
â”‚Â Â  â”œâ”€â”€ part-0-100100000100002_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100001100002_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100002100002_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100003100002_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100004100002_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â””â”€â”€ part-0-100100005100002_batchno0-0-0-1572701993390.carbondata
â”œâ”€â”€ &lt;span class="nv"&gt;o_orderstatus&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;P
â”‚Â Â  â”œâ”€â”€ 0_1572702007422.carbonindexmerge
â”‚Â Â  â”œâ”€â”€ part-0-100100000100003_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100001100003_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100002100003_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100003100003_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â”œâ”€â”€ part-0-100100004100003_batchno0-0-0-1572701993390.carbondata
â”‚Â Â  â””â”€â”€ part-0-100100005100003_batchno0-0-0-1572701993390.carbondata
â””â”€â”€ _SUCCESS
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where we can see that, instead of the usual &lt;code&gt;Fact/Part0/Segment_0&lt;/code&gt; structure, we have one directory per partition. With this structure, Update Delta and Delete Delta files are created within each folder. Whether partitions can make queries more efficient as opposed to MDKs and dictionaries will likely depend on the use case.&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;As we have been digging deeper into the internals of CarbonData, we have discovered that there are not only many interesting features to cover different use cases, but also many tools to increase the performance. I would suggest to take a look at the list of &lt;a href="https://carbondata.apache.org/configuration-parameters.html"&gt;configuration parameters&lt;/a&gt; as well as at the list of &lt;a href="https://carbondata.apache.org/usecases.html"&gt;use cases&lt;/a&gt;, since they can provide some guidance in the (daunting) task getting the better out of CarbonData. Have fun!&lt;/p&gt;</content><category term="Big Data"></category><category term="data storage format"></category></entry><entry><title>Apache CarbonData</title><link href="https://apothem.blog/apache-carbondata.html" rel="alternate"></link><published>2019-09-30T00:00:00+01:00</published><updated>2019-09-30T00:00:00+01:00</updated><author><name>Nicola Vitucci</name></author><id>tag:apothem.blog,2019-09-30:/apache-carbondata.html</id><summary type="html">&lt;p&gt;In the last few years I have been working quite extensively with Apache Spark, and I have come to realize that a good storage format goes a long way toward efficiency and speed. For instance, when dealing with large CSV or JSON files, adding an &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; writing step would â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the last few years I have been working quite extensively with Apache Spark, and I have come to realize that a good storage format goes a long way toward efficiency and speed. For instance, when dealing with large CSV or JSON files, adding an &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; writing step would improve performance in virtually every subsequent task, or at least in all tasks that would profit from a columnar storage. I have briefly dabbled with &lt;a href="https://orc.apache.org/"&gt;Apache ORC&lt;/a&gt; and, then, I found out a rather new format which I haven't really explored until last month, which is what we will see in this article: enter &lt;a href="https://carbondata.apache.org/"&gt;Apache CarbonData&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Developed by Huawei and launched as an Apache Incubator project in 2016, CarbonData is now is at version 1.6.0. The reasons why it caught my interest are several:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the promise to cover different use cases (full scan queries, small scan queries, multi-dimensional OLAP queries) at the same time;&lt;/li&gt;
&lt;li&gt;the tight integration with Spark and other processing engines;&lt;/li&gt;
&lt;li&gt;the increased encoding efficiency thanks to global and local dictionaries;&lt;/li&gt;
&lt;li&gt;the speed-up on filter queries thanks to multi-level indexing;&lt;/li&gt;
&lt;li&gt;the support for Update and Delete operations;&lt;/li&gt;
&lt;li&gt;the concept of Datamaps, additional structures (Time Series, Bloom filter, Lucene full-text, Materialized Views); to reduce execution time for some classes of analytics queries;&lt;/li&gt;
&lt;li&gt;the support of streaming use cases via near-real time insertion.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's quite a lot of features! So, let's see how to make use of each one of them. As usual, I will refer to a CentOS 7 system; there are not so many tools that need to be installed: besides the usual &lt;code&gt;wget&lt;/code&gt;, &lt;code&gt;git&lt;/code&gt;, and &lt;code&gt;Java 8&lt;/code&gt;, we will need &lt;code&gt;gcc&lt;/code&gt; in order to compile the data generation library.&lt;/p&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;In order to generate data to load in our CarbonData tables we will use the TPC-H &lt;em&gt;dbgen&lt;/em&gt; tool, which is commonly used for database benchmarks (and, in fact, is used to benchmark CarbonData as well). The tool creates synthetic data based on a real-world scenario, where vendors place orders to buy parts from suppliers and to sell parts to customers. The TPC-H benchmark includes a query generation tool as well, which we won't use here.&lt;/p&gt;
&lt;p&gt;To build our dataset, let's clone the repo into &lt;code&gt;/tmp&lt;/code&gt; (or any other folder) and build the tools:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; /tmp
$ git clone https://github.com/electrum/tpch-dbgen
$ &lt;span class="nb"&gt;cd&lt;/span&gt; tpch-dbgen
$ make
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and, once finished, let's run the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./dbgen -v
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which will create 8 files for a total size of about 1 GB. If you want to create larger files you can use the &lt;code&gt;-s&lt;/code&gt; parameter (the scale factor) as in this example, which will generate about 10 GB of data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./dbgen -s &lt;span class="m"&gt;10&lt;/span&gt; -v
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We will mostly use only one file, &lt;code&gt;lineitem.tbl&lt;/code&gt;, which is the largest one; anyway, the other files can provide some more context - and can be used for more experiments.&lt;/p&gt;
&lt;h2&gt;Spark and CarbonData&lt;/h2&gt;
&lt;p&gt;We will explore CarbonData features using the simplest setup: a Spark shell in standalone mode with CarbonData already packaged as a JAR file. We therefore need to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;download Spark from &lt;a href="https://www.apache.org/dyn/closer.lua/spark/spark-2.3.4/spark-2.3.4-bin-hadoop2.7.tgz"&gt;this page&lt;/a&gt; (we will use Spark 2.3.4 since the latest version of CarbonData is compatible with Spark up to 2.3.2);&lt;/li&gt;
&lt;li&gt;extract the content of the &lt;code&gt;.tgz&lt;/code&gt; file into a folder (e.g. &lt;code&gt;/opt/spark&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;download the latest version of the CarbonData JAR file built for Spark 2.3.2 from &lt;a href="https://dist.apache.org/repos/dist/release/carbondata/1.6.0/apache-carbondata-1.6.0-bin-spark2.3.2-hadoop2.7.2.jar"&gt;here&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;copy the JAR file to the Spark folder.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can then launch the Spark shell from the Spark folder as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ spark-shell --jars apache-carbondata-1.6.0-bin-spark2.3.2-hadoop2.7.2.jar
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and, once the shell is running, we need to create a &lt;code&gt;CarbonSession&lt;/code&gt; (which is similar to a &lt;code&gt;SparkSession&lt;/code&gt;). Since we will run everything on the local filesystem, we can create a folder such as &lt;code&gt;/var/carbondata&lt;/code&gt; (and assign it a suitable owner and permissions) where we will store both the data and the metastore; supposing that the data will be saved in &lt;code&gt;/var/carbondata/data/store&lt;/code&gt; and the metastore in &lt;code&gt;/var/carbondata/metastore&lt;/code&gt;, the &lt;code&gt;CarbonSession&lt;/code&gt; can be created as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.spark.sql.SparkSession&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.spark.sql.CarbonSession._&lt;/span&gt;

&lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;carbon&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="nc"&gt;SparkSession&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;builder&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConf&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getOrCreateCarbonSession&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/var/carbondata/data/store&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;/var/carbondata/metastore&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(Pro tip: a whole piece of code can be pasted and run into the Scala shell by running the command &lt;code&gt;:paste&lt;/code&gt;, pasting the code, and pressing Ctrl + D)&lt;/p&gt;
&lt;p&gt;Now we are ready!&lt;/p&gt;
&lt;h2&gt;Creating tables and loading data&lt;/h2&gt;
&lt;p&gt;First of all, we need to create a table and load some data into it. Every SQL command can be run via the &lt;code&gt;carbon.sql()&lt;/code&gt; method, so we can create a &lt;code&gt;lineitem&lt;/code&gt; table in this way:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TABLE IF NOT EXISTS lineitem(l_shipdate DATE, l_shipmode STRING, l_shipinstruct STRING, l_returnflag STRING, l_receiptdate DATE, l_orderkey INT, l_partkey INT, l_suppkey STRING, l_linenumber INT, l_quantity DOUBLE, l_extendedprice DOUBLE, l_discount DOUBLE, l_tax DOUBLE, l_linestatus STRING, l_commitdate DATE, l_comment STRING) STORED AS carbondata&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and then load data like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;LOAD DATA INPATH &amp;#39;/tmp/tpch-dbgen/lineitem.tbl&amp;#39; INTO TABLE lineitem OPTIONS(&amp;#39;DELIMITER&amp;#39; = &amp;#39;|&amp;#39;, &amp;#39;FILEHEADER&amp;#39; = &amp;#39;l_orderkey,l_partkey,l_suppkey,l_linenumber,l_quantity,l_extendedprice,l_discount,l_tax,l_returnflag,l_linestatus,l_shipdate,l_commitdate,l_receiptdate,l_shipinstruct,l_shipmode,l_comment&amp;#39;)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can run SQL queries like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT COUNT(*) FROM lineitem WHERE l_shipdate = &amp;#39;1996-06-06&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and also time every query using &lt;code&gt;spark.time()&lt;/code&gt;, for instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;spark.time(carbon.sql(&amp;quot;SELECT COUNT(*) FROM lineitem WHERE l_shipdate = &amp;#39;1996-06-06&amp;#39;&amp;quot;).show)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Multi-dimensional keys (MDKs)&lt;/h2&gt;
&lt;p&gt;Let's now create and populate a new table using multi-dimensional keys (MDKs) by means of the &lt;code&gt;SORT_COLUMNS&lt;/code&gt; table property:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TABLE IF NOT EXISTS lineitem_sorted(l_shipdate DATE, l_shipmode STRING, l_shipinstruct STRING, l_returnflag STRING, l_receiptdate DATE, l_orderkey INT, l_partkey INT, l_suppkey STRING, l_linenumber INT, l_quantity DOUBLE, l_extendedprice DOUBLE, l_discount DOUBLE, l_tax DOUBLE, l_linestatus STRING, l_commitdate DATE, l_comment STRING) STORED AS carbondata TBLPROPERTIES (&amp;#39;SORT_COLUMNS&amp;#39; = &amp;#39;l_shipdate,l_shipmode,l_shipinstruct,l_receiptdate,l_commitdate,l_returnflag,l_linestatus&amp;#39;)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;LOAD DATA INPATH &amp;#39;/tpch-dbgen/small/lineitem.tbl&amp;#39; INTO TABLE lineitem_dic OPTIONS(&amp;#39;DELIMITER&amp;#39; = &amp;#39;|&amp;#39;, &amp;#39;FILEHEADER&amp;#39; = &amp;#39;l_orderkey,l_partkey,l_suppkey,l_linenumber,l_quantity,l_extendedprice,l_discount,l_tax,l_returnflag,l_linestatus,l_shipdate,l_commitdate,l_receiptdate,l_shipinstruct,l_shipmode,l_comment&amp;#39;)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's run the following query:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT COUNT(*) FROM lineitem WHERE l_shipdate = &amp;#39;1996-06-06&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;On my machine, a VPS with 8 vCPUs and 32 GB of RAM, it took about 4.2 seconds. Now let's run the same query on the &lt;code&gt;lineitem_sorted&lt;/code&gt; table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT COUNT(*) FROM lineitem_sorted WHERE l_shipdate = &amp;#39;1996-06-06&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It took 90 &lt;em&gt;milliseconds&lt;/em&gt;! So, indeed, MDKs are improving significantly the queries that make use of filters. Let's try with another query on both tables:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT l_shipdate, COUNT(*) c FROM lineitem WHERE l_commitdate &amp;lt; &amp;#39;1996-01-01&amp;#39; GROUP BY l_shipdate ORDER BY c DESC&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT l_shipdate, COUNT(*) c FROM lineitem_sorted WHERE l_commitdate &amp;lt; &amp;#39;1996-01-01&amp;#39; GROUP BY l_shipdate ORDER BY c DESC&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this case the first one takes about 5 seconds, while the second one takes about 3 seconds.&lt;/p&gt;
&lt;p&gt;Let's now take a look at the directories and files that CarbonData created under &lt;code&gt;/var/carbondata/data/store&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ls -lah /var/carbondata/data/store
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We'll see that there are two folders, &lt;code&gt;default&lt;/code&gt; and &lt;code&gt;_system&lt;/code&gt;, where the first one is used to store the default database; since we didn't create a new database, our tables will be a &lt;code&gt;lineitem&lt;/code&gt; and a &lt;code&gt;lineitem_sorted&lt;/code&gt; directories within &lt;code&gt;default&lt;/code&gt;. If we have &lt;code&gt;tree&lt;/code&gt; installed (and, if not, we can simply install it with &lt;code&gt;sudo yum install tree&lt;/code&gt;), we can see for instance the structure of &lt;code&gt;lineitem&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tree /var/carbondata/data/store/default/lineitem

:::text
/var/carbondata/data/store/default/lineitem
â”œâ”€â”€ Fact
â”‚   â””â”€â”€ Part0
â”‚       â””â”€â”€ Segment_0
â”‚           â”œâ”€â”€ 0_1569876396402.carbonindexmerge
â”‚           â””â”€â”€ part-0-0_batchno0-0-0-1569876355317.carbondata
â”œâ”€â”€ LockFiles
â”‚   â”œâ”€â”€ Segment_0.lock
â”‚   â””â”€â”€ tablestatus.lock
â””â”€â”€ Metadata
    â”œâ”€â”€ schema
    â”œâ”€â”€ segments
    â”‚   â””â”€â”€ 0_1569876355317.segment
    â””â”€â”€ tablestatus
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The file names will most likely be different, but we are interested in the directory structure; for more details on CarbonData file format, take a look at the &lt;a href="https://carbondata.apache.org/file-structure-of-carbondata.html"&gt;documentation&lt;/a&gt;. If we look at the size of the &lt;code&gt;Fact&lt;/code&gt; directory, we will see that it is approximately 222 MB:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ du -sh /var/carbondata/data/store/default/lineitem/Fact
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we look at the size of the same directory for the &lt;code&gt;lineitem_sorted&lt;/code&gt; directory, instead, we will see that it's 190 MB! So, MDKs are convenient from the storage point of view as well.&lt;/p&gt;
&lt;h2&gt;Update and Delete operations&lt;/h2&gt;
&lt;p&gt;Update operations are very simple in CarbonData. The format is the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;UPDATE&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;table_name&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; 
&lt;span class="k"&gt;SET&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column_name_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;column_name_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;column_name_n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column_1_expression&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;column_2_expression&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="n"&gt;column_n_expression&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;filter_condition&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;for simple updates where the column expressions are calculated from the same table, or the following for more generic updates:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;UPDATE&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="k"&gt;table_name&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="k"&gt;SET&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column_name_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;column_name_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;sourceColumn_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sourceColumn_2&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;sourceTable&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;filter_condition&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;filter_condition&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's see an example. First of all, let's check the total number of records and the number of records for a specific &lt;code&gt;l_shipdate&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT COUNT(*) FROM lineitem_sorted&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;

&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT COUNT(*) FROM lineitem_sorted WHERE l_shipdate = &amp;#39;1996-06-06&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We get 6,001,215 and 2,454 results respectively. Let's update the &lt;code&gt;l_shipdate&lt;/code&gt; for those same records:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;UPDATE lineitem_sorted SET (l_shipdate) = (&amp;#39;2016-06-06&amp;#39;) WHERE l_shipdate = &amp;#39;1996-06-06&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, running the second &lt;code&gt;COUNT&lt;/code&gt; query will not return any results, while we'll get 2,454 results by running this query instead:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT COUNT(*) FROM lineitem_sorted WHERE l_shipdate = &amp;#39;2016-06-06&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Delete operation is even simpler. Let's run the following query:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;DELETE FROM lineitem_sorted WHERE l_shipdate = &amp;#39;2016-06-06&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now the last &lt;code&gt;COUNT&lt;/code&gt; query will return zero results.&lt;/p&gt;
&lt;h2&gt;Datamaps&lt;/h2&gt;
&lt;p&gt;The concept of datamap is quite interesting: basically, a new data structure is added on top of the existing data in order to improve the performance of some specific queries. Let's look for instance at the &lt;em&gt;Lucene&lt;/em&gt; datamap, which adds a Lucene-based full-text index to a give column. &lt;/p&gt;
&lt;h3&gt;Lucene datamap&lt;/h3&gt;
&lt;p&gt;In order to create a Lucene full-text datamap, we run the following query:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE DATAMAP comment ON TABLE lineitem_sorted USING &amp;#39;lucene&amp;#39; DMPROPERTIES(&amp;#39;INDEX_COLUMNS&amp;#39; = &amp;#39;l_comment&amp;#39;, &amp;#39;SPLIT_BLOCKLET&amp;#39; = &amp;#39;false&amp;#39;)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We have just created a Lucene index on the &lt;code&gt;l_comment&lt;/code&gt; column which stores the BlockletIds as well (it is not very clear to me why but, if this property is not present, the full-text queries throw exceptions). When the index is ready we can run queries such as the following, where we ask for comments containing words that start with &lt;code&gt;quick&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT l_comment FROM lineitem_sorted WHERE TEXT_MATCH_WITH_LIMIT(&amp;#39;l_comment:quick*&amp;#39;, 10)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which will return a result like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+------------------------------------------+
|l_comment                                 |
+------------------------------------------+
|fix. quickly ironic instruct              |
| packages detect furiously quick          |
|ely ironic deposits sleep quickly un      |
|ffily regular ideas haggle quick          |
|y ironic instructions among the quick     |
|ts wake quickly after the u               |
|e quickly along the express ideas-- slyly |
|ctions. quickly even                      |
|about the quickly express pl              |
|s nag quick                               |
|ly regular deposits. even deposits kindle |
|ly. furiously                             |
|ajole slyly after the blithely re         |
|aggle blithely slyly even inst            |
|ithe pinto beans. special, iron           |
|silent foxes. slyly                       |
|sts sleep af                              |
|. daring pinto beans wake                 |
|slyly after the furio                     |
| ironic requests. final, ironic depo      |
+------------------------------------------+
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or like the following, where we ask for comments containing words that start with &lt;code&gt;quick&lt;/code&gt; but no words that start with &lt;code&gt;ironic&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT l_comment FROM lineitem_sorted WHERE TEXT_MATCH_WITH_LIMIT(&amp;#39;l_comment:quick* -l_comment:ironic*&amp;#39;, 10)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which will result in something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+------------------------------------------+
|l_comment                                 |
+------------------------------------------+
| packages detect furiously quick          |
|ffily regular ideas haggle quick          |
|ts wake quickly after the u               |
|e quickly along the express ideas-- slyly |
|ctions. quickly even                      |
|about the quickly express pl              |
|s nag quick                               |
|gle slowly. quickly regular theodo        |
|t quickly blithely                        |
|unts affix quickly! regu                  |
|ly. furiously                             |
|aggle blithely slyly even inst            |
|silent foxes. slyly                       |
|sts sleep af                              |
|. daring pinto beans wake                 |
|slyly after the furio                     |
| ironic requests. final, ironic depo      |
| orbits. blithely unusual ideas above th  |
|ost after the furiously express           |
|kly final accounts wake b                 |
+------------------------------------------+
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Another thing which is not clearly specified is why more than 10 results are returned even though, starting with the 11th, they seem irrelevant; until this is clarified, I would recommend to add a &lt;code&gt;LIMIT 10&lt;/code&gt; to the query.&lt;/p&gt;
&lt;h3&gt;Time Series datamap&lt;/h3&gt;
&lt;p&gt;Another useful datamap is the Time Series datamap, which creates a separate table to optimize queries on time series. We can run queries like the following without a datamap:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT TIMESERIES(l_shipdate, &amp;#39;year&amp;#39;) t, AVG(l_quantity) FROM lineitem_sorted GROUP BY t ORDER BY t DESC&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where we calculate the average &lt;code&gt;l_quantity&lt;/code&gt; for every year, but the performance would not be great (it takes about 4.2 seconds on my machine). We can instead create a Time Series datamap, but we first need to convert the &lt;code&gt;l_shipdate&lt;/code&gt; to a &lt;code&gt;timestamp&lt;/code&gt; field, and since we already created a datamap on our table we cannot run any &lt;code&gt;ALTER TABLE&lt;/code&gt; statements; we can anyway create a new table with a &lt;em&gt;CTAS&lt;/em&gt; (&lt;em&gt;create table as&lt;/em&gt;) statement adding a &lt;code&gt;l_shipdate_t&lt;/code&gt; field of type &lt;code&gt;timestamp&lt;/code&gt; from the existing &lt;code&gt;l_shipdate&lt;/code&gt; field:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE TABLE lineitem_t STORED AS carbondata AS SELECT *, timestamp(l_shipdate) l_shipdate_t FROM lineitem&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and then create a datamap on top of the newly-created table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE DATAMAP agg_qty_year ON TABLE lineitem_t USING &amp;#39;timeseries&amp;#39; DMPROPERTIES(&amp;#39;EVENT_TIME&amp;#39; = &amp;#39;l_shipdate_t&amp;#39;, &amp;#39;YEAR_GRANULARITY&amp;#39; = &amp;#39;1&amp;#39;) AS SELECT l_shipdate_t, AVG(l_quantity) qty FROM lineitem_t GROUP BY l_shipdate_t&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, the previous query (rewritten just to use the new table and the added field) will run much faster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT TIMESERIES(l_shipdate_t, &amp;#39;year&amp;#39;) t, AVG(l_quantity) FROM lineitem_t GROUP BY t ORDER BY t DESC&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(it takes less than 1 second on my machine).&lt;/p&gt;
&lt;p&gt;The query and the datamap we have created work on a year granularity, but we can create as many datamaps as we need to support finer granularities (down to the minute).&lt;/p&gt;
&lt;h3&gt;Multivalue datamap&lt;/h3&gt;
&lt;p&gt;The last datamap we will see (as we are not going to cover the Bloom datamap) is the Multivalued (MV) datamap, which generalizes the deprecated Pre-Aggregate datamap. This type of datamap is very useful for aggregation queries such as the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SELECT l_partkey, AVG(l_quantity) avg_q, AVG(l_extendedprice) avg_p FROM lineitem_sorted GROUP BY l_partkey ORDER BY avg_q DESC LIMIT 10&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where we calculate several aggregated quantities after grouping by the &lt;code&gt;l_partkey&lt;/code&gt; field, whose cardinality is relatively high. This query takes about 2.8 seconds on my machine.&lt;/p&gt;
&lt;p&gt;Let's create an MV datamap:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;carbon&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;CREATE DATAMAP avg_qty_price ON TABLE lineitem_sorted USING &amp;#39;MV&amp;#39; AS SELECT l_partkey, AVG(l_quantity) avg_q, AVG(l_extendedprice) avg_p FROM lineitem_sorted GROUP BY l_partkey&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now the previous query takes less than half a second. It is worth noting that, now that the datamap is in place, every time that new data are added (for instance via the &lt;code&gt;INSERT&lt;/code&gt; statement) the datamap will reflect the changes.&lt;/p&gt;
&lt;p&gt;We can also make sure that the query actually makes use of the datamap by using the &lt;code&gt;explain&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;carbon.sql&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;SELECT l_partkey, AVG(l_quantity) avg_q, AVG(l_extendedprice) avg_p FROM lineitem_sorted GROUP BY l_partkey ORDER BY avg_q DESC LIMIT 10&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;.explain
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The resulting plan is the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;== Physical Plan ==
TakeOrderedAndProject(limit=10, orderBy=[avg_q#2573 DESC NULLS LAST], output=[l_partkey#2689,avg_q#2573,avg_p#2574])
+- *(1) Project [lineitem_sorted_l_partkey#2572 AS l_partkey#2689, avg_q#2573, avg_p#2574]
   +- *(1) FileScan carbondata default.avg_qty_price_table[lineitem_sorted_l_partkey#2572,avg_q#2573,avg_p#2574] ReadSchema: struct&amp;lt;avg_p:double,avg_q:double,lineitem_sorted_l_partkey:int&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where we can see a &lt;code&gt;FileScan&lt;/code&gt; on the &lt;code&gt;avg_qty_price_table&lt;/code&gt; table (the MV datamap is backed by a table).&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;We have seen some of the best features that CarbonData offers, although we haven't looked in too much detail at the performance and at the structure of the data, and we haven't explored the streaming capabilities and other integrations. The documentation is quite good even though some changes are not well described (for instance, after the &lt;code&gt;SORT_COLUMNS&lt;/code&gt; option has been introduced, do we still need the &lt;code&gt;DICTIONARY_INCLUDE&lt;/code&gt; option? what is the difference?), but the project is still alive and has a fairly large community to support it. I definitely want to find out more and to follow it closely.&lt;/p&gt;</content><category term="Big Data"></category><category term="data storage format"></category></entry><entry><title>Apache Rya</title><link href="https://apothem.blog/apache-rya.html" rel="alternate"></link><published>2019-08-31T00:00:00+01:00</published><updated>2019-08-31T00:00:00+01:00</updated><author><name>Nicola Vitucci</name></author><id>tag:apothem.blog,2019-08-31:/apache-rya.html</id><summary type="html">&lt;p&gt;Since I have been working with Semantic Web technologies for quite some time, I was looking forward to explore new Apache projects within the area. &lt;a href="https://rya.incubator.apache.org/"&gt;Apache Rya&lt;/a&gt; fit the purpose perfectly, as it is a SPARQL-enabled triplestore for Big Data, promising to scale to billions of triples across multiple nodes â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Since I have been working with Semantic Web technologies for quite some time, I was looking forward to explore new Apache projects within the area. &lt;a href="https://rya.incubator.apache.org/"&gt;Apache Rya&lt;/a&gt; fit the purpose perfectly, as it is a SPARQL-enabled triplestore for Big Data, promising to scale to billions of triples across multiple nodes. If you are completely lost after this terminology, you can take a look at the &lt;a href="https://en.wikipedia.org/wiki/Semantic_Web"&gt;Wikipedia article&lt;/a&gt; and the &lt;a href="https://www.w3.org/2009/Talks/0615-SanJose-tutorial-IH/Slides.pdf"&gt;W3C presentation&lt;/a&gt; to get an idea of the subject.&lt;/p&gt;
&lt;p&gt;Rya runs on top of another Apache project, &lt;a href="https://accumulo.apache.org/"&gt;Accumulo&lt;/a&gt; (a distributed key/value store), and is deployed as a Web service. Since Accumulo by itself requires a fairly complex installation procedure as it is based on &lt;a href="https://hadoop.apache.org/"&gt;Apache Hadoop&lt;/a&gt; and &lt;a href="https://zookeeper.apache.org/"&gt;Apache Zookeeper&lt;/a&gt;, we will use yet another great Apache project, &lt;a href="http://fluo.apache.org/"&gt;Apache Fluo&lt;/a&gt; (or, more specifically, its subproject &lt;a href="https://github.com/apache/fluo-uno"&gt;Fluo Uno&lt;/a&gt;), to simplify the whole procedure; we will then use a well-known server, &lt;a href="http://tomcat.apache.org/"&gt;Apache Tomcat&lt;/a&gt;, to run it.&lt;/p&gt;
&lt;p&gt;In this article we will see how to get a single-machine Rya instance up and running and how to perform basic operations; scalability, management, and more complex operations will be covered in another article. As usual, the reference operating system will be CentOS 7 (if possible, an empty/fresh instance).&lt;/p&gt;
&lt;h2&gt;Preparing the environment&lt;/h2&gt;
&lt;h3&gt;Installing Tomcat&lt;/h3&gt;
&lt;p&gt;First of all we need to have Tomcat installed and running. On CentOS 7 it is just a matter of running the following two commands (both requiring &lt;code&gt;sudo&lt;/code&gt; rights):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo yum install tomcat
$ sudo service tomcat start
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Prerequisites for Fluo Uno&lt;/h3&gt;
&lt;p&gt;In the following, we will download and use Fluo Uno to setup a single-machine Accumulo instance; its sibling project, &lt;a href="https://github.com/apache/fluo-muchos"&gt;Fluo Muchos&lt;/a&gt;, can be used instead to create a real cluster in Amazon's EC2.&lt;/p&gt;
&lt;p&gt;In order to use Fluo Uno, we need to have Java 8, &lt;em&gt;wget&lt;/em&gt;, &lt;em&gt;maven&lt;/em&gt;, and &lt;em&gt;git&lt;/em&gt; already installed; if any of these components is missing, it can be installed with &lt;code&gt;sudo yum install&lt;/code&gt;. We also need to install the &lt;em&gt;shasum&lt;/em&gt; program (&lt;strong&gt;not&lt;/strong&gt; the &lt;em&gt;sha1sum&lt;/em&gt; or the similar ones that are likely already installed), which is contained in the &lt;em&gt;perl-Digest-SHA&lt;/em&gt; package, and we might install &lt;em&gt;curl&lt;/em&gt; to test Rya's API. A quick way to install all the needed packages is to run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo yum install java wget maven git perl-Digest-SHA curl
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once all the components are present, we need to export the Java path. If Java has been installed under, say, &lt;code&gt;/usr/lib/jvm/java-1.8.0&lt;/code&gt;, we need to run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;JAVA_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/lib/jvm/java-1.8.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, in order for Hadoop to work correctly, we need to make sure we can log into localhost with SSH:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ssh localhost
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If this does not work, we need to create a local pair of SSH keys. First of all, let's make sure that we do not really have any SSH keys:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ls ~/.ssh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If no results are returned, we can run this command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ssh-keygen
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which will create a default RSA key. If, instead, some keys are already present, try to debug the error and, before running the keygen, back them up first. Once the keys are created, they need to be added to the list of authorized keys:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, &lt;code&gt;ssh localhost&lt;/code&gt; should successfully open a new shell.&lt;/p&gt;
&lt;h3&gt;Downloading and using Fluo Uno&lt;/h3&gt;
&lt;p&gt;We are now ready to clone Fluo Uno from Github:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/apache/fluo-uno.git
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and enter the directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; fluo-uno
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since at the time of writing Rya supports Accumulo only up to version 1.7, we need to add the next two lines at the beginning of &lt;code&gt;./conf/uno.conf&lt;/code&gt; to ensure compatibility:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;HADOOP_VERSION=2.9.0
ACCUMULO_VERSION=1.7.3
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After saving, we can run the following commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./bin/uno fetch accumulo
$ ./bin/uno setup accumulo
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If everything ran successfully, we should see an output like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Apache Hadoop 3.2.0 is running
    * NameNode status: http://localhost:9870/
    * ResourceManager status: http://localhost:8088/
    * view logs at /path/to/fluo-uno/install/logs/hadoop
Apache ZooKeeper 3.4.14 is running
    * view logs at /path/to/fluo-uno/install/logs/zookeeper
Apache Accumulo 2.0.0 is running
    * Accumulo Monitor: http://localhost:9995/
    * view logs at /path/to/fluo-uno/install/logs/accumulo
Setup complete.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If the output is different or some errors are displayed, try checking the &lt;code&gt;./install/logs&lt;/code&gt; folder to debug.&lt;/p&gt;
&lt;h2&gt;Installing Rya&lt;/h2&gt;
&lt;p&gt;Once the environment is ready as detailed in the previous sections, we can clone Rya:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone https://github.com/apache/incubator-rya
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and build it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; incubator-rya
$ mvn clean install
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(the last command will take a while to complete because all the tests are run, which is highly recommended).&lt;/p&gt;
&lt;p&gt;When Maven has finished building, we run the following command to deploy Rya on Tomcat:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo cp ./web/web.rya/target/web.rya.war /var/lib/tomcat/webapps/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Before we can use Rya, we need to create an &lt;code&gt;environment.properties&lt;/code&gt; file with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Accumulo instance name
instance.name=uno
# Accumulo Zookeepers
instance.zk=localhost:2181
# Accumulo username
instance.username=root
# Accumulo password
instance.password=secret

# Rya Table Prefix
rya.tableprefix=triplestore_
# To display the query plan
rya.displayqueryplan=true
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;then assign it to the Tomcat user (usually &lt;code&gt;tomcat&lt;/code&gt;), move it into the Web application classpath and restart Tomcat:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo chown tomcat:tomcat environment.properties
$ sudo mv environment.properties /var/lib/tomcat/webapps/web.rya/WEB-INF/classes
$ sudo service tomcat restart
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to test that Rya is deployed and configured successfully, we can run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl -I &lt;span class="s2"&gt;&amp;quot;localhost:8080/web.rya/queryrdf&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which should give 200 as the status code. Yay, we have a running Rya instance!&lt;/p&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;Let's open the SPARQL endpoint on &lt;code&gt;http://localhost:8080/web.rya/sparqlQuery.jsp&lt;/code&gt;. Rya's SPARQL endpoint supports SPARQL Update, so we can insert some example triples using the following query (inspired by Rya's documentation):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;INSERT DATA&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;GRAPH&lt;/span&gt; &lt;span class="nl"&gt;&amp;lt;http://example.com/mygraph&amp;gt;&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nl"&gt;&amp;lt;http://mynamespace/ProductType1&amp;gt;&lt;/span&gt; &lt;span class="nl"&gt;&amp;lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&amp;gt;&lt;/span&gt; &lt;span class="nl"&gt;&amp;lt;http://mynamespace/ProductType&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
        &lt;span class="nl"&gt;&amp;lt;http://mynamespace/ProductType1&amp;gt;&lt;/span&gt; &lt;span class="nl"&gt;&amp;lt;http://www.w3.org/2000/01/rdf-schema#label&amp;gt;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Thing&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
        &lt;span class="nl"&gt;&amp;lt;http://mynamespace/ProductType1&amp;gt;&lt;/span&gt; &lt;span class="nl"&gt;&amp;lt;http://purl.org/dc/elements/1.1/publisher&amp;gt;&lt;/span&gt; &lt;span class="nl"&gt;&amp;lt;http://mynamespace/Publisher1&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;then retrieve them with another SPARQL query and selecting &lt;em&gt;JSON&lt;/em&gt; as the result format:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;GRAPH&lt;/span&gt; &lt;span class="nv"&gt;?g&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nv"&gt;?s&lt;/span&gt; &lt;span class="nv"&gt;?p&lt;/span&gt; &lt;span class="nv"&gt;?o&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which should give a result similar to the following (the order of the fields might be different):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;quot;head&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;vars&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
         &lt;span class="s2"&gt;&amp;quot;g&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
         &lt;span class="s2"&gt;&amp;quot;s&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
         &lt;span class="s2"&gt;&amp;quot;p&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
         &lt;span class="s2"&gt;&amp;quot;o&amp;quot;&lt;/span&gt;
      &lt;span class="p"&gt;]&lt;/span&gt;
   &lt;span class="p"&gt;},&lt;/span&gt;
   &lt;span class="nt"&gt;&amp;quot;results&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;bindings&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
         &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;s&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://mynamespace/ProductType1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;o&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://mynamespace/Publisher1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;g&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://example.com/mygraph&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;p&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://purl.org/dc/elements/1.1/publisher&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
         &lt;span class="p"&gt;},&lt;/span&gt;
         &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;o&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://mynamespace/ProductType&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;s&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://mynamespace/ProductType1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;p&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;g&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://example.com/mygraph&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
         &lt;span class="p"&gt;},&lt;/span&gt;
         &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;g&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://example.com/mygraph&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;p&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://www.w3.org/2000/01/rdf-schema#label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;o&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Thing&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;literal&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;s&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://mynamespace/ProductType1&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
         &lt;span class="p"&gt;},&lt;/span&gt;
         &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;o&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;literal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;3.0.0&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;s&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;urn:org.apache.rya/2012/05#rts&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;p&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;urn:org.apache.rya/2012/05#version&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
         &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;]&lt;/span&gt;
   &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can get the same result using the &lt;code&gt;queryrdf&lt;/code&gt; REST endpoint:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl &lt;span class="s2"&gt;&amp;quot;localhost:8080/web.rya/queryrdf&amp;quot;&lt;/span&gt; -d query.resultformat&lt;span class="o"&gt;=&lt;/span&gt;json --data-urlencode &lt;span class="nv"&gt;query&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;SELECT * WHERE {GRAPH ?g {?s ?p ?o}}&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Like other Apache projects still in incubation, Rya's documentation and examples may be still somewhat incomplete or difficult to follow. My aim was to try and streamline the installation process, and to provide a couple examples which allow a user to quickly see that everything is working correctly. A more complex setup and more interesting examples to come soon!&lt;/p&gt;</content><category term="Big Data"></category><category term="Semantic Web"></category></entry><entry><title>Apache Atlas (part 2)</title><link href="https://apothem.blog/apache-atlas-part-2.html" rel="alternate"></link><published>2019-07-31T00:00:00+01:00</published><updated>2019-07-31T00:00:00+01:00</updated><author><name>Nicola Vitucci</name></author><id>tag:apothem.blog,2019-07-31:/apache-atlas-part-2.html</id><summary type="html">&lt;p&gt;Since Atlas is a fairly large and complex project, one article was definitely not enough to explore all of its capabilities. Building on the &lt;a href="https://apothem.blog/apache-atlas.html"&gt;previous article&lt;/a&gt;, we will explore classifications and glossary, the REST API, and two more sources of lineage information (Spark and Kafka).&lt;/p&gt;
&lt;h2&gt;Classification&lt;/h2&gt;
&lt;p&gt;Let's start with classification â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Since Atlas is a fairly large and complex project, one article was definitely not enough to explore all of its capabilities. Building on the &lt;a href="https://apothem.blog/apache-atlas.html"&gt;previous article&lt;/a&gt;, we will explore classifications and glossary, the REST API, and two more sources of lineage information (Spark and Kafka).&lt;/p&gt;
&lt;h2&gt;Classification&lt;/h2&gt;
&lt;p&gt;Let's start with classification. In our previous example we created some Hive tables containing information about people and their documents, and now we'll put ourselves in the shoes of a data steward: How can we describe the data contained in a table, or even in a single column? How can we explain that some data should not be visible to everybody, or that their value is higher, or that they are covered by a SLA?&lt;/p&gt;
&lt;p&gt;Let's take the &lt;code&gt;people&lt;/code&gt; table we created before, which looks like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;surname&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;person_id&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="c1"&gt;------|---------|-----|-----------|&lt;/span&gt;
&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;Jane&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;     &lt;span class="n"&gt;Doe&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;  &lt;span class="mi"&gt;22&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;         &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;John&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;   &lt;span class="n"&gt;Smith&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;  &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;         &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
&lt;span class="p"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We might want to say that this table contains &lt;a href="https://en.wikipedia.org/wiki/Personal_data"&gt;personally identifying information (PII)&lt;/a&gt;, so that other teams can create policies to control the access to such information. In Atlas, doing this is a matter of creating the classification itself:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_classification.png" alt="Atlas classification" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_classification_create.png" alt="Atlas classification" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;and assigning it to the items we want to classify:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_classification_before.png" alt="Atlas classification" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_classification_add.png" alt="Atlas classification" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;An interesting thing we will see is that this classification automatically extends to the &lt;code&gt;joined&lt;/code&gt; table, and that is where Atlas really shines: the classification is propagated thanks to the lineage information that relates all these tables.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_classification_after.png" alt="Atlas classification" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;Let's create another classification, &lt;code&gt;External&lt;/code&gt;, with a &lt;code&gt;Group&lt;/code&gt; string attribute:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_classification_attr.png" alt="Atlas classification" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;This classification is meant to capture the provenance of a certain dataset, and for this reason it is not to be automatically propagated through the lineage (if a certain group is responsible for a dataset, it is not necessarily responsible for derived data). We can prevent the propagation of a classification by clicking on the "Propagate" checkbox when assigning the classification to an item:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_classification_noprop.png" alt="Atlas classification" class="img-fluid" /&gt;&lt;/p&gt;
&lt;h2&gt;Glossary&lt;/h2&gt;
&lt;p&gt;We have seen how to classify the metadata and how the classifications take advantage of lineage information. What if we want to attach tags describing data and metadata according to a specific terminology? In pretty much the same way we have created classifications, we can create one or more &lt;em&gt;glossaries&lt;/em&gt; to group together &lt;em&gt;terms&lt;/em&gt; and &lt;em&gt;categories&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In the "Glossary" section we need first of all to create a glossary, for instance &lt;code&gt;Identification&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_glossary_add.png" alt="Atlas glossary" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;Then, we create a category by clicking on the "Terms/Category" selector and selecting "Create category" from the glossary contextual menu:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_glossary_cat.png" alt="Atlas glossary" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;Now, in the same way, we switch to "Terms" and create the two terms &lt;code&gt;National ID card&lt;/code&gt; and &lt;code&gt;Passport&lt;/code&gt;. If we click on any of the terms, we will see the term's detail page where we can assign classifications and categories. We can even create relationships between terms, for instance to say that a &lt;code&gt;National ID card&lt;/code&gt; is somewhat related to a &lt;code&gt;Passport&lt;/code&gt; via a &lt;code&gt;seeAlso&lt;/code&gt; relation. After this, we can finally assign the terms to any metadata item, for instance to the &lt;code&gt;document&lt;/code&gt; table, by clicking on the plus sign under the "Term" column in the search view:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_terms_assign.png" alt="Atlas glossary" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;For further information, the &lt;a href="https://atlas.apache.org/Glossary.html"&gt;Glossary page&lt;/a&gt; on the Atlas website is very detailed and shows some useful examples.&lt;/p&gt;
&lt;h2&gt;The REST API&lt;/h2&gt;
&lt;p&gt;Another very strong feature that Atlas offers is its rich REST API. Basically, everything that can be done via the UI can be done via the API as well. A &lt;a href="https://atlas.apache.org/2.0.0/api/v2/ui/index.html"&gt;Swagger interactive interface&lt;/a&gt; is available on the website for easier exploration; here we will touch on the main sections with a couple examples.&lt;/p&gt;
&lt;p&gt;As usual, the following API calls can be performed with any client (curl, Postman, etc.). In the following, every call is run via &lt;code&gt;curl&lt;/code&gt; with JSON as the response content type and with the correct access credentials (which, as a default, are &lt;code&gt;admin / admin&lt;/code&gt;); in other words, an API call such as &lt;code&gt;/api/atlas/v2/types/typedefs&lt;/code&gt; is performed with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl -u admin:admin -H &lt;span class="s2"&gt;&amp;quot;Accept: application/json&amp;quot;&lt;/span&gt; http://localhost:21000/api/atlas/v2/types/typedefs
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If the client is based on the command line, a tool such as &lt;code&gt;json_pp&lt;/code&gt; is recommended in order to pretty-print the JSON response.&lt;/p&gt;
&lt;h4&gt;Types&lt;/h4&gt;
&lt;p&gt;To list all entity types:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/types/typedefs
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Description of a single type by type name (e.g. &lt;code&gt;hive_table&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/types/typedef/name/hive_table
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Search&lt;/h4&gt;
&lt;p&gt;Metadata entities can be searched in multiple ways. The basic search endpoint has to be used with at least one parameter, for instance the type name:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/search/basic?typeName=hive_table
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or a full-text query:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/search/basic?typeName=hive_table&amp;amp;query=people
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or a classification:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/search/basic?classification=PII
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we had deleted any entities (e.g. if we dropped any Hive tables), we would see a few entities with a field &lt;code&gt;"status": "DELETED"&lt;/code&gt;; in order to exclude such entities we should add the &lt;code&gt;excludeDeletedEntities=true&lt;/code&gt; parameter:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/search/basic?typeName=hive_table&amp;amp;query=people&amp;amp;excludeDeletedEntities=true
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;An advanced search endpoint using Atlas DSL is available as well, but we will not cover it here. Further information on the DSL can be found &lt;a href="https://atlas.apache.org/Search-Advanced.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Single entity&lt;/h4&gt;
&lt;p&gt;The previous endpoints returned a (possibly empty) list of entities as a result. If we want to explore a single entity, we have to extract its &lt;code&gt;guid&lt;/code&gt; from any of the previous calls. Let's say we are interested in the entity with &lt;code&gt;"guid": "ad9915a8-fdab-4570-964f-8f636a8da20c"&lt;/code&gt;; a rich description of the entity can be obtained by calling this endpoint:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/entity/guid/ad9915a8-fdab-4570-964f-8f636a8da20c
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We might be interested in the entity's classifications only:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/entity/guid/ad9915a8-fdab-4570-964f-8f636a8da20c/classifications
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or in its lineage:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/lineage/ad9915a8-fdab-4570-964f-8f636a8da20c
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since the lineage is a graph, the &lt;code&gt;relations&lt;/code&gt; object will contain a list of all the relationships where each relationship is an edge connecting an entity to another:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;relations&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;fromEntityId&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;edefa7cb-c19d-4955-9da5-22e43786ced5&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;relationshipId&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;33591207-ac0d-4374-91a2-5fadd71d6f0c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;toEntityId&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;9a7712d5-31f8-403a-92ea-77fad0d59e61&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;fromEntityId&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ad9915a8-fdab-4570-964f-8f636a8da20c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;toEntityId&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;edefa7cb-c19d-4955-9da5-22e43786ced5&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;relationshipId&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;22bd9863-0161-4334-b73f-1f33fa8880cb&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to explore each of these relationships, we can call the &lt;code&gt;relationship&lt;/code&gt; endpoint with the &lt;code&gt;guid&lt;/code&gt; of the relationship we are interested in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/relationship/guid/33591207-ac0d-4374-91a2-5fadd71d6f0c
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Glossary&lt;/h4&gt;
&lt;p&gt;To retrieve all glossaries:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/glossary
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To describe a specific glossary by using its &lt;code&gt;guid&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/glossary/dd2de287-96ae-4c24-8402-1eea0d477b59
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and, more specifically, its terms:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/glossary/dd2de287-96ae-4c24-8402-1eea0d477b59/terms
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and categories:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/api/atlas/v2/glossary/dd2de287-96ae-4c24-8402-1eea0d477b59/categories
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;More (meta)data sources&lt;/h2&gt;
&lt;p&gt;In order to see how Atlas can be expanded with more sources, we will add a connector to &lt;a href="https://spark.apache.org/"&gt;Apache Spark&lt;/a&gt; and use it to track the lineage of both standard batch processes and stream processes with &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; as a source. As usual, since the focus of the article is not on any of the two sources, the easiest way to get started with them will be used.&lt;/p&gt;
&lt;h4&gt;Preparation&lt;/h4&gt;
&lt;p&gt;Let's first of all install Spark. We could use the previously installed Apache Bigtop repository and install Spark via &lt;code&gt;yum&lt;/code&gt;, but since the connector needs at least Spark 2.3 to work, we need to &lt;a href="https://spark.apache.org/downloads.html"&gt;download it&lt;/a&gt; (choosing the package pre-built for Hadoop 2.7) and extract it to a folder of our choice, for instance &lt;code&gt;/opt/spark&lt;/code&gt;. When this is done, we need to copy the Atlas configuration file in the Spark configuration folder, for instance (if Atlas was installed in &lt;code&gt;/opt/atlas/apache-atlas-2.0.0/&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; /opt/spark/spark-2.4.3-bin-hadoop2.7
$ cp /opt/atlas/apache-atlas-2.0.0/conf/atlas-application.properties conf/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After this, we need to download the Spark-Atlas connector; we will use &lt;a href="https://github.com/hortonworks-spark/spark-atlas-connector"&gt;Hortonworks's connector&lt;/a&gt; since it has been created for this specific use case. Once again, we will need Maven and Git installed (but we can use any version of Maven):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone https://github.com/hortonworks-spark/spark-atlas-connector
$ &lt;span class="nb"&gt;cd&lt;/span&gt; spark-atlas-connector
$ mvn package -DskipTests
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once the package is built, we need to "patch" Atlas to include the Spark model:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ cp patch/1100-spark_model.json /opt/atlas/apache-atlas-2.0.0/models/1000-Hadoop
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and then we have to restart Atlas:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; /opt/atlas/apache-atlas-2.0.0/
$ ./bin/atlas_stop.py
$ ./bin/atlas_start.py
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we open Atlas again and look into the "Search By Type" dropdown menu, we should see a few more entity types starting with &lt;code&gt;spark_&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Spark batch&lt;/h4&gt;
&lt;p&gt;As an example, let's create a test CSV file in &lt;code&gt;/tmp/test.csv&lt;/code&gt; with this content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;person_id,name,surname
1,John,Smith
2,Jane,Doe
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are now ready to launch the Spark shell as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ /opt/spark/spark-2.4.3-bin-hadoop2.7/bin/spark-shell &lt;span class="se"&gt;\&lt;/span&gt;
    --jars spark-atlas-connector/target/spark-atlas-connector_2.11-0.1.0-SNAPSHOT.jar &lt;span class="se"&gt;\&lt;/span&gt;
    --conf spark.extraListeners&lt;span class="o"&gt;=&lt;/span&gt;com.hortonworks.spark.atlas.SparkAtlasEventTracker &lt;span class="se"&gt;\&lt;/span&gt;
    --conf spark.sql.queryExecutionListeners&lt;span class="o"&gt;=&lt;/span&gt;com.hortonworks.spark.atlas.SparkAtlasEventTracker &lt;span class="se"&gt;\&lt;/span&gt;
    --conf spark.sql.streaming.streamingQueryListeners&lt;span class="o"&gt;=&lt;/span&gt;com.hortonworks.spark.atlas.SparkAtlasStreamingQueryEventTracker
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;From the Spark shell, let's run the following commands to read the test CSV file and write it to a different CSV file with one derived column:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;header&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/tmp/test.csv&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withColumn&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fullname&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;surname&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;))))&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/tmp/test2.csv&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(A series of warnings about configuration values might appear.)&lt;/p&gt;
&lt;p&gt;If everything went fine, on Atlas we should see at least one item when searching by the &lt;code&gt;spark_process&lt;/code&gt; type. When we open the detail page for such item, the lineage section should show something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_spark_batch.png" alt="Atlas-Spark connector" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;We will now save the same file to a Hive table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saveAsTable&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;spark_test_table&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;spark_test_table&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withColumn&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fullname&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;surname&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;))))&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saveAsTable&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;spark_test_table_derived&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now the lineage will look like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_spark_batch_2.png" alt="Atlas-Spark connector" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;(If the lineage does not change, try to click on the "Clear" button and search for Spark processes again.)&lt;/p&gt;
&lt;p&gt;The current version of the connector considers everything that is run in a single shell execution as a single process, so the lineage will include all the steps together.&lt;/p&gt;
&lt;h4&gt;Spark streaming&lt;/h4&gt;
&lt;p&gt;In order to track the lineage of a Spark streaming application, we will use &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; as a stream source. Atlas already includes a Kafka server, but we will download the full package in order to create the topics more easily.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ wget http://apache.mirror.anlx.net/kafka/2.2.0/kafka_2.12-2.2.0.tgz
$ tar zxvf kafka_2.12-2.2.0.tgz
$ &lt;span class="nb"&gt;cd&lt;/span&gt; kafka_2.12-2.2.0/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to create a topic, we can run the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./bin/kafka-topics.sh --create --bootstrap-server localhost:9027 --replication-factor &lt;span class="m"&gt;1&lt;/span&gt; --partitions &lt;span class="m"&gt;1&lt;/span&gt; --topic test-topic
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The address used as a bootstrap server has to be the same that is configured as the value of the &lt;code&gt;atlas.kafka.bootstrap.servers&lt;/code&gt; parameter in &lt;code&gt;atlas-application.properties&lt;/code&gt; (the Atlas configuration file within its &lt;code&gt;conf&lt;/code&gt; directory), unless we want to run a separate Kafka server.&lt;/p&gt;
&lt;p&gt;Now we need to store some data into Kafka. We'll do it with the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ./bin/kafka-console-producer.sh --broker-list localhost:9027 --topic test-topic
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where we will be prompted to insert one or more lines; after inserting a few lines, &lt;code&gt;Ctrl + D&lt;/code&gt; will close the prompt. We are now ready to run the Spark shell again, this time adding &lt;code&gt;--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3&lt;/code&gt; to avoid Kafka-related exceptions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ /opt/spark/spark-2.4.3-bin-hadoop2.7/bin/spark-shell &lt;span class="se"&gt;\&lt;/span&gt;
    --jars /opt/atlas/spark-atlas-connector/spark-atlas-connector-assembly/target/spark-atlas-connector-assembly-0.1.0-SNAPSHOT.jar &lt;span class="se"&gt;\&lt;/span&gt;
    --conf spark.extraListeners&lt;span class="o"&gt;=&lt;/span&gt;com.hortonworks.spark.atlas.SparkAtlasEventTracker &lt;span class="se"&gt;\&lt;/span&gt;
    --conf spark.sql.queryExecutionListeners&lt;span class="o"&gt;=&lt;/span&gt;com.hortonworks.spark.atlas.SparkAtlasEventTracker &lt;span class="se"&gt;\&lt;/span&gt;
    --conf spark.sql.streaming.streamingQueryListeners&lt;span class="o"&gt;=&lt;/span&gt;com.hortonworks.spark.atlas.SparkAtlasStreamingQueryEventTracker &lt;span class="se"&gt;\&lt;/span&gt;
    --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;From the shell, we can then run this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;readStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;kafka&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;kafka.bootstrap.servers&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;localhost:9027&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;subscribe&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;test-topic&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;startingOffsets&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;earliest&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;df2&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;selectExpr&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;topic&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;CAST(key AS STRING)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;CAST(value AS STRING)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writeStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;outputMode&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;append&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;json&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;path&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;/tmp/stream_kafka.json&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;checkpointLocation&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;/tmp/chkpoint_dir&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where we basically create a dataset out of a Kafka stream by subscribing to the &lt;code&gt;test-topic&lt;/code&gt; topic and starting from its beginning, then we create a new stream to be written as a JSON file (actually, as a directory containing multiple JSON files).&lt;/p&gt;
&lt;p&gt;If we check the lineage for this process, we should now see something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_spark_streaming.png" alt="Atlas-Spark connector" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;We can even route the data from the first topic into another Kafka topic! All we need to do is to create a new topic:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ /opt/kafka/kafka_2.12-2.2.0/bin/kafka-topics.sh --create --bootstrap-server localhost:9027 --replication-factor &lt;span class="m"&gt;1&lt;/span&gt; --partitions &lt;span class="m"&gt;1&lt;/span&gt; --topic test-topic-rec
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and then, from the shell, run this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;readStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;kafka&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;kafka.bootstrap.servers&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;localhost:9027&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;subscribe&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;test-topic&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;startingOffsets&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;earliest&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;val&lt;/span&gt; &lt;span class="n"&gt;df2&lt;/span&gt; &lt;span class="k"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;selectExpr&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;topic&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;CAST(key AS STRING)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;CAST(value AS STRING)&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writeStream&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;kafka&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;kafka.bootstrap.servers&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;localhost:9027&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;topic&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;test-topic-rec&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;option&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;checkpointLocation&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;/tmp/chkpoint_dir_rec&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which is the same as before except for the different type of write stream (Kafka rather than JSON files), and will result in this lineage:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_spark_streaming_2.png" alt="Atlas-Spark connector" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;(In case an exception such as &lt;code&gt;org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server 'localhost:9026' with timeout of 200 ms&lt;/code&gt; is thrown, the value of the &lt;code&gt;atlas.kafka.zookeeper.connection.timeout.ms&lt;/code&gt; field within the &lt;code&gt;atlas-application.properties&lt;/code&gt; configuration file should be increased (e.g. set it to 2000) and Atlas be restarted.)&lt;/p&gt;
&lt;p&gt;As we can see, lineage information from a Spark batch or streaming job can be easily sent to Atlas. Spark ML pipelines are also supported by the connector, but since this would require patching and recompiling Atlas, I decided to skip them for the time being.&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;In this article we have explored Atlas capabilities into more details, understanding what classifications and glossaries are and how to make use of them, then listing a few REST API endpoints along with examples, and finally trying out an external connector to add even more lineage information. Atlas is quite mature as a solution, but documentation is still a bit scattered; even if I haven't touched on everything (especially the underlying datastores and the creation of new entities), I hope to have added a little bit to the coverage of this fantastic tool.&lt;/p&gt;</content><category term="Big Data"></category></entry><entry><title>Apache Atlas</title><link href="https://apothem.blog/apache-atlas.html" rel="alternate"></link><published>2019-06-30T00:00:00+01:00</published><updated>2019-06-30T00:00:00+01:00</updated><author><name>Nicola Vitucci</name></author><id>tag:apothem.blog,2019-06-30:/apache-atlas.html</id><summary type="html">&lt;p&gt;Since I have always been interested in (and mainly working with) Semantic Web technologies and knowledge engineering, metadata is a topic I care about quite a lot. "Metadata" means "data about data", which practically speaking may include the format, the source, the purpose, the author, the creation date, and many â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Since I have always been interested in (and mainly working with) Semantic Web technologies and knowledge engineering, metadata is a topic I care about quite a lot. "Metadata" means "data about data", which practically speaking may include the format, the source, the purpose, the author, the creation date, and many other aspects of some given data. Many languages and tools have been proposed to make the inclusion and management of metadata easier, but only recently there has been a renewed interest in metadata from a Big Data perspective; in fact, given the vast number of data sources and processes that a Big Data architecture might have to manage, a scalable tool for metadata management and governance was much needed. Enter &lt;a href="https://atlas.apache.org/"&gt;Apache Atlas&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From its homepage:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Apache Atlas provides open metadata management and governance capabilities for organizations to build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts and the data governance team.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In short, Atlas is a tool that adds metadata management features (descriptions, classifications, controlled vocabularies, tagging, search, and so on) to a Hadoop-based data environment (such as Hive) through bridges, hooks (for automated updates) and an easy-to-use UI.&lt;/p&gt;
&lt;p&gt;So far, Atlas has mostly been used and deployed with the &lt;a href="https://hortonworks.com/products/data-platforms/hdp/"&gt;Hortonworks Data Platform (HDP)&lt;/a&gt; and, recently, added to AWS as an &lt;a href="https://aws.amazon.com/blogs/big-data/metadata-classification-lineage-and-discovery-using-apache-atlas-on-amazon-emr/"&gt;Amazon EMR template&lt;/a&gt;. Since I wanted to look at its latest version and install it &lt;em&gt;vanilla&lt;/em&gt;, this article will be different from the previous ones in that I will write about a more widely used project and, being it a bit lacking in documentation, my main focus will be to make it work rather than providing many examples. I will then take the chance to touch on another nice Apache project, &lt;a href="https://bigtop.apache.org/"&gt;Bigtop&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the following, I will use CentOS 7 as the underlying operating system. If you are not already using it, you can either install it on a &lt;a href="https://wiki.centos.org/HowTos/Virtualization/Introduction"&gt;virtual machine&lt;/a&gt; using your favourite VM tool, or use a &lt;a href="https://docs.docker.com/install/linux/docker-ce/centos/"&gt;Docker image&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Installing Bigtop&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://bigtop.apache.org/"&gt;Apache Bigtop&lt;/a&gt; is a comprehensive Big Data package, offered both as a downloadable archive and as an RPM repository. Its intent is to make the installation and management of Big Data tools easy. Although being actively developed, it is not as up-to-date as other commercially-backed solutions; this means that Bigtop components are often a few versions behind their standalone counterparts. Since this article is about Atlas, this will not be an issue as we do not need the latest version of any of the components.&lt;/p&gt;
&lt;p&gt;In order to use Bigtop and to have Hadoop and Hive available as CentOS RPM packages, we need to install the Bigtop repository by downloading the &lt;a href="http://apache.mirror.anlx.net/bigtop/bigtop-1.4.0/repos/centos7/bigtop.repo"&gt;bigtop.repo&lt;/a&gt; file into the &lt;code&gt;/etc/yum.repos.d/&lt;/code&gt; folder on our CentOS environment.&lt;/p&gt;
&lt;h3&gt;Installing Hadoop and Hive&lt;/h3&gt;
&lt;p&gt;Our first step will be to have a working Hadoop environment, with Hive on top of it. With the Bigtop repository now available, we can install Hive (and the other components it depends on) with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ yum install hive hive-metastore hive-server2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since this is a "development" installation, we will use the embedded Derby database as Hive metastore; for a production environment, a fully-fledged DBMS such as PostgreSQL or MySQL should be used. &lt;/p&gt;
&lt;p&gt;In order to initialize the metastore, we need to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find where the Derby DB will be located; it should be &lt;code&gt;/var/lib/hive/metastore/metastore_db&lt;/code&gt;, or anyway the value of the &lt;code&gt;javax.jdo.option.ConnectionURL&lt;/code&gt; property in the &lt;code&gt;hive-site.xml&lt;/code&gt; file (likely located under &lt;code&gt;/etc/hive/conf/&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to the parent folder of the metastore root folder (in our case &lt;code&gt;/var/lib/hive/&lt;/code&gt;) and run the following command as root:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ /usr/lib/hive/bin/schematool -initSchema -dbType derby
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we want to use another user or to change the location of the Derby DB, we need to update the &lt;code&gt;hive-site.xml&lt;/code&gt; accordingly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch &lt;code&gt;hive-metastore&lt;/code&gt; and &lt;code&gt;hive-server2&lt;/code&gt; with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo service hive-metastore start
$ sudo service hive-hiveserver2 start
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try running a Hive query, for instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo hive -e &lt;span class="s2"&gt;&amp;quot;show databases;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(&lt;code&gt;sudo&lt;/code&gt; is necessary if we performed the previous operations as root).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If everything ran successfully, we should see something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;OK
default
Time taken: 8.111 seconds, Fetched: 1 row(s)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;surrounded by other log lines. If the Hive query fails with an exception related to access rights, we might need to execute the following operations:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ hdfs dfs -mkdir /tmp
$ hdfs dfs -mkdir /user/hive/warehouse
$ hdfs dfs -chmod g+w /tmp
$ hdfs dfs -chmod g+w /user/hive/warehouse
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Installing Atlas&lt;/h3&gt;
&lt;h4&gt;Java environment&lt;/h4&gt;
&lt;p&gt;In order to install Atlas, let's first of all make sure that Java 8 is installed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ java -version
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It is important here that the Java version be 8, because full compatibility with newer versions is not guaranteed. If the installed version of Java is not 8, we need to install Java 8 alongside; then, in the following, we have to replace the &lt;code&gt;java&lt;/code&gt; command with the full path of the Java 8 binary (e.g. &lt;code&gt;/usr/lib/jvm/java-1.8.0/jre/bin/java&lt;/code&gt;) or take advantage of the &lt;code&gt;alternatives&lt;/code&gt; tool (as explained for example &lt;a href="https://access.redhat.com/documentation/en-US/JBoss_Communications_Platform/5.1/html/Platform_Installation_Guide/sect-Setting_the_Default_JDK.html"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;After this, let's make sure that the environment variable &lt;code&gt;JAVA_HOME&lt;/code&gt; is initialized:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$JAVA_HOME&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If the result is an empty string, we need to initialize it with a command such as this one:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;JAVA_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.212.b04-0.el7_6.x86_64/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;using whatever path leads to our Java SDK (&lt;em&gt;not&lt;/em&gt; JRE) home folder. It is not needed to add this line to the &lt;code&gt;.bashrc&lt;/code&gt; file if all the steps are performed from the same terminal within the same session, although it would be still useful to do so.&lt;/p&gt;
&lt;h4&gt;Installing Maven&lt;/h4&gt;
&lt;p&gt;Let's now install Maven, which is roughly to Java what &lt;code&gt;pip&lt;/code&gt; is to Python. The version we will need in order to build Atlas correctly is at least the 3.5, and likely the CentOS 7 distribution we are using only provides an older version. If that is the case (and it can be found out by trying to run &lt;code&gt;sudo yum install maven&lt;/code&gt; and looking at the version of the package), we need to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Add the &lt;a href="https://wiki.centos.org/AdditionalResources/Repositories/SCL"&gt;Software Collections repository&lt;/a&gt; to &lt;code&gt;yum&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo yum install centos-release-scl
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;code&gt;rh-maven35&lt;/code&gt; package:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo yum install rh-maven35
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Open a shell in the environment containing the installed package:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ scl &lt;span class="nb"&gt;enable&lt;/span&gt; rh-maven35 bash
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check the Maven version:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ mvn --version
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If everything worked correctly, Maven 3.5 should be available in the new shell. This environment will only be needed to compile and build Atlas, after which it can be closed.&lt;/p&gt;
&lt;h4&gt;Building Atlas&lt;/h4&gt;
&lt;p&gt;Now we can download Atlas from the &lt;a href="https://www.apache.org/dyn/closer.cgi/atlas/2.0.0/apache-atlas-2.0.0-sources.tar.gz"&gt;download page&lt;/a&gt; and decompress it in a folder of our choice:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tar zxvf apache-atlas-2.0.0-sources.tar.gz
$ &lt;span class="nb"&gt;cd&lt;/span&gt; apache-atlas-sources-2.0.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to run Atlas in the simplest way, we will build it with both Apache Cassandra (the underlying database) and Apache Solr (the underlying search engine) as embedded services; in a production environment, the two services would be configured each on its own. Let's run this command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ mvn clean -DskipTests package -Pdist,embedded-cassandra-solr
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and wait until it completes successfully, after which we run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; ..
$ tar zxvf apache-atlas-sources-2.0.0/distro/target/apache-atlas-2.0.0-server.tar.gz
$ &lt;span class="nb"&gt;cd&lt;/span&gt; apache-atlas-2.0.0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are now in the main Atlas folder, from which we can launch the Atlas server:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ bin/atlas_start.py
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;then open the browser on &lt;code&gt;http://localhost:21000&lt;/code&gt; and presto! Atlas is available and ready for our experiments.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_login.png" alt="Atlas login" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;The default credentials are &lt;code&gt;admin / admin&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_main.png" alt="Atlas main screen" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;We can also run a "quick start" script from the same folder and populate Atlas with some example data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ bin/quick_start.py
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and start exploring the tool.&lt;/p&gt;
&lt;h4&gt;Installing the Hive hook&lt;/h4&gt;
&lt;p&gt;Now that Atlas is up and running, we need to let Atlas "know" about Hive metadata and listen for changes. First of all, let's make the Hive hook available to Atlas:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tar zxvf ../apache-atlas-sources-2.0.0/distro/target/apache-atlas-2.0.0-hive-hook.tar.gz
$ mv apache-atlas-hive-hook-2.0.0/* .
$ rm -rf apache-atlas-hive-hook-2.0.0/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Because of &lt;a href="https://issues.apache.org/jira/browse/ATLAS-3172"&gt;a bug&lt;/a&gt;, some JAR files need to be copied manually:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ cp ../apache-atlas-sources-2.0.0/webapp/target/atlas-webapp-2.0.0/WEB-INF/lib/jackson-jaxrs-base-2.9.8.jar hook/hive/atlas-hive-plugin-impl
$ cp ../apache-atlas-sources-2.0.0/webapp/target/atlas-webapp-2.0.0/WEB-INF/lib/jackson-jaxrs-json-provider-2.9.8.jar hook/hive/atlas-hive-plugin-impl
$ cp ../apache-atlas-sources-2.0.0/webapp/target/atlas-webapp-2.0.0/WEB-INF/lib/jackson-module-jaxb-annotations-2.9.8.jar hook/hive/atlas-hive-plugin-impl
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is enough from Atlas side; now we need to configure Hive as well so that it knows where to send data to Atlas when a change occurs. First of all we need to initialize the &lt;code&gt;HIVE_HOME&lt;/code&gt; environment variable:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;HIVE_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/lib/hive
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, we need to copy (or create a symlink to) the Atlas configuration files into Hive configuration folder:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ cp conf/atlas-application.properties /etc/hive/conf/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and to update the &lt;code&gt;/etc/hive/conf/hive-site.xml&lt;/code&gt; adding this content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hive.exec.post.hooks&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;org.apache.atlas.hive.hook.HiveHook&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;then, finally, to add the following line to &lt;code&gt;/etc/hive/conf/hive-env.sh&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;HIVE_AUX_JARS_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;lt;atlas package&amp;gt;/hook/hive
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Hive-Atlas connection&lt;/h3&gt;
&lt;p&gt;We are now ready to check that Hive can actually send information to Atlas. Let's launch the Hive CLI (with &lt;code&gt;sudo&lt;/code&gt; if the Derby DB was initialized as root):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ hive
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and run the following commands to create a &lt;code&gt;test&lt;/code&gt; database and a &lt;code&gt;people&lt;/code&gt; table with some data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;create&lt;/span&gt; &lt;span class="k"&gt;database&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;use&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;people&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;surname&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;person_id&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;people&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;John&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Smith&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;people&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Jack&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Smith&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;people&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;Jane&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;Doe&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and let's run a &lt;code&gt;SELECT&lt;/code&gt; query to check that the data has been inserted correctly:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;people&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If everything ran correctly, we should see something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;OK
Jane    Doe     22      3
John    Smith   30      1
Jack    Smith   32      2
Time taken: 9.918 seconds, Fetched: 3 row(s)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we can add another table with some more data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;documents&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;person_id&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;doc_id&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;country&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;documents&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;US&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;documents&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;US&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;documents&lt;/span&gt; &lt;span class="k"&gt;VALUES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;UK&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and, finally, a new table from a &lt;code&gt;JOIN&lt;/code&gt; across the two tables just created:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;joined&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;people&lt;/span&gt; &lt;span class="k"&gt;JOIN&lt;/span&gt; &lt;span class="n"&gt;documents&lt;/span&gt; &lt;span class="k"&gt;USING&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;person_id&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As a check, we can run another &lt;code&gt;SELECT&lt;/code&gt; query:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hive&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;joined&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and verify that the output is like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;OK&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;       &lt;span class="n"&gt;Jane&lt;/span&gt;    &lt;span class="n"&gt;Doe&lt;/span&gt;     &lt;span class="mi"&gt;22&lt;/span&gt;      &lt;span class="mi"&gt;3&lt;/span&gt;       &lt;span class="n"&gt;UK&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="n"&gt;John&lt;/span&gt;    &lt;span class="n"&gt;Smith&lt;/span&gt;   &lt;span class="mi"&gt;30&lt;/span&gt;      &lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="n"&gt;US&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt;       &lt;span class="n"&gt;Jack&lt;/span&gt;    &lt;span class="n"&gt;Smith&lt;/span&gt;   &lt;span class="mi"&gt;32&lt;/span&gt;      &lt;span class="mi"&gt;2&lt;/span&gt;       &lt;span class="n"&gt;US&lt;/span&gt;
&lt;span class="n"&gt;Time&lt;/span&gt; &lt;span class="n"&gt;taken&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;297&lt;/span&gt; &lt;span class="n"&gt;seconds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Fetched&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="k"&gt;row&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;During this session we might receive this warning:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We do not need to worry about it, because this is related to the way Hive runs. From the point of view of Atlas, it does not make any difference.&lt;/p&gt;
&lt;p&gt;Let's check if Atlas is aware of these changes by selecting &lt;code&gt;hive_table&lt;/code&gt; in the &lt;code&gt;Search by type&lt;/code&gt; dropdown menu.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_hive_tables.png" alt="Atlas - Hive tables" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;The three tables can be seen in the main section, and for each of the tables we can see useful information such as their schema:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_schema.png" alt="Atlas - Schema" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;and their lineage:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/atlas_lineage.png" alt="Atlas - Lineage" class="img-fluid" /&gt;&lt;/p&gt;
&lt;p&gt;which we will expand upon in another article.&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;In this article we saw how to install Apache Atlas from scratch (a not-so-straightforward process) and how to configure Hive hooks in order to automatically send changes in the Hive metastore to Atlas. In another article we will look a bit deeper into Atlas capabilities and we will see how to configure more data sources.&lt;/p&gt;</content><category term="Big Data"></category></entry><entry><title>Apache MetaModel</title><link href="https://apothem.blog/apache-metamodel.html" rel="alternate"></link><published>2019-05-31T00:00:00+01:00</published><updated>2019-05-31T00:00:00+01:00</updated><author><name>Nicola Vitucci</name></author><id>tag:apothem.blog,2019-05-31:/apache-metamodel.html</id><summary type="html">&lt;p&gt;It's a few years now since I've got the "&lt;a href="https://martinfowler.com/bliki/PolyglotPersistence.html"&gt;polyglot persistence&lt;/a&gt;" bug, first out of interest, then out of necessity. Given the abundance of data models and storage technologies available today, it is crucial to be aware of the strengths and weaknesses of each solution; furthermore, more often than not â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's a few years now since I've got the "&lt;a href="https://martinfowler.com/bliki/PolyglotPersistence.html"&gt;polyglot persistence&lt;/a&gt;" bug, first out of interest, then out of necessity. Given the abundance of data models and storage technologies available today, it is crucial to be aware of the strengths and weaknesses of each solution; furthermore, more often than not, an architecture that integrates several types of solutions is desirable or needed.&lt;/p&gt;
&lt;p&gt;Leaving architectural questions aside, the main question in this polyglot scenario is: how to read and interpret data from disparate data sources in a reliable and uniform fashion? Is it better to create a data lake out of all the sources, or to take a federated approach where every query is dispatched to its appropriate database? The Apache project we will work with today, &lt;a href="https://metamodel.apache.org/"&gt;MetaModel&lt;/a&gt;, aims at providing tools to deal with such challenges.&lt;/p&gt;
&lt;h3&gt;Main concepts&lt;/h3&gt;
&lt;p&gt;Apache MetaModel provides &lt;em&gt;"a common interface for discovery, exploration of metadata and querying of different types of data sources"&lt;/em&gt;, which means that, through the use of concepts such as &lt;em&gt;datasets&lt;/em&gt;, &lt;em&gt;tables&lt;/em&gt;, and &lt;em&gt;columns&lt;/em&gt;, it exposes a basic abstraction common to all the data sources it can connect to; such abstract data model can then be queried using SQL as an "interlanguage". It is important to note that, as stated on the website, &lt;em&gt;"MetaModel &lt;em&gt;isn't&lt;/em&gt; a data mapping framework"&lt;/em&gt;; in other words its main intended usage is not to integrate different terminologies within the same domain, but rather to make it easy to add new datasources and to maximize the usage of metadata.&lt;/p&gt;
&lt;p&gt;MetaModel is not the only library to provide tools for data integration. A similar approach is offered by other Apache projects such as &lt;a href="https://spark.apache.org/"&gt;Spark&lt;/a&gt; or &lt;a href="https://drill.apache.org/"&gt;Drill&lt;/a&gt;, but MetaModel might be better suited than such Big Data tools in scenarios where the main challenge is the diversity in the data and the schema variability rather than the scale; it can be plugged into an existing project with no additional setup and it offers many convenience methods to start using data sources (including plain CSV, JSON and XML files!) right away. Now let's see how to include MetaModel in a project, along with a few examples.&lt;/p&gt;
&lt;p&gt;(Note: in some sections of this article I will make use of Docker for convenience. If you don't want to use Docker at the moment, you can just skip those sections; otherwise, make sure to &lt;a href="https://docs.docker.com/install/"&gt;install it&lt;/a&gt; in order to follow.)&lt;/p&gt;
&lt;h3&gt;Setup&lt;/h3&gt;
&lt;p&gt;The best way to get started with MetaModel is to include it as a Maven dependency. If you use an IDE such as Eclipse or IntelliJ, all you need to do is to create a new &lt;code&gt;apache-metamodel-example&lt;/code&gt; Maven project; otherwise, you can have a look &lt;a href="https://maven.apache.org/guides/getting-started/maven-in-five-minutes.html"&gt;here&lt;/a&gt; to get an idea of how to install and work with Maven. In any case, as usual you will find a fully working project on the &lt;a href="https://github.com/nvitucci/apothem-resources"&gt;associated repository&lt;/a&gt; under the &lt;code&gt;apache-metamodel&lt;/code&gt; folder.&lt;/p&gt;
&lt;p&gt;The content of the &lt;code&gt;pom.xml&lt;/code&gt; file should look like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;project&lt;/span&gt; &lt;span class="na"&gt;xmlns=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://maven.apache.org/POM/4.0.0&amp;quot;&lt;/span&gt;
         &lt;span class="na"&gt;xmlns:xsi=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot;&lt;/span&gt;
         &lt;span class="na"&gt;xsi:schemaLocation=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;modelVersion&amp;gt;&lt;/span&gt;4.0.0&lt;span class="nt"&gt;&amp;lt;/modelVersion&amp;gt;&lt;/span&gt;

     &lt;span class="nt"&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;groupId&lt;span class="nt"&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;apache-metamodel-example&lt;span class="nt"&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;version&amp;gt;&lt;/span&gt;0.0.1-SNAPSHOT&lt;span class="nt"&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;

     &lt;span class="nt"&gt;&amp;lt;dependencies&amp;gt;&lt;/span&gt;
         &lt;span class="nt"&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
             &lt;span class="nt"&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;org.apache.metamodel&lt;span class="nt"&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
             &lt;span class="nt"&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;MetaModel-full&lt;span class="nt"&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
             &lt;span class="nt"&gt;&amp;lt;version&amp;gt;&lt;/span&gt;5.3.0&lt;span class="nt"&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
         &lt;span class="nt"&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;
     &lt;span class="nt"&gt;&amp;lt;/dependencies&amp;gt;&lt;/span&gt;
 &lt;span class="nt"&gt;&amp;lt;/project&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once the &lt;code&gt;pom.xml&lt;/code&gt; file is saved, your IDE should download all the needed dependencies automatically; if you are doing everything manually, you can run the following command to trigger a first build:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ mvn package
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now let's create a new Java file called &lt;code&gt;MetaModelExample.java&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;package&lt;/span&gt; &lt;span class="nn"&gt;com.example&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.metamodel.data.DataSet&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;org.apache.metamodel.data.Row&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;MetaModelExample&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;[])&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;MetaModelExample&lt;/span&gt; &lt;span class="n"&gt;example&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;MetamodelExample&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you don't get any errors in the imports, the project is set up correctly.&lt;/p&gt;
&lt;h3&gt;Examples&lt;/h3&gt;
&lt;p&gt;We can now start to add a few different data sources to explore MetaModel's capabilities.&lt;/p&gt;
&lt;h4&gt;CSV files&lt;/h4&gt;
&lt;p&gt;Let's start with a simple CSV file. We can manually create an &lt;code&gt;example.csv&lt;/code&gt; file under &lt;code&gt;/tmp&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;project,language,completed
Project1,Java,true
Project2,Java,false
Project3,Python,true
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's add the following method to the &lt;code&gt;MetaModelExample&lt;/code&gt; class:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;processCsv&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;DataContext&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataContextFactory&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;createCsvDataContext&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
    &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;tableNames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getDefaultSchema&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;getTableNames&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

    &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Arrays&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tableNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toArray&lt;/span&gt;&lt;span class="o"&gt;()));&lt;/span&gt;

    &lt;span class="kd"&gt;final&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tableNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;get&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;

    &lt;span class="n"&gt;DataSet&lt;/span&gt; &lt;span class="n"&gt;dataSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;query&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;from&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;select&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;project&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;where&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;language&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;eq&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Java&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;and&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;completed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;eq&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;execute&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Row&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;dataSet&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toRows&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt;
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Arrays&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getValues&lt;/span&gt;&lt;span class="o"&gt;()));&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and then a call to this method within the main:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;MetaModelExample&lt;/span&gt; &lt;span class="n"&gt;example&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;MetamodelExample&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

&lt;span class="n"&gt;example&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;processCsv&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/tmp/example.csv&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we run the main method, we will get the following output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;[example.csv, default_table]
[Project2]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;What have we just done?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We created a &lt;code&gt;DataContext&lt;/code&gt; (more specifically, a &lt;code&gt;CsvDataContext&lt;/code&gt;) wrapping the CSV file.&lt;/li&gt;
&lt;li&gt;We extracted the names of all the tables from the default schema and printed them; since the data source is a single file, the default schema is the only available schema and there is only one table (the second one being a convenience &lt;code&gt;default_table&lt;/code&gt; alias).&lt;/li&gt;
&lt;li&gt;We ran a SQL query on the first table: we selected the &lt;code&gt;project&lt;/code&gt; column from the rows where the &lt;code&gt;language&lt;/code&gt; is &lt;code&gt;Java&lt;/code&gt; and &lt;code&gt;completed&lt;/code&gt; is &lt;code&gt;false&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;We printed the result of the query.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, in the end, we ran a SQL query on a CSV file! Perhaps even more interestingly, we can update the file with some update instructions by replacing the &lt;code&gt;DataContext&lt;/code&gt; class with &lt;code&gt;UpdateableDataContext&lt;/code&gt; and then adding the following lines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;executeUpdate&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;UpdateScript&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;UpdateCallback&lt;/span&gt; &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;update&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;value&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;completed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;where&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;language&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;eq&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Java&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;execute&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;insertInto&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;value&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;project&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Project4&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;value&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;language&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Java&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;value&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;completed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;false&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
                &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;execute&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;});&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we run again the previous query, we'll see that the file has been updated and &lt;code&gt;Project2&lt;/code&gt; is no longer listed, while &lt;code&gt;Project4&lt;/code&gt; is. Neat!&lt;/p&gt;
&lt;h4&gt;XLSX files&lt;/h4&gt;
&lt;p&gt;Now let's look at a slightly more complex example. Spreadsheets can contain more than one table, which makes them closer to a relational database; MetaModel can use XLSX (&lt;a href="https://en.wikipedia.org/wiki/Office_Open_XML"&gt;Office Open XML&lt;/a&gt; Workbook) files, which can be read and saved with open source tools such as LibreOffice and OpenOffice. You can find an example file on the repo, or create your own one. All we need to do is to add a method such as this one:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;processSpreadsheet&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;DataContext&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataContextFactory&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;createExcelDataContext&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;

    &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sheetNames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getDefaultSchema&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;getTableNames&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Arrays&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sheetNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toArray&lt;/span&gt;&lt;span class="o"&gt;()));&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;sheetName&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sheetNames&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;sheetColumns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getDefaultSchema&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;getTableByName&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sheetName&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;getColumns&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Column&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sheetColumns&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getName&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;

        &lt;span class="n"&gt;DataSet&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;query&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;from&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sheetName&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;selectAll&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;execute&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Row&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toRows&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt;
            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Arrays&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getValues&lt;/span&gt;&lt;span class="o"&gt;()));&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and include it in the main method as we did with the previous example. The output will show the name of all the sheets and, for each sheet, the names of its columns and its data. But there is more! Since in this example the first column of each sheet represents the ID of a customer, we can perform a join between the two sheets by adding the following lines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;DataSet&lt;/span&gt; &lt;span class="n"&gt;joined&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;query&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;from&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sheetNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;get&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;innerJoin&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sheetNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;get&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)).&lt;/span&gt;&lt;span class="na"&gt;on&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;select&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Names.surname&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Products.amount&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;execute&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Row&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;joined&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toRows&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Arrays&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getValues&lt;/span&gt;&lt;span class="o"&gt;()));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and the output will show the selected columns from the joined table.&lt;/p&gt;
&lt;h4&gt;JSON files&lt;/h4&gt;
&lt;p&gt;So far we have looked at tabular data, but what if we have data structured in a different format such as JSON? We may need to make some compromise between the expressivity of a non-relational model and the ease of use of a relational model, so in some cases we will need to "flatten out" some internal fields. Given instead a simple (and quite common) case, where the file is an array containing objects with the same fields like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;we can write code similar to what we already wrote before and run SQL queries on JSON files:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;processJson&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;DataContext&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataContextFactory&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;createJsonDataContext&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;

    &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;tableNames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getDefaultSchema&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;getTableNames&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Arrays&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tableNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toArray&lt;/span&gt;&lt;span class="o"&gt;()));&lt;/span&gt;

    &lt;span class="n"&gt;DataSet&lt;/span&gt; &lt;span class="n"&gt;dataSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;query&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;from&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tableNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;get&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;select&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;where&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;gte&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
            &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;execute&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Row&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;dataSet&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toRows&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt;
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Arrays&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getValues&lt;/span&gt;&lt;span class="o"&gt;()));&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Databases&lt;/h4&gt;
&lt;p&gt;By now we have understood how to "enhance" static files with querying capabilities, but what if we want to connect to a real datastore such as SQLite, PostgreSQL or MongoDB?&lt;/p&gt;
&lt;p&gt;MetaModel provides connectors for a wide variety of databases, including a generic JDBC connector and a specific connector for MongoDB, which support schema creation and inserts/updates as well. Let's see an example with MongoDB:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;connectToMongo&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;UpdateableDataContext&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataContextFactory&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;createMongoDbDataContext&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;
            &lt;span class="s"&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;27017&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;test&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;

    &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;tableNames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getDefaultSchema&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;getTableNames&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tableNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;isEmpty&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;executeUpdate&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;UpdateScript&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;UpdateCallback&lt;/span&gt; &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
                &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;mytable&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

                &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;createTable&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getDataContext&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;getDefaultSchema&lt;/span&gt;&lt;span class="o"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
                        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;withColumn&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;color&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;ofType&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ColumnType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;VARCHAR&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
                        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;withColumn&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;size&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;ofType&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ColumnType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;CHAR&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
                        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;execute&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

                &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;insertInto&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;value&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;color&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;red&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;value&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;size&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;L&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;execute&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
                &lt;span class="n"&gt;callback&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;insertInto&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;table&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;value&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;color&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;yellow&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;value&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;size&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;S&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;).&lt;/span&gt;&lt;span class="na"&gt;execute&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
            &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="o"&gt;});&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;tableNames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getDefaultSchema&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;getTableNames&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Arrays&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tableNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toArray&lt;/span&gt;&lt;span class="o"&gt;()));&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this example, we create a connection to a MongoDB instance running on &lt;code&gt;localhost:27017&lt;/code&gt; and we connect to the &lt;code&gt;test&lt;/code&gt; database (which might not exist yet); then, we get the list of "tables" (MongoDB collections) and, if none is found, we create a new one called &lt;code&gt;mytable&lt;/code&gt; with "columns" (MongoDB document fields) &lt;code&gt;color&lt;/code&gt; and &lt;code&gt;size&lt;/code&gt;; finally, we insert two example "records" (MongoDB documents) in the newly created collections. In order to make sure that this works, we can add a query section like in the previous examples:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;DataSet&lt;/span&gt; &lt;span class="n"&gt;dataSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;query&lt;/span&gt;&lt;span class="o"&gt;().&lt;/span&gt;&lt;span class="na"&gt;from&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tableNames&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;get&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;selectAll&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt;
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;where&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;size&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;eq&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;S&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
        &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;execute&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Row&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;dataSet&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toRows&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Arrays&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getValues&lt;/span&gt;&lt;span class="o"&gt;()));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and we can see that only the second record will be printed, with the values we inserted plus an additional value (the &lt;code&gt;_id&lt;/code&gt; field that MongoDB creates automatically if none is supplied).&lt;/p&gt;
&lt;p&gt;In order to run this example, the easiest solution is to download and run a MongoDB Docker image:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ docker pull mongo
$ docker run -p &lt;span class="m"&gt;27017&lt;/span&gt;:27017 mongo
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If this is not an option, MongoDB can be installed manually following the instructions &lt;a href="https://www.mongodb.com/download-center/community"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Other data sources can be connected to and used in a similar fashion.&lt;/p&gt;
&lt;h3&gt;Apache Membrane&lt;/h3&gt;
&lt;p&gt;A fantastic and little known addition to MetaModel is its subproject &lt;a href="https://cwiki.apache.org/confluence/display/METAMODEL/Membrane"&gt;Membrane&lt;/a&gt;. Membrane is essentially a &lt;a href="https://en.wikipedia.org/wiki/Representational_state_transfer"&gt;RESTful&lt;/a&gt; Web service that can be used to manage and query different data sources &lt;em&gt;live&lt;/em&gt; by adding them to &lt;em&gt;"tenants"&lt;/em&gt; (which are basically independent contexts, each with its own connections to any number of data sources).&lt;/p&gt;
&lt;p&gt;In order to run Membrane, we should clone the repository and build it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone https://github.com/apache/metamodel-membrane
$ mvn clean install
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since it has many dependencies, the building process will take a while. When the build is ready, we can run it with the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ java -server -jar undertow/target/membrane-undertow-server.jar
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The server will now run on port 8080, and we can submit requests to it by using &lt;code&gt;curl&lt;/code&gt; or any REST client such as &lt;a href="https://www.getpostman.com/"&gt;Postman&lt;/a&gt;, &lt;a href="https://install.advancedrestclient.com/"&gt;Advanced REST Client&lt;/a&gt;, &lt;a href="https://restlet.com/modules/client/"&gt;Restlet client&lt;/a&gt;, etc.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;If you are using Docker, you can use cool API description tools such as &lt;a href="https://swagger.io/tools/swagger-ui/"&gt;Swagger UI&lt;/a&gt; or &lt;a href="https://github.com/Rebilly/ReDoc"&gt;ReDoc&lt;/a&gt;; they can both be easily installed with &lt;code&gt;docker pull&lt;/code&gt; and run with &lt;code&gt;docker run&lt;/code&gt;, making sure to use a port different from 8080 where Membrane is running, and to use the Membrane-generated &lt;code&gt;swagger.json&lt;/code&gt; as the API specification:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ docker pull redocly/redoc
$ docker run -p &lt;span class="m"&gt;8000&lt;/span&gt;:80 -e &lt;span class="nv"&gt;SPEC_URL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;http://localhost:8080/swagger.json redocly/redoc
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to call Membrane from such services, though, &lt;a href="https://en.wikipedia.org/wiki/Cross-origin_resource_sharing#Request_headers"&gt;CORS headers&lt;/a&gt; have to be enabled; I sent a &lt;a href="https://github.com/apache/metamodel-membrane/pull/22"&gt;pull request&lt;/a&gt; to add this capability with the help of a &lt;code&gt;MEMBRANE_ENABLE_CORS&lt;/code&gt; variable, so that the server can be run with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nv"&gt;MEMBRANE_ENABLE_CORS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt; java -server -jar undertow/target/membrane-undertow-server.jar
&lt;/pre&gt;&lt;/div&gt;


&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;Having the server running, we can try out the endpoints. Let's first of all create a tenant:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl -X PUT http://localhost:8080/my-tenant
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We will now add our first data source, for instance a CSV file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl -H &lt;span class="s2"&gt;&amp;quot;Content-Type: application/json&amp;quot;&lt;/span&gt; -X PUT http://localhost:8080/my-tenant/my-csv -d &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="s1"&gt;{&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;type&amp;quot;: &amp;quot;csv&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;resource&amp;quot;: &amp;quot;/tmp/example.csv&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;quote-char&amp;quot;: &amp;quot;\&amp;quot;&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;separator-char&amp;quot;: &amp;quot;,&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;escape-char&amp;quot;: &amp;quot;\\&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;encoding&amp;quot;: &amp;quot;UTF-8&amp;quot;&lt;/span&gt;
&lt;span class="s1"&gt;}&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and we will get a response like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;datasource&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;my-csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;tenant&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;my-tenant&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;updateable&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;query_uri&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/my-tenant/my-csv/query&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;schemas&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;information_schema&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/my-tenant/my-csv/s/information_schema&amp;quot;&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;resources&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;uri&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/my-tenant/my-csv/s/resources&amp;quot;&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which will show some endpoints we can explore. The &lt;code&gt;/my-tenant/my-csv/s/information_schema&lt;/code&gt; endpoint, for instance, shows the structure of a generic information schema; the &lt;code&gt;/my-tenant/my-csv/s/resources&lt;/code&gt; endpoint, instead, shows the tables of our datasource - we can, for instance, get the metadata on the CSV file by exploring the &lt;code&gt;/my-tenant/my-csv/s/resources/t/default_table&lt;/code&gt; endpoint. The most interesting endpoint, though, is probably &lt;code&gt;/my-tenant/my-csv/query&lt;/code&gt;: we can submit a query as a GET parameter and immediately get results. For instance, we can call:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl -G &lt;span class="s2"&gt;&amp;quot;http://localhost:8080/my-tenant/my-csv/query&amp;quot;&lt;/span&gt; --data-urlencode &lt;span class="s2"&gt;&amp;quot;sql=SELECT * FROM default_table&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;to get the whole content of the example CSV file. (Here, &lt;code&gt;-G&lt;/code&gt; and &lt;code&gt;--data-urlencode&lt;/code&gt; are used in order to URL-encode the &lt;code&gt;sql&lt;/code&gt; parameter and still send it as a &lt;code&gt;GET&lt;/code&gt; request.)&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;MetaModel is an easy-to-use and extendable tool that makes it easier to integrate multiple data sources programmatically (even at runtime, with its subproject Membrane). I believe it is worth exploring further since it can simplify what is usually a chore, although most likely it cannot easily scale.&lt;/p&gt;
&lt;p&gt;Just a note of caution: since the project is at a very early stage, there are no strict security measures and the Membrane API could expose information that should not be exposed. Take this into account before releasing anything!&lt;/p&gt;</content><category term="Big Data"></category><category term="library"></category><category term="database"></category></entry><entry><title>Apache Daffodil</title><link href="https://apothem.blog/apache-daffodil.html" rel="alternate"></link><published>2019-04-07T00:00:00+01:00</published><updated>2019-04-07T00:00:00+01:00</updated><author><name>Nicola Vitucci</name></author><id>tag:apothem.blog,2019-04-07:/apache-daffodil.html</id><summary type="html">&lt;p&gt;Let's start the blog with &lt;a href="https://daffodil.apache.org"&gt;Apache Daffodil&lt;/a&gt;. Daffodil presents itself as &lt;em&gt;"an open-source implementation of the Data Format Description Language to convert between fixed format data and XML/JSON"&lt;/em&gt;; basically, by using Daffodil, one should be able to read data saved into an "obscure" format and convert them into an â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Let's start the blog with &lt;a href="https://daffodil.apache.org"&gt;Apache Daffodil&lt;/a&gt;. Daffodil presents itself as &lt;em&gt;"an open-source implementation of the Data Format Description Language to convert between fixed format data and XML/JSON"&lt;/em&gt;; basically, by using Daffodil, one should be able to read data saved into an "obscure" format and convert them into an easy-to-use format, provided that a schema of the original data format is available.&lt;/p&gt;
&lt;p&gt;Daffodil's approach is different from using a data serialization format that includes a schema (e.g. Apache Avro or Protobuf): it is &lt;em&gt;descriptive&lt;/em&gt; rather than &lt;em&gt;prescriptive&lt;/em&gt;, meaning that it allows the data to be interpreted by using only additional information rather than additional software. Daffodil uses DFDL schemas to convert raw data into an abstract model called &lt;em&gt;infoset&lt;/em&gt;, which can be represented in a &lt;a href="https://daffodil.apache.org/infoset/"&gt;variety of formats&lt;/a&gt; including XML and JSON.&lt;/p&gt;
&lt;p&gt;The main tasks that Daffodil can perform are &lt;em&gt;parsing&lt;/em&gt;, &lt;em&gt;unparsing&lt;/em&gt;, and &lt;em&gt;testing&lt;/em&gt; (plus a few more). Parsing is the task of converting data into one or more infosets, while unparsing is the reverse task of converting an infoset into the original format; testing uses the Test Data Markup Language (TDML) to check that parsing and unparsing work correctly, either as single tasks or in a chain. In order to parse or unparse data, we need to use a schema; some publicly available schemas can be found on &lt;a href="https://github.com/DFDLSchemas"&gt;Github&lt;/a&gt;, but most likely a custom schema needs to be developed for a specific dataset.&lt;/p&gt;
&lt;p&gt;Let's see now how the whole machinery works. In the following I will assume that you are using a Linux environment and that you are starting from scratch. You should be able to follow the same steps on MacOS, while if you are using Windows you might need to perform some of the steps manually or to write some additional code (for instance, some Java or Python code to preprocess the files); as an alternative, you can just use the &lt;a href="https://github.com/nvitucci/apothem-resources"&gt;associated repo&lt;/a&gt; where you will find all the files you need.&lt;/p&gt;
&lt;h3&gt;Example data&lt;/h3&gt;
&lt;p&gt;Since I am a sort of astronomy geek, I used the &lt;a href="http://tdc-www.harvard.edu/catalogs/bsc5.html"&gt;Bright Star Catalogue&lt;/a&gt; as an example data source. The BSC is a catalog of more than 9,000 objects visible to the naked eye, delivered as a compressed fixed-length format file with many fields of different types (see the section &lt;em&gt;"Byte-by-byte Description of file: catalog"&lt;/em&gt; of the &lt;a href="http://tdc-www.harvard.edu/catalogs/bsc5.readme"&gt;bsc5.readme&lt;/a&gt; file). Since it does not use separators, we cannot parse it as a CSV-like file; we need to parse it in a different way. Let's &lt;a href="http://tdc-www.harvard.edu/catalogs/bsc5.dat.gz"&gt;download&lt;/a&gt; and decompress it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ wget -q http://tdc-www.harvard.edu/catalogs/bsc5.dat.gz
$ gunzip bsc5.dat.gz
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can check how the file looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ head bsc5.dat
   1          BD+44 4550      3 36042          46           000001.1+444022000509.9+451345114.44-16.88 6.70  +0.07 +0.08         A1Vn               -0.012-0.018      -018      195  4.2  21.6AC   3
   2          BD-01 4525      6128569                       235956.2-010330000503.8-003011 98.33-61.14 6.29  +1.10 +1.02        gG9                 +0.045-0.060      +014V
   3 33    PscBD-06 6357     281285721002I         Var?     000013.0-061601000520.1-054227 93.75-65.93 4.61  +1.04 +0.89 +0.54   K0IIIbCN-0.5       -0.009+0.089 +.014-006SB1O &amp;lt; 17  2.5   0.0     3*
   4 86    PegBD+12 5063     87 917012004                   000033.8+125023000542.0+132346106.19-47.98 5.51  +0.90               G5III              +0.045-0.012      -002V?
   5          BD+57 2865    123 21085          61  V640 Cas 000101.8+575245000616.0+582612117.03-03.92 5.96  +0.67 +0.20         G5V                +0.263+0.030 +.047-012V          0.8   1.4      *
   6          CD-4914337    142214963      W                000108.4-493751000619.0-490430321.61-66.38 5.70  +0.52 +0.05         G1IV               +0.565-0.038 +.050+003SB         5.7   5.4      *
   7 10    CasBD+63 2107    144 109782005                   000114.4+633822000626.5+641146118.06  1.75 5.59  -0.03 -0.19         B9III             e+0.008 0.000      -000V     153                 *
   8          BD+28 4704    166 73743          69     33    000125.2+282811000636.8+290117111.26-32.83 6.13  +0.75 +0.33         K0V                +0.380-0.182 +.067-008V          2.6 158.6AB   4*
   9          CD-23    4    2031660531003                   000143.0-233947000650.1-230627 52.21-79.14 6.18  +0.38 +0.05         A7V                +0.100-0.045      +003V
  10          BD-18 6428    256147090                       000211.8-175639000718.2-172311 74.36-75.90 6.19  +0.14 +0.10         A6Vn               -0.018+0.036      -009V?    195                 *
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A deeper inspection reveals that the file is not correctly formatted because not all the rows have the same length. In order to make writing a working DFDL schema easier, let's fix this by padding all the rows to a length of 197 (the sum of the lengths of all the fields as described in the &lt;code&gt;bsc5.readme&lt;/code&gt; file) and creating a new file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ awk &lt;span class="s1"&gt;&amp;#39;{printf &amp;quot;%-197s\n&amp;quot;, $0}&amp;#39;&lt;/span&gt; bsc5.dat &amp;gt; bsc5_padded.dat
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's extract the first two lines into a sample file that we will use in the examples:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ head -n &lt;span class="m"&gt;2&lt;/span&gt; bsc5_padded.dat &amp;gt; bsc5_padded_sample.dat
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we can continue with the DFDL schema definition.&lt;/p&gt;
&lt;h3&gt;DFDL schema&lt;/h3&gt;
&lt;p&gt;A basic DFDL schema should look like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;xs:schema&lt;/span&gt; &lt;span class="na"&gt;xmlns:xs=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.w3.org/2001/XMLSchema&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;xmlns:fn=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.w3.org/2005/xpath-functions&amp;quot;&lt;/span&gt;
  &lt;span class="na"&gt;xmlns:dfdl=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.ogf.org/dfdl/dfdl-1.0/&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;xmlns:ex=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://example.com&amp;quot;&lt;/span&gt;
  &lt;span class="na"&gt;targetNamespace=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://example.com&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;elementFormDefault=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;unqualified&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;xs:include&lt;/span&gt; &lt;span class="na"&gt;schemaLocation=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;org/apache/daffodil/xsd/DFDLGeneralFormat.dfdl.xsd&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;xs:annotation&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;xs:appinfo&lt;/span&gt; &lt;span class="na"&gt;source=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.ogf.org/dfdl/&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;dfdl:format&lt;/span&gt; &lt;span class="na"&gt;ref=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ex:GeneralFormat&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/xs:appinfo&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/xs:annotation&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;record&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xs:string&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;explicit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:length=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;197&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthUnits=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;characters&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/xs:schema&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here we are declaring:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;an XML schema with a custom namespace &lt;code&gt;http://example.com&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;the inclusion of a &lt;code&gt;DFDLGeneralFormat.dfdl.xsd&lt;/code&gt; XML schema;&lt;/li&gt;
&lt;li&gt;an annotation that DFDL will use to determine some default values for the schema properties;&lt;/li&gt;
&lt;li&gt;an element that represents the data being interpreted, along with its type and some DFDL properties.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's create a &lt;code&gt;v1&lt;/code&gt; folder and write this XML content into &lt;code&gt;v1/bsc.dfdl.xsd&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Parsing&lt;/h3&gt;
&lt;p&gt;In order to try and parse our sample data using this schema, we need to download the Apache Daffodil binaries from the &lt;a href="https://daffodil.apache.org/releases/"&gt;Daffodil releases page&lt;/a&gt;; at the time of writing, the most recent version is the &lt;a href="https://www.apache.org/dyn/closer.lua/incubator/daffodil/2.3.0/bin/apache-daffodil-2.3.0-incubating-bin.tgz"&gt;2.3.0&lt;/a&gt;. Let's decompress the .tgz file into our current folder and run the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ apache-daffodil-2.3.0-incubating-bin/bin/daffodil parse -s v1/bsc.dfdl.xsd bsc5_padded_sample.dat
&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot; ?&amp;gt;
&amp;lt;ex:record xmlns:ex=&amp;quot;http://example.com&amp;quot;&amp;gt;   1          BD+44 4550      3 36042          46           000001.1+444022000509.9+451345114.44-16.88 6.70  +0.07 +0.08         A1Vn               -0.012-0.018      -018      195  4.2  21.6AC   3 &amp;lt;/ex:record&amp;gt;
[warning] Left over data. Consumed 1576 bit(s) with at least 1592 bit(s) remaining.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is our first Daffodil infoset! Yes, we do get a warning, but we start to see where we are headed. Now let's take a closer look at the &lt;code&gt;xs:element&lt;/code&gt; we have declared in the schema:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;record&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xs:string&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;explicit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:length=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;197&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthUnits=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;characters&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;its name is &lt;code&gt;record&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;it is a &lt;code&gt;xs:string&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;it has a predefined length (&lt;code&gt;dfdl:lengthKind="explicit"&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;the length is 197 units (&lt;code&gt;dfdl:length="197"&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;the length is measured in characters (&lt;code&gt;dfdl:lengthUnits="characters"&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The parser correctly read the first 197 characters of our sample file, then it stopped. We need to specify that the file actually contains a &lt;em&gt;sequence&lt;/em&gt; of lines &lt;em&gt;separated&lt;/em&gt; by a newline, so we replace the &lt;code&gt;record&lt;/code&gt; definition with the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;...
&lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;file&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;xs:complexType&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;xs:sequence&lt;/span&gt; &lt;span class="na"&gt;dfdl:separator=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;%NL;&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:separatorPosition=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;postfix&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;record&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xs:string&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;explicit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:length=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;197&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthUnits=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;characters&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;maxOccurs=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;unbounded&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/xs:sequence&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/xs:complexType&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/xs:element&amp;gt;&lt;/span&gt;
...
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We wrapped the previous &lt;code&gt;record&lt;/code&gt; within a &lt;code&gt;xs:sequence&lt;/code&gt; and added a &lt;code&gt;maxOccurs="unbounded"&lt;/code&gt; property to declare that it can occur an unlimited number of times; the sequence is parsed by using newlines as postfix record separators and is part of a parent &lt;code&gt;xs:element&lt;/code&gt; called &lt;code&gt;file&lt;/code&gt;. Let's create a &lt;code&gt;bsc.dfdl.xsd&lt;/code&gt; file with the updated element in a &lt;code&gt;v2&lt;/code&gt; folder and run the parser again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ apache-daffodil-2.3.0-incubating-bin/bin/daffodil parse -s v2/bsc.dfdl.xsd bsc5_padded_sample.dat
&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot; ?&amp;gt;
&amp;lt;ex:file xmlns:ex=&amp;quot;http://example.com&amp;quot;&amp;gt;
  &amp;lt;record&amp;gt;   1          BD+44 4550      3 36042          46           000001.1+444022000509.9+451345114.44-16.88 6.70  +0.07 +0.08         A1Vn               -0.012-0.018      -018      195  4.2  21.6AC   3 &amp;lt;/record&amp;gt;
  &amp;lt;record&amp;gt;   2          BD-01 4525      6128569                       235956.2-010330000503.8-003011 98.33-61.14 6.29  +1.10 +1.02        gG9                 +0.045-0.060      +014V                          &amp;lt;/record&amp;gt;
&amp;lt;/ex:file&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Great, no more leftover bits! The records are not very informative though, since we haven't described the fields. We need to extend the definition further by stating that the &lt;code&gt;record&lt;/code&gt; is itself a sequence of fixed-length fields, each with its own name:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;...
&lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;file&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;xs:complexType&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;xs:sequence&lt;/span&gt; &lt;span class="na"&gt;dfdl:separator=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;%NL;&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:separatorPosition=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;postfix&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;record&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;maxOccurs=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;unbounded&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;xs:complexType&amp;gt;&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;lt;xs:sequence&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;HR&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xs:string&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;explicit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:length=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;4&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthUnits=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;characters&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Name&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xs:string&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;explicit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:length=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;10&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthUnits=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;characters&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;DM&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xs:string&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;explicit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:length=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;11&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthUnits=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;characters&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;HD&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xs:string&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;explicit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:length=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;6&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthUnits=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;characters&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
            ...
          &lt;span class="nt"&gt;&amp;lt;/xs:sequence&amp;gt;&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;lt;/xs:complexType&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;/xs:element&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/xs:sequence&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/xs:complexType&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/xs:element&amp;gt;&lt;/span&gt;
...
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The updated version can be found in the &lt;code&gt;bsc.dfdl.xsd&lt;/code&gt; file in the &lt;code&gt;v3&lt;/code&gt; folder. When we run the parser on the new file we get this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot; ?&amp;gt;
&amp;lt;ex:file xmlns:ex=&amp;quot;http://example.com&amp;quot;&amp;gt;
  &amp;lt;record&amp;gt;
    &amp;lt;HR&amp;gt;   1&amp;lt;/HR&amp;gt;
    &amp;lt;Name&amp;gt;          &amp;lt;/Name&amp;gt;
    &amp;lt;DM&amp;gt;BD+44 4550 &amp;lt;/DM&amp;gt;
    &amp;lt;HD&amp;gt;     3&amp;lt;/HD&amp;gt;
    ...
  &amp;lt;/record&amp;gt;
  &amp;lt;record&amp;gt;
    &amp;lt;HR&amp;gt;   2&amp;lt;/HR&amp;gt;
    &amp;lt;Name&amp;gt;          &amp;lt;/Name&amp;gt;
    &amp;lt;DM&amp;gt;BD-01 4525 &amp;lt;/DM&amp;gt;
    &amp;lt;HD&amp;gt;     6&amp;lt;/HD&amp;gt;
    ...
  &amp;lt;/record&amp;gt;
&amp;lt;/ex:file&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's run the parser again, now saving the result into &lt;code&gt;v3/bsc5_padded_sample.xml&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ apache-daffodil-2.3.0-incubating-bin/bin/daffodil parse -s v3/bsc.dfdl.xsd bsc5_padded_sample.dat &amp;gt; v3/bsc5_padded_sample.xml
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are now ready to try Daffodil's unparsing capabilities.&lt;/p&gt;
&lt;h3&gt;Unparsing&lt;/h3&gt;
&lt;p&gt;Let's run the unparser on the infoset:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ apache-daffodil-2.3.0-incubating-bin/bin/daffodil unparse -s v3/bsc.dfdl.xsd v3/bsc5_padded_sample.xml
   1          BD+44 4550      3 36042          46           000001.1+444022000509.9+451345114.44-16.88 6.70  +0.07 +0.08         A1Vn               -0.012-0.018      -018      195  4.2  21.6AC   3 
   2          BD-01 4525      6128569                       235956.2-010330000503.8-003011 98.33-61.14 6.29  +1.10 +1.02        gG9                 +0.045-0.060      +014V
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The output looks very similar to our original &lt;code&gt;bsc5_padded_sample.dat&lt;/code&gt; file. Let's compare the two just to make sure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ apache-daffodil-2.3.0-incubating-bin/bin/daffodil unparse -s v3/bsc.dfdl.xsd v3/bsc5_padded_sample.xml &amp;gt; v3/bsc5_padded_sample_unparsed.dat
$ diff -s bsc5_padded_sample.dat v3/bsc5_padded_sample_unparsed.dat
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;diff&lt;/code&gt; command reports that the two files are identical, which means that we successfully reconstructed the source file from the derived infoset; now, if we add a new record using the schema we created and then we unparse the infoset, we will get as an output an updated file in the original format. This doesn't mean, of course, that the updated file will be valid; we have been pretty shallow in defining the schema, so all we can be sure about is that the new file will have the same structure as the original file.&lt;/p&gt;
&lt;h3&gt;Testing&lt;/h3&gt;
&lt;p&gt;Daffodil provides a way to check that not only parsing and unparsing work as separate tasks, but also that a sequence of parsing and unparsing leave the original file unchanged; this is done by defining test cases using the Test Data Markup Language (&lt;a href="https://daffodil.apache.org/tdml/"&gt;TDML&lt;/a&gt;). In order to replicate the &lt;code&gt;diff&lt;/code&gt; test, we can create the &lt;code&gt;bsc.tdml&lt;/code&gt; file in the &lt;code&gt;v3&lt;/code&gt; folder as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cp"&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;testSuite&lt;/span&gt; &lt;span class="na"&gt;suiteName=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Namespaces&amp;quot;&lt;/span&gt;
  &lt;span class="na"&gt;xmlns:dfdl=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.ogf.org/dfdl/dfdl-1.0/&amp;quot;&lt;/span&gt;
  &lt;span class="na"&gt;xmlns:tdml=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.ibm.com/xmlns/dfdl/testData&amp;quot;&lt;/span&gt;
  &lt;span class="na"&gt;defaultRoundTrip=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;onePass&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;lt;tdml:parserTestCase&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;bsc_test&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;root=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;file&amp;quot;&lt;/span&gt;
    &lt;span class="na"&gt;model=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;bsc.dfdl.xsd&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;description=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Test parsing and unparsing&amp;quot;&lt;/span&gt;
    &lt;span class="na"&gt;roundTrip=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;onePass&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;tdml:document&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;tdml:documentPart&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;file&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;../bsc5_padded_sample.dat&lt;span class="nt"&gt;&amp;lt;/tdml:documentPart&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/tdml:document&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;tdml:infoset&amp;gt;&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;lt;tdml:dfdlInfoset&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;file&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;bsc5_padded_sample.xml&lt;span class="nt"&gt;&amp;lt;/tdml:dfdlInfoset&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;/tdml:infoset&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;/tdml:parserTestCase&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/testSuite&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, we run the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ apache-daffodil-2.3.0-incubating-bin/bin/daffodil test v3/bsc.tdml
Creating DFDL Test Suite for v3/bsc.tdml
[Pass] bsc_test

Total: 1, Pass: 1, Fail: 0, Not Found: 0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can see that the "one-pass" chain (i.e. a chain of parsing the data, unparsing the result, and comparing the original data with the unparsed data) is successful, although we have "cheated" a little since we created the test infoset via a parsing task rather than manually. Anyway, this is a useful and quick way to test that the schema we are developing actually behaves as we expect in both directions.&lt;/p&gt;
&lt;h3&gt;Improving the schema&lt;/h3&gt;
&lt;p&gt;At this point we realize that we converted our fields to strings, but we haven't really added much. This may be all we need to feed the data into a pipeline for further cleaning and processing, but we can actually make the schema more useful. We could start with trimming the strings so that they can be used straight away; we can add some properties to the &lt;code&gt;dfdl:format&lt;/code&gt; element:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;dfdl:format&lt;/span&gt; &lt;span class="na"&gt;ref=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ex:GeneralFormat&amp;quot;&lt;/span&gt;
  &lt;span class="na"&gt;textStringPadCharacter=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;%SP;&amp;quot;&lt;/span&gt;
  &lt;span class="na"&gt;textPadKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;padChar&amp;quot;&lt;/span&gt;
  &lt;span class="na"&gt;textTrimKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;padChar&amp;quot;&lt;/span&gt;
  &lt;span class="na"&gt;textStringJustification=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;center&amp;quot;&lt;/span&gt;
&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here we are basically using space (&lt;code&gt;"%SP;"&lt;/code&gt;) as a character to pad with (while unparsing) or to trim (while parsing), and we pad/trim both sides of a string. If we try and parse again, we will get this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot; ?&amp;gt;
&amp;lt;ex:file xmlns:ex=&amp;quot;http://example.com&amp;quot;&amp;gt;
  &amp;lt;record&amp;gt;
    &amp;lt;HR&amp;gt;1&amp;lt;/HR&amp;gt;
    &amp;lt;Name&amp;gt;&amp;lt;/Name&amp;gt;
    &amp;lt;DM&amp;gt;BD+44 4550&amp;lt;/DM&amp;gt;
    &amp;lt;HD&amp;gt;3&amp;lt;/HD&amp;gt;
    ...
  &amp;lt;/record&amp;gt;
  &amp;lt;record&amp;gt;
    &amp;lt;HR&amp;gt;2&amp;lt;/HR&amp;gt;
    &amp;lt;Name&amp;gt;&amp;lt;/Name&amp;gt;
    &amp;lt;DM&amp;gt;BD-01 4525&amp;lt;/DM&amp;gt;
    &amp;lt;HD&amp;gt;6&amp;lt;/HD&amp;gt;
    ...
  &amp;lt;/record&amp;gt;
&amp;lt;/ex:file&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Anyway, if we save the result of the parsing to &lt;code&gt;bsc5_padded_sample.xml&lt;/code&gt; in the &lt;code&gt;v4&lt;/code&gt; folder and then define a new test &lt;code&gt;bsc.tdml&lt;/code&gt; using such result as the &lt;code&gt;tdml:dfdlInfoset&lt;/code&gt;, the test will fail. The reason of the difference between the original file and the unparsed file is that some fields, after being trimmed, could not be padded back correctly; we can see this more clearly by saving the result of the unparsing task to &lt;code&gt;v4/bsc5_padded_sample_unparsed.dat&lt;/code&gt; and comparing it to the original &lt;code&gt;bsc5_padded_sample.dat&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;In short, if we want to extend the schema and keep making use of the unparsing capability, we need to take extra care. It is better to start by extending the definition of one or more fields and checking the results after the change; for instance, if we want to interpret a field as a floating-point number and still be able to write the field back into its original format, we need to make sure that there are no padding, conversion, or precision errors.&lt;/p&gt;
&lt;p&gt;Let's say that we want to interpret the field &lt;code&gt;HD&lt;/code&gt; as an integer (example in the &lt;code&gt;v5&lt;/code&gt; folder); then, we need to change its declaration from this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;HD&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xs:string&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;explicit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:length=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;6&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthUnits=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;characters&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;to this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;HD&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xs:int&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;explicit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:length=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;6&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthUnits=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;characters&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:textPadKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;padChar&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:textTrimKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;padChar&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:textNumberPadCharacter=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;%SP;&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:textNumberJustification=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;right&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:textNumberPattern=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;#&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Although the &lt;code&gt;dfdl:textNumberPadCharacter&lt;/code&gt; and &lt;code&gt;dfdl:textNumberJustification&lt;/code&gt; are redundant, since they have the same default value in the &lt;a href="https://github.com/apache/incubator-daffodil/blob/master/daffodil-lib/src/main/resources/org/apache/daffodil/xsd/DFDLGeneralFormat.dfdl.xsd"&gt;DFDL General Format schema&lt;/a&gt;, it is clear that just changing the type from &lt;code&gt;xs:string&lt;/code&gt; to &lt;code&gt;xs:int&lt;/code&gt; is not enough. The important properties here are &lt;code&gt;dfdl:textPadKind&lt;/code&gt; and &lt;code&gt;dfdl:textTrimKind&lt;/code&gt; (since they are needed to instruct the parser and the unparser to actually use the pad character), and &lt;code&gt;dfdl:textNumberPattern&lt;/code&gt; (since otherwise the default pattern would include a group separator and convert a number string such as "123456" to "123,456"). The test in the &lt;code&gt;v5&lt;/code&gt; folder includes these changes, so it will run successfully.&lt;/p&gt;
&lt;p&gt;If you try and run the parser on the full dataset using the schema defined in the &lt;code&gt;v5&lt;/code&gt; folder, you will get a warning about leftover data. The reason is that not all records contain a value for the &lt;code&gt;HD&lt;/code&gt; field that can be parsed as an integer; we should then allow the &lt;code&gt;HD&lt;/code&gt; element to be &lt;em&gt;nillable&lt;/em&gt; by adding the following attributes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;nillable=&amp;quot;true&amp;quot; dfdl:nilKind=&amp;quot;literalValue&amp;quot; dfdl:nilValue=&amp;quot;%ES;&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this way, when the trimmed value equals an empty string, the element is considered null and no conversion is attempted. You can find the updated schema in the &lt;code&gt;v6&lt;/code&gt; folder.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; I extended the schema further by adding the same &lt;code&gt;xs:int&lt;/code&gt; pattern to other elements, namely &lt;code&gt;HR&lt;/code&gt;, &lt;code&gt;SAO&lt;/code&gt;, and &lt;code&gt;FK5&lt;/code&gt;. I also took the chance to experiment with the declaration of a new type, and things got interesting because of the peculiar behaviour I wanted to replicate.&lt;/p&gt;
&lt;p&gt;The new fields to parse/unparse represent the &lt;a href="https://en.wikipedia.org/wiki/Right_ascension"&gt;right ascension&lt;/a&gt; (RA) and the &lt;a href="https://en.wikipedia.org/wiki/Declination"&gt;declination&lt;/a&gt; (DE) in the &lt;em&gt;hours/minutes/seconds&lt;/em&gt; and &lt;em&gt;degrees/arcminutes/arcseconds&lt;/em&gt; formats respectively; if we exclude RA seconds, all the other fields encode integers as zero-padded two-digit strings. The interesting part is that these fields can be empty, so we cannot use the previous pattern: if we were just to trim the "0" character we would get a null value when the field is "00", which is actually a legitimate value. The solution is to skip the trimming part and designate a double space as the "null value", since the length of the empty field is still two. We can do this by using the property &lt;code&gt;dfdl:nilValue="%WSP;%WSP;"&lt;/code&gt;, so that the new type looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;xs:simpleType&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;twoDigitIntOrNull&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;explicit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:length=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthUnits=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;characters&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:textPadKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;padChar&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:textNumberPadCharacter=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;0&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:nilKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;literalValue&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:nilValue=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;%WSP;%WSP;&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;xs:restriction&lt;/span&gt; &lt;span class="na"&gt;base=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xs:int&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/xs:simpleType&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After defining this new type, the fields can be defined as in this example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;RAh1900&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ex:twoDigitIntOrNull&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;nillable=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;true&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For the sake of completeness the combination &lt;code&gt;dfdl:textPadKind="padChar" dfdl:textNumberPadCharacter="0"&lt;/code&gt; in this case can be replaced by &lt;code&gt;dfdl:textNumberPattern="00" dfdl:textStandardZeroRep="00"&lt;/code&gt;, so that a value of zero can still be correctly unparsed; this is not always necessarily the case, as we can see with the RA &lt;em&gt;seconds&lt;/em&gt; field that we have skipped. The field encodes a decimal number as a string with two digits to the left of the decimal point and one digit to the right, so if we were to use the same approach the zero value would be unparsed to "0000"; we can do this instead:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;xs:simpleType&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fourDigitDoubleOrNull&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;explicit&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:length=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;4&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:lengthUnits=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;characters&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:textNumberPattern=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;00.0&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:textStandardZeroRep=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;00.0&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:nilKind=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;literalValue&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;dfdl:nilValue=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;%WSP;%WSP;%WSP;%WSP;&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;xs:restriction&lt;/span&gt; &lt;span class="na"&gt;base=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;xs:double&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/xs:simpleType&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and define the RA seconds fields as in this example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;xs:element&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;RAs1900&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;type=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ex:fourDigitDoubleOrNull&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;nillable=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;true&amp;quot;&lt;/span&gt; &lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can find the updated schema in the &lt;code&gt;v7&lt;/code&gt; folder of the associated repository.&lt;/p&gt;
&lt;h3&gt;Using a different infoset representation&lt;/h3&gt;
&lt;p&gt;So far we have parsed the data into XML, but what if we prefer JSON instead? We just need to add a &lt;code&gt;-I json&lt;/code&gt; in the parse/unparse command as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ apache-daffodil-2.3.0-incubating-bin/bin/daffodil parse -s v3/bsc.dfdl.xsd -I json bsc5_padded_sample.dat
{
  &amp;quot;file&amp;quot;: {
    &amp;quot;record&amp;quot;: [
      {
        &amp;quot;HR&amp;quot;: &amp;quot;   1&amp;quot;,
        &amp;quot;Name&amp;quot;: &amp;quot;          &amp;quot;,
        &amp;quot;DM&amp;quot;: &amp;quot;BD+44 4550 &amp;quot;,
        &amp;quot;HD&amp;quot;: &amp;quot;     3&amp;quot;,
        ...
      },
      {
        &amp;quot;HR&amp;quot;: &amp;quot;   2&amp;quot;,
        &amp;quot;Name&amp;quot;: &amp;quot;          &amp;quot;,
        &amp;quot;DM&amp;quot;: &amp;quot;BD-01 4525 &amp;quot;,
        &amp;quot;HD&amp;quot;: &amp;quot;     6&amp;quot;,
        ...
      }
    ]
  }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can save the output to a .json file and process it further. JSON is not the only alternative: &lt;a href="https://daffodil.apache.org/infoset/"&gt;other representations&lt;/a&gt; are available as well.&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;We have touched on the main advantages of using Apache Daffodil and, without going too much into the details of DFDL, we created a schema to successfully parse and unparse scientific data. We only used the standalone binaries, but Daffodil is available also as a Java/Scala library and as an &lt;a href="https://nifi.apache.org/"&gt;Apache Nifi&lt;/a&gt; processor.&lt;/p&gt;
&lt;p&gt;One thing to be aware of is the size of the source data: the binaries by default load the whole dataset in memory before starting to process it, which might result in a huge memory footprint or heap space errors. A possible solution in this case would be to split the source file into multiple files, which might be easier when the source is in a text format and harder if it is in a binary format; if this is not practical, it might be worth looking into the streaming capabilities of the Daffodil API.&lt;/p&gt;</content><category term="Big Data"></category><category term="library"></category><category term="XML"></category><category term="JSON"></category></entry></feed>