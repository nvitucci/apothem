<!doctype html>
<html lang="en" prefix="og: http://ogp.me/ns#">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  Apache Atlas (part 2) | APOTHEM
</title>
  <link rel="canonical" href="https://apothem.blog/apache-atlas-part-2.html">

    <link rel="icon" type="image/x-icon" href="https://apothem.blog/favicon.ico">

  <link rel="stylesheet" href="https://apothem.blog/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://apothem.blog/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://apothem.blog/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://apothem.blog/theme/css/theme.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="https://apothem.blog/feeds/all.atom.xml">
  <link rel="alternate" type="application/atom+xml" title="Categories Atom Feed"
        href="https://apothem.blog/feeds/{slug}.atom.xml">  
  <meta name="description" content="Since Atlas is a fairly large and complex project, one article was definitely not enough to explore all of its capabilities. Building on the previous article, we will explore classifications and glossary, the REST API, and two more sources of lineage information (Spark and Kafka). Classification Let's start with classification â€¦">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
    <div class="col-sm-4">
      <a href="https://apothem.blog/">
        <img class="img-fluid rounded" src=https://apothem.blog/images/profile.svg width=180 alt="APOTHEM">
      </a>
    </div>
  <div class="col-sm-8">
    <h1 class="title"><a href="https://apothem.blog/">APOTHEM</a></h1>
      <p class="text-muted">Apache Project(s) of the month</p>
      <ul class="list-inline">
          <li class="list-inline-item"><a href="https://projects.apache.org" target="_blank">projects.apache.org</a></li>
              <li class="list-inline-item text-muted">|</li>
            <li class="list-inline-item"><a href="https://apothem.blog/pages/about.html">About</a></li>
            <li class="list-inline-item"><a href="https://apothem.blog/pages/faq.html">FAQ</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fa fa-github" href="https://github.com/nvitucci/apothem" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fa fa-twitter" href="https://twitter.com/nvitucci" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fa fa-rss" href="feeds/all.atom.xml" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h2>  Apache Atlas (part 2)
</h2>
      <hr>
  <article class="article">
    <header>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2019-07-31T00:00:00+01:00">
          <i class="fa fa-clock-o"></i>
          Wed 31 July 2019
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="https://apothem.blog/category/projects.html">projects</a>
        </li>
          <li class="list-inline-item">
            <i class="fa fa-files-o"></i>
              <a href="https://apothem.blog/tag/big-data.html">#Big Data</a>          </li>
      </ul>
    </header>
    <div class="content">
      <p>Since Atlas is a fairly large and complex project, one article was definitely not enough to explore all of its capabilities. Building on the <a href="https://apothem.blog/apache-atlas.html">previous article</a>, we will explore classifications and glossary, the REST API, and two more sources of lineage information (Spark and Kafka).</p>
<h2 id="classification">Classification</h2>
<p>Let's start with classification. In our previous example we created some Hive tables containing information about people and their documents, and now we'll put ourselves in the shoes of a data steward: How can we describe the data contained in a table, or even in a single column? How can we explain that some data should not be visible to everybody, or that their value is higher, or that they are covered by a SLA?</p>
<p>Let's take the <code>people</code> table we created before, which looks like the following:</p>
<div class="highlight"><pre><span></span><span class="o">|</span> <span class="n">name</span> <span class="o">|</span> <span class="n">surname</span> <span class="o">|</span> <span class="n">age</span> <span class="o">|</span> <span class="n">person_id</span> <span class="o">|</span>
<span class="o">|</span><span class="c1">------|---------|-----|-----------|</span>
<span class="o">|</span> <span class="n">Jane</span> <span class="o">|</span>     <span class="n">Doe</span> <span class="o">|</span>  <span class="mi">22</span> <span class="o">|</span>         <span class="mi">3</span> <span class="o">|</span>
<span class="o">|</span> <span class="n">John</span> <span class="o">|</span>   <span class="n">Smith</span> <span class="o">|</span>  <span class="mi">30</span> <span class="o">|</span>         <span class="mi">1</span> <span class="o">|</span>
<span class="p">...</span>
</pre></div>


<p>We might want to say that this table contains <a href="https://en.wikipedia.org/wiki/Personal_data">personally identifying information (PII)</a>, so that other teams can create policies to control the access to such information. In Atlas, doing this is a matter of creating the classification itself:</p>
<p><img src="images/atlas_classification.png" alt="Atlas classification" class="img-fluid" /></p>
<p><img src="images/atlas_classification_create.png" alt="Atlas classification" class="img-fluid" /></p>
<p>and assigning it to the items we want to classify:</p>
<p><img src="images/atlas_classification_before.png" alt="Atlas classification" class="img-fluid" /></p>
<p><img src="images/atlas_classification_add.png" alt="Atlas classification" class="img-fluid" /></p>
<p>An interesting thing we will see is that this classification automatically extends to the <code>joined</code> table, and that is where Atlas really shines: the classification is propagated thanks to the lineage information that relates all these tables.</p>
<p><img src="images/atlas_classification_after.png" alt="Atlas classification" class="img-fluid" /></p>
<p>Let's create another classification, <code>External</code>, with a <code>Group</code> string attribute:</p>
<p><img src="images/atlas_classification_attr.png" alt="Atlas classification" class="img-fluid" /></p>
<p>This classification is meant to capture the provenance of a certain dataset, and for this reason it is not to be automatically propagated through the lineage (if a certain group is responsible for a dataset, it is not necessarily responsible for derived data). We can prevent the propagation of a classification by clicking on the "Propagate" checkbox when assigning the classification to an item:</p>
<p><img src="images/atlas_classification_noprop.png" alt="Atlas classification" class="img-fluid" /></p>
<h2 id="glossary">Glossary</h2>
<p>We have seen how to classify the metadata and how the classifications take advantage of lineage information. What if we want to attach tags describing data and metadata according to a specific terminology? In pretty much the same way we have created classifications, we can create one or more <em>glossaries</em> to group together <em>terms</em> and <em>categories</em>.</p>
<p>In the "Glossary" section we need first of all to create a glossary, for instance <code>Identification</code>:</p>
<p><img src="images/atlas_glossary_add.png" alt="Atlas glossary" class="img-fluid" /></p>
<p>Then, we create a category by clicking on the "Terms/Category" selector and selecting "Create category" from the glossary contextual menu:</p>
<p><img src="images/atlas_glossary_cat.png" alt="Atlas glossary" class="img-fluid" /></p>
<p>Now, in the same way, we switch to "Terms" and create the two terms <code>National ID card</code> and <code>Passport</code>. If we click on any of the terms, we will see the term's detail page where we can assign classifications and categories. We can even create relationships between terms, for instance to say that a <code>National ID card</code> is somewhat related to a <code>Passport</code> via a <code>seeAlso</code> relation. After this, we can finally assign the terms to any metadata item, for instance to the <code>document</code> table, by clicking on the plus sign under the "Term" column in the search view:</p>
<p><img src="images/atlas_terms_assign.png" alt="Atlas glossary" class="img-fluid" /></p>
<p>For further information, the <a href="https://atlas.apache.org/#/Glossary">Glossary page</a> on the Atlas website is very detailed and shows some useful examples.</p>
<h2 id="the-rest-api">The REST API</h2>
<p>Another very strong feature that Atlas offers is its rich REST API. Basically, everything that can be done via the UI can be done via the API as well. A <a href="https://atlas.apache.org/2.0.0/api/v2/ui/index.html">Swagger interactive interface</a> is available on the website for easier exploration; here we will touch on the main sections with a couple examples.</p>
<p>As usual, the following API calls can be performed with any client (curl, Postman, etc.). In the following, every call is run via <code>curl</code> with JSON as the response content type and with the correct access credentials (which, as a default, are <code>admin / admin</code>); in other words, an API call such as <code>/api/atlas/v2/types/typedefs</code> is performed with:</p>
<div class="highlight"><pre><span></span>$ curl -u admin:admin -H <span class="s2">&quot;Accept: application/json&quot;</span> http://localhost:21000/api/atlas/v2/types/typedefs
</pre></div>


<p>If the client is based on the command line, a tool such as <code>json_pp</code> is recommended in order to pretty-print the JSON response.</p>
<h4 id="types">Types</h4>
<p>To list all entity types:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/types/typedefs
</pre></div>


<p>Description of a single type by type name (e.g. <code>hive_table</code>):</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/types/typedef/name/hive_table
</pre></div>


<h4 id="search">Search</h4>
<p>Metadata entities can be searched in multiple ways. The basic search endpoint has to be used with at least one parameter, for instance the type name:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/search/basic?typeName=hive_table
</pre></div>


<p>or a full-text query:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/search/basic?typeName=hive_table&amp;query=people
</pre></div>


<p>or a classification:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/search/basic?classification=PII
</pre></div>


<p>If we had deleted any entities (e.g. if we dropped any Hive tables), we would see a few entities with a field <code>"status": "DELETED"</code>; in order to exclude such entities we should add the <code>excludeDeletedEntities=true</code> parameter:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/search/basic?typeName=hive_table&amp;query=people&amp;excludeDeletedEntities=true
</pre></div>


<p>An advanced search endpoint using Atlas DSL is available as well, but we will not cover it here. Further information on the DSL can be found <a href="https://atlas.apache.org/#/SearchAdvance">here</a>.</p>
<h4 id="single-entity">Single entity</h4>
<p>The previous endpoints returned a (possibly empty) list of entities as a result. If we want to explore a single entity, we have to extract its <code>guid</code> from any of the previous calls. Let's say we are interested in the entity with <code>"guid": "ad9915a8-fdab-4570-964f-8f636a8da20c"</code>; a rich description of the entity can be obtained by calling this endpoint:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/entity/guid/ad9915a8-fdab-4570-964f-8f636a8da20c
</pre></div>


<p>We might be interested in the entity's classifications only:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/entity/guid/ad9915a8-fdab-4570-964f-8f636a8da20c/classifications
</pre></div>


<p>or in its lineage:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/lineage/ad9915a8-fdab-4570-964f-8f636a8da20c
</pre></div>


<p>Since the lineage is a graph, the <code>relations</code> object will contain a list of all the relationships where each relationship is an edge connecting an entity to another:</p>
<div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="nt">&quot;relations&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="p">{</span>
      <span class="nt">&quot;fromEntityId&quot;</span> <span class="p">:</span> <span class="s2">&quot;edefa7cb-c19d-4955-9da5-22e43786ced5&quot;</span><span class="p">,</span>
      <span class="nt">&quot;relationshipId&quot;</span> <span class="p">:</span> <span class="s2">&quot;33591207-ac0d-4374-91a2-5fadd71d6f0c&quot;</span><span class="p">,</span>
      <span class="nt">&quot;toEntityId&quot;</span> <span class="p">:</span> <span class="s2">&quot;9a7712d5-31f8-403a-92ea-77fad0d59e61&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
      <span class="nt">&quot;fromEntityId&quot;</span> <span class="p">:</span> <span class="s2">&quot;ad9915a8-fdab-4570-964f-8f636a8da20c&quot;</span><span class="p">,</span>
      <span class="nt">&quot;toEntityId&quot;</span> <span class="p">:</span> <span class="s2">&quot;edefa7cb-c19d-4955-9da5-22e43786ced5&quot;</span><span class="p">,</span>
      <span class="nt">&quot;relationshipId&quot;</span> <span class="p">:</span> <span class="s2">&quot;22bd9863-0161-4334-b73f-1f33fa8880cb&quot;</span>
    <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>
</pre></div>


<p>In order to explore each of these relationships, we can call the <code>relationship</code> endpoint with the <code>guid</code> of the relationship we are interested in:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/relationship/guid/33591207-ac0d-4374-91a2-5fadd71d6f0c
</pre></div>


<h4 id="glossary_1">Glossary</h4>
<p>To retrieve all glossaries:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/glossary
</pre></div>


<p>To describe a specific glossary by using its <code>guid</code>:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/glossary/dd2de287-96ae-4c24-8402-1eea0d477b59
</pre></div>


<p>and, more specifically, its terms:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/glossary/dd2de287-96ae-4c24-8402-1eea0d477b59/terms
</pre></div>


<p>and categories:</p>
<div class="highlight"><pre><span></span>/api/atlas/v2/glossary/dd2de287-96ae-4c24-8402-1eea0d477b59/categories
</pre></div>


<h2 id="more-metadata-sources">More (meta)data sources</h2>
<p>In order to see how Atlas can be expanded with more sources, we will add a connector to <a href="https://spark.apache.org/">Apache Spark</a> and use it to track the lineage of both standard batch processes and stream processes with <a href="https://kafka.apache.org/">Apache Kafka</a> as a source. As usual, since the focus of the article is not on any of the two sources, the easiest way to get started with them will be used.</p>
<h4 id="preparation">Preparation</h4>
<p>Let's first of all install Spark. We could use the previously installed Apache Bigtop repository and install Spark via <code>yum</code>, but since the connector needs at least Spark 2.3 to work, we need to <a href="https://spark.apache.org/downloads.html">download it</a> (choosing the package pre-built for Hadoop 2.7) and extract it to a folder of our choice, for instance <code>/opt/spark</code>. When this is done, we need to copy the Atlas configuration file in the Spark configuration folder, for instance (if Atlas was installed in <code>/opt/atlas/apache-atlas-2.0.0/</code>):</p>
<div class="highlight"><pre><span></span>$ <span class="nb">cd</span> /opt/spark/spark-2.4.3-bin-hadoop2.7
$ cp /opt/atlas/apache-atlas-2.0.0/conf/atlas-application.properties conf/
</pre></div>


<p>After this, we need to download the Spark-Atlas connector; we will use <a href="https://github.com/hortonworks-spark/spark-atlas-connector">Hortonworks's connector</a> since it has been created for this specific use case. Once again, we will need Maven and Git installed (but we can use any version of Maven):</p>
<div class="highlight"><pre><span></span>$ git clone https://github.com/hortonworks-spark/spark-atlas-connector
$ <span class="nb">cd</span> spark-atlas-connector
$ mvn package -DskipTests
</pre></div>


<p>Once the package is built, we need to "patch" Atlas to include the Spark model:</p>
<div class="highlight"><pre><span></span>$ cp patch/1100-spark_model.json /opt/atlas/apache-atlas-2.0.0/models/1000-Hadoop
</pre></div>


<p>and then we have to restart Atlas:</p>
<div class="highlight"><pre><span></span>$ <span class="nb">cd</span> /opt/atlas/apache-atlas-2.0.0/
$ ./bin/atlas_stop.py
$ ./bin/atlas_start.py
</pre></div>


<p>If we open Atlas again and look into the "Search By Type" dropdown menu, we should see a few more entity types starting with <code>spark_</code>.</p>
<h4 id="spark-batch">Spark batch</h4>
<p>As an example, let's create a test CSV file in <code>/tmp/test.csv</code> with this content:</p>
<div class="highlight"><pre><span></span>person_id,name,surname
1,John,Smith
2,Jane,Doe
</pre></div>


<p>We are now ready to launch the Spark shell as follows:</p>
<div class="highlight"><pre><span></span>$ /opt/spark/spark-2.4.3-bin-hadoop2.7/bin/spark-shell <span class="se">\</span>
    --jars spark-atlas-connector/target/spark-atlas-connector_2.11-0.1.0-SNAPSHOT.jar <span class="se">\</span>
    --conf spark.extraListeners<span class="o">=</span>com.hortonworks.spark.atlas.SparkAtlasEventTracker <span class="se">\</span>
    --conf spark.sql.queryExecutionListeners<span class="o">=</span>com.hortonworks.spark.atlas.SparkAtlasEventTracker <span class="se">\</span>
    --conf spark.sql.streaming.streamingQueryListeners<span class="o">=</span>com.hortonworks.spark.atlas.SparkAtlasStreamingQueryEventTracker
</pre></div>


<p>From the Spark shell, let's run the following commands to read the test CSV file and write it to a different CSV file with one derived column:</p>
<div class="highlight"><pre><span></span><span class="k">val</span> <span class="n">test</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;header&quot;</span><span class="o">,</span> <span class="kc">true</span><span class="o">).</span><span class="n">csv</span><span class="o">(</span><span class="s">&quot;/tmp/test.csv&quot;</span><span class="o">);</span>
<span class="n">test</span><span class="o">.</span><span class="n">withColumn</span><span class="o">(</span><span class="s">&quot;fullname&quot;</span><span class="o">,</span> <span class="n">concat</span><span class="o">(</span><span class="n">upper</span><span class="o">(</span><span class="n">col</span><span class="o">(</span><span class="s">&quot;surname&quot;</span><span class="o">)),</span> <span class="n">upper</span><span class="o">(</span><span class="n">col</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">))))</span>
    <span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">csv</span><span class="o">(</span><span class="s">&quot;/tmp/test2.csv&quot;</span><span class="o">);</span>
</pre></div>


<p>(A series of warnings about configuration values might appear.)</p>
<p>If everything went fine, on Atlas we should see at least one item when searching by the <code>spark_process</code> type. When we open the detail page for such item, the lineage section should show something like this:</p>
<p><img src="images/atlas_spark_batch.png" alt="Atlas-Spark connector" class="img-fluid" /></p>
<p>We will now save the same file to a Hive table:</p>
<div class="highlight"><pre><span></span><span class="n">test</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">saveAsTable</span><span class="o">(</span><span class="s">&quot;spark_test_table&quot;</span><span class="o">);</span>
<span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">table</span><span class="o">(</span><span class="s">&quot;spark_test_table&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">withColumn</span><span class="o">(</span><span class="s">&quot;fullname&quot;</span><span class="o">,</span> <span class="n">concat</span><span class="o">(</span><span class="n">upper</span><span class="o">(</span><span class="n">col</span><span class="o">(</span><span class="s">&quot;surname&quot;</span><span class="o">)),</span> <span class="n">upper</span><span class="o">(</span><span class="n">col</span><span class="o">(</span><span class="s">&quot;name&quot;</span><span class="o">))))</span>
    <span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">saveAsTable</span><span class="o">(</span><span class="s">&quot;spark_test_table_derived&quot;</span><span class="o">);</span>
</pre></div>


<p>Now the lineage will look like this:</p>
<p><img src="images/atlas_spark_batch_2.png" alt="Atlas-Spark connector" class="img-fluid" /></p>
<p>(If the lineage does not change, try to click on the "Clear" button and search for Spark processes again.)</p>
<p>The current version of the connector considers everything that is run in a single shell execution as a single process, so the lineage will include all the steps together.</p>
<h4 id="spark-streaming">Spark streaming</h4>
<p>In order to track the lineage of a Spark streaming application, we will use <a href="https://kafka.apache.org/">Apache Kafka</a> as a stream source. Atlas already includes a Kafka server, but we will download the full package in order to create the topics more easily.</p>
<div class="highlight"><pre><span></span>$ wget http://apache.mirror.anlx.net/kafka/2.2.0/kafka_2.12-2.2.0.tgz
$ tar zxvf kafka_2.12-2.2.0.tgz
$ <span class="nb">cd</span> kafka_2.12-2.2.0/
</pre></div>


<p>In order to create a topic, we can run the following:</p>
<div class="highlight"><pre><span></span>$ ./bin/kafka-topics.sh --create --bootstrap-server localhost:9027 --replication-factor <span class="m">1</span> --partitions <span class="m">1</span> --topic test-topic
</pre></div>


<p>The address used as a bootstrap server has to be the same that is configured as the value of the <code>atlas.kafka.bootstrap.servers</code> parameter in <code>atlas-application.properties</code> (the Atlas configuration file within its <code>conf</code> directory), unless we want to run a separate Kafka server.</p>
<p>Now we need to store some data into Kafka. We'll do it with the following command:</p>
<div class="highlight"><pre><span></span>$ ./bin/kafka-console-producer.sh --broker-list localhost:9027 --topic test-topic
</pre></div>


<p>where we will be prompted to insert one or more lines; after inserting a few lines, <code>Ctrl + D</code> will close the prompt. We are now ready to run the Spark shell again, this time adding <code>--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3</code> to avoid Kafka-related exceptions:</p>
<div class="highlight"><pre><span></span>$ /opt/spark/spark-2.4.3-bin-hadoop2.7/bin/spark-shell <span class="se">\</span>
    --jars /opt/atlas/spark-atlas-connector/spark-atlas-connector-assembly/target/spark-atlas-connector-assembly-0.1.0-SNAPSHOT.jar <span class="se">\</span>
    --conf spark.extraListeners<span class="o">=</span>com.hortonworks.spark.atlas.SparkAtlasEventTracker <span class="se">\</span>
    --conf spark.sql.queryExecutionListeners<span class="o">=</span>com.hortonworks.spark.atlas.SparkAtlasEventTracker <span class="se">\</span>
    --conf spark.sql.streaming.streamingQueryListeners<span class="o">=</span>com.hortonworks.spark.atlas.SparkAtlasStreamingQueryEventTracker <span class="se">\</span>
    --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3
</pre></div>


<p>From the shell, we can then run this:</p>
<div class="highlight"><pre><span></span><span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;kafka&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;kafka.bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;localhost:9027&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;subscribe&quot;</span><span class="o">,</span> <span class="s">&quot;test-topic&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;startingOffsets&quot;</span><span class="o">,</span> <span class="s">&quot;earliest&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">load</span><span class="o">;</span>

<span class="k">val</span> <span class="n">df2</span> <span class="k">=</span> <span class="n">df</span><span class="o">.</span><span class="n">selectExpr</span><span class="o">(</span><span class="s">&quot;topic&quot;</span><span class="o">,</span> <span class="s">&quot;CAST(key AS STRING)&quot;</span><span class="o">,</span> <span class="s">&quot;CAST(value AS STRING)&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">outputMode</span><span class="o">(</span><span class="s">&quot;append&quot;</span><span class="o">).</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;json&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;path&quot;</span><span class="o">,</span> <span class="s">&quot;/tmp/stream_kafka.json&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;checkpointLocation&quot;</span><span class="o">,</span> <span class="s">&quot;/tmp/chkpoint_dir&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">start</span><span class="o">;</span>
</pre></div>


<p>where we basically create a dataset out of a Kafka stream by subscribing to the <code>test-topic</code> topic and starting from its beginning, then we create a new stream to be written as a JSON file (actually, as a directory containing multiple JSON files).</p>
<p>If we check the lineage for this process, we should now see something like this:</p>
<p><img src="images/atlas_spark_streaming.png" alt="Atlas-Spark connector" class="img-fluid" /></p>
<p>We can even route the data from the first topic into another Kafka topic! All we need to do is to create a new topic:</p>
<div class="highlight"><pre><span></span>$ /opt/kafka/kafka_2.12-2.2.0/bin/kafka-topics.sh --create --bootstrap-server localhost:9027 --replication-factor <span class="m">1</span> --partitions <span class="m">1</span> --topic test-topic-rec
</pre></div>


<p>and then, from the shell, run this:</p>
<div class="highlight"><pre><span></span><span class="k">val</span> <span class="n">df</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;kafka&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;kafka.bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;localhost:9027&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;subscribe&quot;</span><span class="o">,</span> <span class="s">&quot;test-topic&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;startingOffsets&quot;</span><span class="o">,</span> <span class="s">&quot;earliest&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">load</span><span class="o">;</span>

<span class="k">val</span> <span class="n">df2</span> <span class="k">=</span> <span class="n">df</span><span class="o">.</span><span class="n">selectExpr</span><span class="o">(</span><span class="s">&quot;topic&quot;</span><span class="o">,</span> <span class="s">&quot;CAST(key AS STRING)&quot;</span><span class="o">,</span> <span class="s">&quot;CAST(value AS STRING)&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">&quot;kafka&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;kafka.bootstrap.servers&quot;</span><span class="o">,</span> <span class="s">&quot;localhost:9027&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;topic&quot;</span><span class="o">,</span> <span class="s">&quot;test-topic-rec&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;checkpointLocation&quot;</span><span class="o">,</span> <span class="s">&quot;/tmp/chkpoint_dir_rec&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="n">start</span><span class="o">;</span>
</pre></div>


<p>which is the same as before except for the different type of write stream (Kafka rather than JSON files), and will result in this lineage:</p>
<p><img src="images/atlas_spark_streaming_2.png" alt="Atlas-Spark connector" class="img-fluid" /></p>
<p>(In case an exception such as <code>org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server 'localhost:9026' with timeout of 200 ms</code> is thrown, the value of the <code>atlas.kafka.zookeeper.connection.timeout.ms</code> field within the <code>atlas-application.properties</code> configuration file should be increased (e.g. set it to 2000) and Atlas be restarted.)</p>
<p>As we can see, lineage information from a Spark batch or streaming job can be easily sent to Atlas. Spark ML pipelines are also supported by the connector, but since this would require patching and recompiling Atlas, I decided to skip them for the time being.</p>
<h2 id="conclusions">Conclusions</h2>
<p>In this article we have explored Atlas capabilities into more details, understanding what classifications and glossaries are and how to make use of them, then listing a few REST API endpoints along with examples, and finally trying out an external connector to add even more lineage information. Atlas is quite mature as a solution, but documentation is still a bit scattered; even if I haven't touched on everything (especially the underlying datastores and the creation of new entities), I hope to have added a little bit to the coverage of this fantastic tool.</p>
    </div>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <ul class="col-sm-6 list-inline">
    <li class="list-inline-item"><a href="https://apothem.blog/archives.html">Archives</a></li>
    <li class="list-inline-item"><a href="https://apothem.blog/categories.html">Categories</a></li>
      <li class="list-inline-item"><a href="https://apothem.blog/tags.html">Tags</a></li>
  </ul>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p>
</div>    </div>
  </footer>
</body>

</html>