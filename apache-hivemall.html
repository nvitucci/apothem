<!doctype html>
<html lang="en" prefix="og: http://ogp.me/ns#">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  Apache Hivemall | APOTHEM
</title>
  <link rel="canonical" href="https://apothem.blog/apache-hivemall.html">

    <link rel="icon" type="image/x-icon" href="https://apothem.blog/favicon.ico">

  <link rel="stylesheet" href="https://apothem.blog/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://apothem.blog/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://apothem.blog/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://apothem.blog/theme/css/theme.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="https://apothem.blog/feeds/all.atom.xml">
  <link rel="alternate" type="application/atom+xml" title="Categories Atom Feed"
        href="https://apothem.blog/feeds/{slug}.atom.xml">  
  <meta name="description" content="With this article we will move a little bit out of the data engineering space and delve into another subject I love: we will explore the world of distributed machine learning with Apache Hivemall. From the project's homepage: Hivemall is a scalable machine learning library that runs on Apache Hive â€¦">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
    <div class="col-sm-4">
      <a href="https://apothem.blog/">
        <img class="img-fluid rounded" src=https://apothem.blog/images/profile.svg width=180 alt="APOTHEM">
      </a>
    </div>
  <div class="col-sm-8">
    <h1 class="title"><a href="https://apothem.blog/">APOTHEM</a></h1>
      <p class="text-muted">Apache Project(s) of the month</p>
      <ul class="list-inline">
          <li class="list-inline-item"><a href="https://projects.apache.org" target="_blank">projects.apache.org</a></li>
              <li class="list-inline-item text-muted">|</li>
            <li class="list-inline-item"><a href="https://apothem.blog/pages/about.html">About</a></li>
            <li class="list-inline-item"><a href="https://apothem.blog/pages/faq.html">FAQ</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fa fa-github" href="https://github.com/nvitucci/apothem" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fa fa-twitter" href="https://twitter.com/nvitucci" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fa fa-rss" href="feeds/all.atom.xml" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h2>  Apache Hivemall
</h2>
      <hr>
  <article class="article">
    <header>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2020-02-02T00:00:00+00:00">
          <i class="fa fa-clock-o"></i>
          Sun 02 February 2020
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="https://apothem.blog/category/projects.html">projects</a>
        </li>
          <li class="list-inline-item">
            <i class="fa fa-files-o"></i>
              <a href="https://apothem.blog/tag/big-data.html">#Big Data</a>,               <a href="https://apothem.blog/tag/machine-learning.html">#Machine Learning</a>          </li>
      </ul>
    </header>
    <div class="content">
      <p>With this article we will move a little bit out of the data engineering space and delve into another subject I love: we will explore the world of distributed machine learning with <a href="https://hivemall.incubator.apache.org/">Apache Hivemall</a>. From the project's homepage:</p>
<div class="highlight"><pre><span></span>Hivemall is a scalable machine learning library that runs on Apache Hive, Spark and Pig.
</pre></div>


<p>In short, Hivemall is a collection of Hive UDFs (User-Defined Functions) to create, train, and evaluate machine learning models using any of the supported engines. Since Hive provides a SQL-like query language, Hivemall is basically <strong>machine learning in SQL</strong>; it is then well suited to users who are already fluent in SQL or who need an additional abstraction layer on top of machine learning libraries.</p>
<p>We will explore some of the many capabilities that Hivemall provides by tackling one of the most common machine learning tasks: classification. Let's start by setting up the environment.</p>
<h3>Setting up Hivemall and Spark</h3>
<p>Like for other Java-based Apache projects, we will need to install git, Java 8, and Maven 3.5+ (check <a href="https://apothem.blog/apache-atlas.html">the article on Apache Atlas</a> if you need a detailed walkthrough). Once we have installed everything, we need to build Hivemall:</p>
<div class="highlight"><pre><span></span>$ git clone https://github.com/apache/incubator-hivemall.git
$ <span class="nb">cd</span> incubator-hivemall
$ bin/build.sh
</pre></div>


<p>Since Hive is a bit cumbersome to install and configure, and we will need to do a bit of data preprocessing as well, we will run Hive queries from Spark instead. We therefore need to:</p>
<ul>
<li>
<p>download Spark 2.4.4 from <a href="https://spark.apache.org/downloads.html">this page</a> (choosing the 2.4.4 release pre-built for Hadoop 2.7);</p>
</li>
<li>
<p>extract the content of the .tgz file into a directory, possibly alongside the directory where we cloned the Hivemall repository:</p>
<div class="highlight"><pre><span></span>$ tar zxvf spark-2.4.4-bin-hadoop2.7.tgz
</pre></div>


</li>
<li>
<p>launch the Spark shell with the Hivemall JAR file (release <code>0.6.2-SNAPSHOT</code> as of now) built before:</p>
<div class="highlight"><pre><span></span>$ spark-2.4.4-bin-hadoop2.7/bin/spark-shell <span class="se">\</span>
  --jars incubator-hivemall/target/hivemall-all-0.6.2-incubating-SNAPSHOT.jar
</pre></div>


</li>
</ul>
<p>Once the Spark shell is running, we need to load all the Hivemall functions:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">:</span><span class="kt">load</span> <span class="kt">incubator-hivemall/resources/ddl/define-all.spark</span>
</pre></div>


<p>We will likely see some errors because some Hivemall functions already exist in Spark 2.4.4, but there is no need to worry because the Spark functions will not be overridden. We are now ready to start using Hivemall!</p>
<h3>Classification</h3>
<p><a href="https://en.wikipedia.org/wiki/Classification_(machine_learning)">Classification</a> is the task of assigning a category to an <em>observation</em> based on the value of its <em>features</em>. For example, we may have some emails that we want to classify as "spam" or "non-spam" based on the presence of one or more keywords; this is called <em>binary classification</em> because we only have two categories, or one category that an instance belongs to or not. Another example, which is a sort of "Hello world" for machine learning and classification, is the categorization of a specific variant of the <a href="https://en.wikipedia.org/wiki/Iris_(plant)">iris</a> plant depending on features such as the length and width of its sepals and petals; this is called <em>multiclass classification</em> because there are more than two different classes. We will use the <a href="https://archive.ics.uci.edu/ml/datasets/Iris">Iris Data Set</a> in this section.</p>
<p>The algorithm we will use is called <a href="https://xgboost.readthedocs.io/en/latest/">XGBoost</a>. There are several reasons behind this choice:</p>
<ul>
<li>I like its conceptual simplicity and its <em>interpretability</em>, which is an important feature of modern machine learning algorithms;</li>
<li>it is designed for efficiency and has already proved itself;</li>
<li>it does not require a lot of preprocessing (for instance, it does not require feature scaling);</li>
<li>it is now supported in Hivemall starting with version 0.6.0 (the supported XGBoost version being 0.90).</li>
</ul>
<p>If you are interested in the theoretical and technical details, do take a look at the excellent documentation <a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">here</a>. For our purposes, we need to keep in mind just one thing: simplifying a lot, XGBoost will basically give us a group of decision trees.</p>
<h4>Data preprocessing</h4>
<p>First of all, we need to download our dataset from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data">here</a> and put it alongside the previous directories. The data will look like the following:</p>
<div class="highlight"><pre><span></span>5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
...
</pre></div>


<p>Let's go back to our Spark shell and run the following command:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">iris</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">option</span><span class="o">(</span><span class="s">&quot;inferSchema&quot;</span><span class="o">,</span> <span class="kc">true</span><span class="o">).</span><span class="n">csv</span><span class="o">(</span><span class="s">&quot;iris.data&quot;</span><span class="o">).</span><span class="n">toDF</span><span class="o">(</span>
    <span class="s">&quot;sepal_length&quot;</span><span class="o">,</span> <span class="s">&quot;sepal_width&quot;</span><span class="o">,</span> <span class="s">&quot;petal_length&quot;</span><span class="o">,</span> <span class="s">&quot;petal_width&quot;</span><span class="o">,</span> <span class="s">&quot;class&quot;</span><span class="o">)</span>
</pre></div>


<p>This command will read the dataset file interpreting it as a CSV file, automatically creating a schema and assigning the provided names to the columns. We can check both the schema and the first 5 rows of the dataset:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">iris</span><span class="o">.</span><span class="n">printSchema</span>
<span class="n">scala</span><span class="o">&gt;</span> <span class="n">iris</span><span class="o">.</span><span class="n">show</span><span class="o">(</span><span class="mi">5</span><span class="o">)</span>
</pre></div>


<p>Now we can create a temporary view of our Spark Dataframe so that we can use it like a Hive table:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">iris</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="o">(</span><span class="s">&quot;iris&quot;</span><span class="o">)</span>
</pre></div>


<p>and then we can run a SQL query as a check:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM iris&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">(</span><span class="mi">5</span><span class="o">)</span>
</pre></div>


<!-- This is commented out. -->

<div class="highlight"><pre><span></span>+------------+-----------+------------+-----------+-----------+
|sepal_length|sepal_width|petal_length|petal_width|      class|
+------------+-----------+------------+-----------+-----------+
|         5.1|        3.5|         1.4|        0.2|Iris-setosa|
|         4.9|        3.0|         1.4|        0.2|Iris-setosa|
|         4.7|        3.2|         1.3|        0.2|Iris-setosa|
|         4.6|        3.1|         1.5|        0.2|Iris-setosa|
|         5.0|        3.6|         1.4|        0.2|Iris-setosa|
+------------+-----------+------------+-----------+-----------+
</pre></div>


<p>We have our raw data table, but in order to run any Hivemall algorithm we need to tweak it a little. More specifically, we need to:</p>
<ul>
<li>assign a unique ID to each row;</li>
<li>create a <em>feature vector</em> such as <code>[1:5.1, 2:3.5, 3:1.4, 4:0.2]</code>, that is a string array where each element is of the form <code>feature:value</code>, from the feature columns;</li>
<li>transform the iris "class" from a string to an integer value.</li>
</ul>
<p>These steps are part of a very important phase of a machine learning pipeline called <em>feature engineering</em>. We can create a new table with all these transformations with the following command:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    CREATE TABLE iris_processed AS</span>
<span class="s">    SELECT monotonically_increasing_id() rowid,</span>
<span class="s">    indexed_features(sepal_length, sepal_width, petal_length, petal_width) features,</span>
<span class="s">    quantify(true, class) label</span>
<span class="s">    FROM iris</span>
<span class="s">    ORDER BY class&quot;&quot;&quot;</span><span class="o">)</span>
</pre></div>


<p>While <code>monotonically_increasing_id</code> is a Spark function, <code>indexed_features</code> and <code>quantify</code> are both Hivemall UDFs:</p>
<ul>
<li><code>indexed_features</code> will create an array from the values passed as parameters, prepending each value with its position in the array (starting from 1, not 0, since 0 is reserved for the <a href="https://hivemall.incubator.apache.org/userguide/tips/addbias.html">bias feature</a>);</li>
<li><code>quantify</code> can be used to convert a categorical variable into a numerical variable (the <code>ORDER BY</code> is used to make sure that it is applied on one reducer only, thus consistently associating the same number to the same non-numerical variable).</li>
</ul>
<p>There are <a href="https://hivemall.incubator.apache.org/userguide/tips/rowid.html">other ways</a> to create a unique ID for each row; Hivemall provides a function called <code>rowid</code>, but it does not work in Spark (it would throw a <code>java.lang.IllegalStateException: MapredContext is not set</code> exception).</p>
<p>Let's see how the new table looks like:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM iris_processed&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="kc">false</span><span class="o">)</span>
</pre></div>


<!-- This is commented out. -->

<div class="highlight"><pre><span></span>+-----+----------------------------+-----+
|rowid|features                    |label|
+-----+----------------------------+-----+
|0    |[1:5.1, 2:3.5, 3:1.4, 4:0.2]|0    |
|1    |[1:4.9, 2:3.0, 3:1.4, 4:0.2]|0    |
|2    |[1:4.7, 2:3.2, 3:1.3, 4:0.2]|0    |
|3    |[1:4.6, 2:3.1, 3:1.5, 4:0.2]|0    |
|4    |[1:5.0, 2:3.6, 3:1.4, 4:0.2]|0    |
+-----+----------------------------+-----+
</pre></div>


<p>The table is ready to be used for training.</p>
<h4>Training</h4>
<p>We need to add one more function first (which should be included in the <code>define-all.spark</code> file now, but are not at the moment):</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;CREATE TEMPORARY FUNCTION train_xgboost AS &#39;hivemall.xgboost.XGBoostTrainUDTF&#39;&quot;</span><span class="o">)</span>
</pre></div>


<p>We also need to set the <code>spark.sql.shuffle.partitions</code> (or <code>mapred.reduce.tasks</code> if we use Hive directly, not through Spark) configuration parameter to a "low" number, for instance to 10 (instead of the default 200).</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SET spark.sql.shuffle.partitions=10&quot;</span><span class="o">)</span>
</pre></div>


<p>If we don't do this and the dataset is small (as in our case, with only 150 rows), we will have some empty partitions which will cause an error like the following during the training phase:</p>
<div class="highlight"><pre><span></span>...
Caused by: ml.dmlc.xgboost4j.java.XGBoostError: [01:14:14] /home/travis/build/myui/build-xgboost-jvm/xgboost/src/learner.cc:723: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
...
</pre></div>


<p>Let's check that there are no empty partitions by running this command, which should give no output:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM iris_processed CLUSTER BY rand(0)&quot;</span><span class="o">).</span><span class="n">foreachPartition</span><span class="o">(</span>
    <span class="n">part</span> <span class="k">=&gt;</span> <span class="k">if</span> <span class="o">(</span><span class="n">part</span><span class="o">.</span><span class="n">length</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span> <span class="n">println</span><span class="o">(</span><span class="s">s&quot;Zero-length partition&quot;</span><span class="o">))</span>
</pre></div>


<p>Another way to check is to run the following command, which should return the same number of partitions as the value of the <code>spark.sql.shuffle.partitions</code> parameter:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT DISTINCT spark_partition_id() FROM (SELECT * FROM iris_processed CLUSTER BY rand(0))&quot;</span><span class="o">).</span><span class="n">count</span>
</pre></div>


<p>(Try to set <code>spark.sql.shuffle.partitions</code> to 50 or 100 and run the previous commands to see a different behaviour.)</p>
<p>Finally we can create the model:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    CREATE TABLE iris_model_softmax AS</span>
<span class="s">    SELECT train_xgboost(features, label, &#39;-objective multi:softmax -num_class 3&#39;) AS (model_id, model)</span>
<span class="s">    FROM (SELECT features, label FROM iris_processed CLUSTER BY rand(0))</span>
<span class="s">    &quot;&quot;&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>When performing a multiclass classification task, we have to explicitly specify the <code>-num_class</code> parameter; we have 3 classes in our dataset, therefore we set it to 3. The other parameter is the objective function: <code>multi:softmax</code> will use the <code>softmax</code> objective and return only the predicted class, while <code>multi:softprob</code> will use the <code>softprob</code> objective and return the probability associated to each class (see <a href="https://xgboost.readthedocs.io/en/release_0.90/parameter.html#learning-task-parameters">the XGBoost docs</a> for more information, and for the other parameters). A model using the <code>softprob</code> objective can be created as follows:</p>
<div class="highlight"><pre><span></span>scala&gt; spark.sql(&quot;&quot;&quot;
    CREATE TABLE iris_model_softprob AS
    SELECT train_xgboost(features, label, &#39;-objective multi:softprob -num_class 3&#39;) AS (model_id, model)
    FROM (SELECT features, label FROM iris_processed CLUSTER BY rand(0))
    &quot;&quot;&quot;).show
</pre></div>


<p>Another important consideration is the use of <code>CLUSTER BY rand(0)</code>, which is used to shuffle the dataset in order to prevent any bias. Please note that, for simplicity, we didn't split the dataset into a training and a test set; in real-world applications this should always be done.</p>
<p>Now that the model is trained, we can read it:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">modelArray</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM iris_model_softprob&quot;</span><span class="o">).</span><span class="n">take</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">head</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">toString</span>
</pre></div>


<p>and then visualize it:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">import</span> <span class="nn">org.apache.hadoop.io.Text</span>
<span class="n">scala</span><span class="o">&gt;</span> <span class="k">import</span> <span class="nn">hivemall.xgboost.utils.XGBoostUtils.deserializeBooster</span>
<span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">booster</span> <span class="k">=</span> <span class="n">deserializeBooster</span><span class="o">(</span><span class="k">new</span> <span class="nc">Text</span><span class="o">(</span><span class="n">modelArray</span><span class="o">))</span>
<span class="n">scala</span><span class="o">&gt;</span> <span class="n">println</span><span class="o">(</span><span class="n">booster</span><span class="o">.</span><span class="n">getModelDump</span><span class="o">(</span><span class="nc">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">](),</span> <span class="kc">false</span><span class="o">,</span> <span class="s">&quot;json&quot;</span><span class="o">).</span><span class="n">mkString</span><span class="o">(</span><span class="s">&quot;\n&quot;</span><span class="o">))</span>
</pre></div>


<p>The parameters of the <code>getModelDump</code> method are:</p>
<ul>
<li>an array of feature names (potentially empty, to map the default <code>f0...fn</code> names to user-defined names);</li>
<li>a boolean to include/exclude the split statistics;</li>
<li>the output format (at the moment only <code>"text"</code> or <code>"json"</code>).</li>
</ul>
<p>We can also get the importance of each feature:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">println</span><span class="o">(</span><span class="n">booster</span><span class="o">.</span><span class="n">getFeatureScore</span><span class="o">(</span><span class="nc">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">]()))</span>
</pre></div>


<!-- Empty -->

<div class="highlight"><pre><span></span>{f1=18, f2=2, f3=17}
</pre></div>


<p>and the feature importances for several measures (one of <code>"weight"</code>, <code>"gain"</code>, <code>"cover"</code>, <code>"total_gain"</code>, <code>"total_cover"</code>):</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">booster</span><span class="o">.</span><span class="n">getScore</span><span class="o">(</span><span class="nc">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">](),</span> <span class="s">&quot;gain&quot;</span><span class="o">)</span>
</pre></div>


<!-- Empty -->

<div class="highlight"><pre><span></span>{f1=1.4596930238833332, f2=0.0668432191, f3=1.5014036119411764}
</pre></div>


<p>We can customize the feature names in both methods:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="n">featureNames</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="s">&quot;bias&quot;</span><span class="o">,</span> <span class="s">&quot;sepal_length&quot;</span><span class="o">,</span> <span class="s">&quot;sepal_width&quot;</span><span class="o">,</span> <span class="s">&quot;petal_length&quot;</span><span class="o">,</span> <span class="s">&quot;petal_width&quot;</span><span class="o">)</span>
<span class="n">scala</span><span class="o">&gt;</span> <span class="n">println</span><span class="o">(</span><span class="n">booster</span><span class="o">.</span><span class="n">getFeatureScore</span><span class="o">(</span><span class="n">featureNames</span><span class="o">))</span>
<span class="n">scala</span><span class="o">&gt;</span> <span class="n">booster</span><span class="o">.</span><span class="n">getScore</span><span class="o">(</span><span class="n">featureNames</span><span class="o">,</span> <span class="s">&quot;gain&quot;</span><span class="o">)</span>
</pre></div>


<!-- Empty -->

<div class="highlight"><pre><span></span>{sepal_width=2, sepal_length=18, petal_length=17}
{sepal_width=0.0668432191, sepal_length=1.4596930238833332, petal_length=1.5014036119411764}
</pre></div>


<h4>Prediction</h4>
<p>Now that the model is ready, we can use it to classify some data. It is important to note that the way Hivemall makes XGBoost parallel is somewhat different from the parallel implementation of the official library; in fact, instead of using the <a href="https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html">AllReduce paradigm</a> where all the parallel workers have to communicate, Hivemall uses a <a href="https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier">voting classifier</a> on top of many <em>independent</em> XGBoost learners. In this way every worker can train its own local model based on its partition of data (possibly overlapping with other partitions if the <code>amplify</code> or <code>rand_amplify</code> functions are used), and the actual prediction happens via a majority vote (if the objective function is <code>softmax</code>, since its output is a class label) or by averaging (if the objective function is <code>softprob</code>, since its output is a list of probabilities). We will see both examples.</p>
<p>Like we did for training, we need first of all to include a few more functions:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;CREATE TEMPORARY FUNCTION xgboost_predict_one AS &#39;hivemall.xgboost.XGBoostPredictOneUDTF&#39;&quot;</span><span class="o">)</span>
<span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;CREATE TEMPORARY FUNCTION xgboost_predict_triple AS &#39;hivemall.xgboost.XGBoostPredictTripleUDTF&#39;&quot;</span><span class="o">)</span>
<span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;CREATE TEMPORARY FUNCTION majority_vote AS &#39;hivemall.tools.aggr.MajorityVoteUDAF&#39;&quot;</span><span class="o">)</span>
</pre></div>


<p>and to set another configuration option:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SET spark.sql.crossJoin.enabled=true&quot;</span><span class="o">)</span>
</pre></div>


<p>This is necessary because Hivemall uses a left join without condition (basically a cross join) between the model and the data to make predictions, and cross joins are disabled by default for performance reasons.</p>
<p>Let's now create a table containing our predictions:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    CREATE TABLE iris_pred_softmax AS</span>
<span class="s">    SELECT rowid, majority_vote(CAST(predicted AS INT)) AS label</span>
<span class="s">    FROM (</span>
<span class="s">        SELECT xgboost_predict_one(rowid, features, model_id, model) AS (rowid, predicted)</span>
<span class="s">        FROM iris_model_softmax l LEFT JOIN iris_processed r) t</span>
<span class="s">    GROUP BY rowid&quot;&quot;&quot;</span><span class="o">)</span>
</pre></div>


<p>We use the <code>xgboost_predict_one</code> function to predict the class of each row of the <code>iris_processed</code> table with the model stored in the <code>model</code> column of the <code>iris_model_softmax</code> table; the <code>rowid</code> is stored as well to make the model evaluation easier later on. In this case we use the <code>majority_vote</code> function to "vote" for the most frequent prediction, which will become the actual prediction. We can check the predicted values from the new table:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM iris_pred_softmax ORDER BY rowid&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>Once again, keep in mind that for this example we didn't split the dataset into a training set and a test set. If we want to get the probability of each class assignment, instead, we can use the <code>xgboost_predict_triple</code> function as follows:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    SELECT rowid, argmax(collect_list(avg_proba)) label, collect_list(avg_proba) proba</span>
<span class="s">    FROM (</span>
<span class="s">        CREATE TABLE iris_pred_softprob AS</span>
<span class="s">        SELECT rowid, label, avg(proba) avg_proba</span>
<span class="s">        FROM (</span>
<span class="s">            SELECT xgboost_predict_triple(rowid, features, model_id, model) AS (rowid, label, proba)</span>
<span class="s">            FROM iris_model_softprob l LEFT JOIN iris_processed r)</span>
<span class="s">        GROUP BY rowid, label ORDER BY rowid, label)</span>
<span class="s">    GROUP BY rowid ORDER BY rowid&quot;&quot;&quot;</span><span class="o">)</span>
</pre></div>


<p>The prediction works in the same way as before, with three important differences:</p>
<ul>
<li>the prediction function now returns three values instead of two, including the probability as well;</li>
<li>we do not "vote" for the most frequent class, but rather we average the probabilities of each class and select the class with the maximum average probability;</li>
<li>the model we run the prediction against has to be created with the <code>multi:softprob</code> objective, otherwise the <code>xgboost_predict_triple</code> will return the wrong results.</li>
</ul>
<p>We can also use two simpler functions, <code>xgboost_predict</code> and <code>xgboost_batch_predict</code>, to obtain the same result (look at the <a href="https://hivemall.incubator.apache.org/userguide/binaryclass/news20b_xgboost.html">examples</a> for more details):</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;CREATE TEMPORARY FUNCTION xgboost_predict AS &#39;hivemall.xgboost.XGBoostOnlinePredictUDTF&#39;&quot;</span><span class="o">)</span>
<span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;CREATE TEMPORARY FUNCTION xgboost_batch_predict AS &#39;hivemall.xgboost.XGBoostBatchPredictUDTF&#39;&quot;</span><span class="o">)</span>

<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    CREATE TABLE iris_pred_softprob AS</span>
<span class="s">    SELECT rowid, argmax(array_avg(predicted)), array_avg(predicted) as prob</span>
<span class="s">    FROM (</span>
<span class="s">        SELECT xgboost_predict(rowid, features, model_id, model) AS (rowid, predicted)</span>
<span class="s">        FROM iris_model_softprob l LEFT JOIN iris_processed r)</span>
<span class="s">    GROUP BY rowid ORDER BY rowid&quot;&quot;&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="kc">false</span><span class="o">)</span>

<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    CREATE TABLE iris_pred_softprob AS</span>
<span class="s">    SELECT rowid, argmax(array_avg(predicted)), array_avg(predicted) as prob</span>
<span class="s">    FROM (</span>
<span class="s">        SELECT xgboost_batch_predict(rowid, features, model_id, model) AS (rowid, predicted)</span>
<span class="s">        FROM iris_model_softprob l LEFT JOIN iris_processed r)</span>
<span class="s">    GROUP BY rowid ORDER BY rowid&quot;&quot;&quot;</span><span class="o">).</span><span class="n">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="kc">false</span><span class="o">)</span>
</pre></div>


<p>The <code>array_avg</code> function is used to calculate the average of an array column. Again, we can check the predicted values:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;SELECT * FROM iris_pred_softprob ORDER BY rowid&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<h4>Evaluation</h4>
<p>It is time now to evaluate our models. Since we performed our predictions on the same dataset that we used for training, we would expect the models to perform really good. Let's start with the <code>softmax</code> model:</p>
<div class="highlight"><pre><span></span><span class="n">scala</span><span class="o">&gt;</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;&quot;&quot;</span>
<span class="s">    SELECT sum(if(actual=predicted, 1.0, 0.0))/count(1)</span>
<span class="s">    FROM (</span>
<span class="s">        SELECT source.label actual, pred.label predicted</span>
<span class="s">        FROM iris_processed source JOIN iris_pred_softmax pred USING (rowid))&quot;&quot;&quot;</span><span class="o">).</span><span class="n">show</span>
</pre></div>


<p>Here we are simply counting all the instances where the actual value corresponds to the predicted value (using the <code>rowid</code> column to join the training dataset and the predicted dataset), then we divide that number by the number of all the instances to get a ratio. As expected, the ratio is really high (even though not exactly 1.0); we can also run exactly the same query by just swapping <code>iris_pred_softmax</code> with <code>iris_pred_softprob</code> to evaluate the second model. If we do not find very close values, the reason can be parallelization (since each partition calculates its own model); in order to get very close results we would need to reduce the <code>spark.sql.shuffle.partitions</code> value to one, effectively making the whole computation happen in only one partition by creating a single XGBoost model, which will contain the full dataset (obviously not recommended for large datasets).</p>
<h3>Conclusions</h3>
<p>We have seen how Hivemall makes the XGBoost algorithm (and the classification task in general) very easy to implement and to understand. Of course there is much more to it, for instance:</p>
<ul>
<li>XGBoost can be configured with many different parameters, so it's worth reading its <a href="https://hivemall.incubator.apache.org/userguide/binaryclass/news20b_xgboost.html">Hivemall usage guide</a> and its <a href="https://xgboost.readthedocs.io/en/release_0.90/parameter.html">configuration docs</a>;</li>
<li>XGBoost can be used also for <a href="https://hivemall.incubator.apache.org/userguide/binaryclass/news20b_xgboost.html">binary classification</a> and for <a href="https://hivemall.incubator.apache.org/userguide/regression/e2006_xgboost.html">regression</a> tasks;</li>
<li>Hivemall features other algorithms for classification, including a <a href="https://hivemall.incubator.apache.org/userguide/binaryclass/a9a_generic.html">general and highly customizable classifier</a>, <a href="https://hivemall.incubator.apache.org/userguide/binaryclass/a9a_lr.html">logistic regression</a>, and <a href="https://hivemall.incubator.apache.org/userguide/binaryclass/a9a_minibatch.html">mini-batch gradient descent</a>.</li>
</ul>
<p>There are many other machine learning tasks to cover, such as <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression</a> and <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering</a>, and we have just barely touched on feature engineering. So, stay tuned for another article on this very interesting project!</p>
    </div>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <ul class="col-sm-6 list-inline">
    <li class="list-inline-item"><a href="https://apothem.blog/archives.html">Archives</a></li>
    <li class="list-inline-item"><a href="https://apothem.blog/categories.html">Categories</a></li>
      <li class="list-inline-item"><a href="https://apothem.blog/tags.html">Tags</a></li>
  </ul>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p>
</div>    </div>
  </footer>
</body>

</html>